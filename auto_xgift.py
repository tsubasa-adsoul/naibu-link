import tkinter as tk
from tkinter import messagebox, filedialog
import customtkinter as ctk
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import time
import re
import threading
import csv
import sys
import os

ctk.set_appearance_mode("light")
ctk.set_default_color_theme("blue")

class XGiftAnalyzer:
    def __init__(self):
        self.root = ctk.CTk()
        self.root.title("ğŸ”— xgift.jpå°‚ç”¨å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆAFFINGERå¯¾å¿œï¼‰")
        self.root.geometry("1200x800")
        
        self.pages = {}
        self.links = []
        self.detailed_links = []
        
        self.setup_ui()
        
        # è‡ªå‹•åˆ†æé–‹å§‹ï¼ˆEXEåŒ–å¯¾å¿œï¼‰
        self.root.after(1000, self.auto_start_analysis)  # 1ç§’å¾Œã«è‡ªå‹•é–‹å§‹

    def setup_ui(self):
        main_frame = ctk.CTkScrollableFrame(self.root)
        main_frame.pack(fill="both", expand=True, padx=20, pady=20)

        # ã‚¿ã‚¤ãƒˆãƒ«
        ctk.CTkLabel(main_frame, text="ğŸ”— xgift.jpå°‚ç”¨å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æãƒ„ãƒ¼ãƒ«", 
                    font=ctk.CTkFont(size=28, weight="bold")).pack(pady=10)
        ctk.CTkLabel(main_frame, text="AFFINGER ãƒ†ãƒ¼ãƒå¯¾å¿œç‰ˆ", 
                    font=ctk.CTkFont(size=16), text_color="gray").pack(pady=5)

        # URLå…¥åŠ›
        self.url_entry = ctk.CTkEntry(main_frame, placeholder_text="https://xgift.jp/blog/", width=500)
        self.url_entry.pack(pady=10)
        self.url_entry.insert(0, "https://xgift.jp/blog/")

        # åˆ†æãƒœã‚¿ãƒ³
        self.analyze_btn = ctk.CTkButton(main_frame, text="åˆ†æé–‹å§‹", command=self.start_analysis)
        self.analyze_btn.pack(pady=10)

        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
        self.status_label = ctk.CTkLabel(main_frame, text="")
        self.status_label.pack(pady=10)

        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
        self.progress = ctk.CTkProgressBar(main_frame, width=400)
        self.progress.pack(pady=5)

        # çµæœã‚¨ãƒªã‚¢
        self.result_frame = ctk.CTkScrollableFrame(main_frame, height=400)
        self.result_frame.pack(fill="both", expand=True, pady=20)

        # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒœã‚¿ãƒ³ï¼ˆè©³ç´°ã®ã¿ï¼‰
        ctk.CTkButton(main_frame, text="è©³ç´°CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ", command=self.export_detailed).pack(pady=10)

    def normalize_url(self, url):
        """URLæ­£è¦åŒ–"""
        if not url: return ""
        try:
            parsed = urlparse(url)
            normalized = f"https://{parsed.netloc.replace('www.', '')}{parsed.path.rstrip('/')}"
            return normalized
        except:
            return url

    def is_article_page(self, url):
        """å€‹åˆ¥è¨˜äº‹ãƒšãƒ¼ã‚¸ã‹ã©ã†ã‹åˆ¤å®šï¼ˆAFFINGERå¯¾å¿œãƒ»ãƒ‡ãƒãƒƒã‚°ä»˜ãï¼‰"""
        path = urlparse(self.normalize_url(url)).path.lower()
        
        print(f"        è¨˜äº‹åˆ¤å®š: {url}")
        print(f"          ãƒ‘ã‚¹: {path}")
        
        # é™¤å¤–ï¼šæ˜ã‚‰ã‹ã«è¨˜äº‹ã§ã¯ãªã„ã‚‚ã®
        exclude_patterns = [
            '/site/', '/wp-admin', '/wp-content', '/wp-json', '/feed', '/rss',
            '.', '/page/', '/category/', '/tag/', '/author/', '/search',
            '/privacy', '/terms', '/contact', '/about'
        ]
        for pattern in exclude_patterns:
            if pattern in path:
                print(f"          âœ— é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³: {pattern}")
                return False
        
        # xgift.jp ã®æ§‹é€ ã‚’æŸ”è»Ÿã«åˆ¤å®š
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: /blog/è¨˜äº‹ã‚¹ãƒ©ãƒƒã‚°/ ã®å½¢å¼ï¼ˆAFFINGERå…¸å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
        if path.startswith('/blog/') and path != '/blog':
            article_path = path[6:]  # '/blog/' ã‚’é™¤å»
            if article_path and '/' not in article_path.rstrip('/'):
                clean_path = article_path.rstrip('/')
                if clean_path and len(clean_path) > 2:  # çŸ­ã™ãã‚‹ã‚¹ãƒ©ãƒƒã‚°ã¯é™¤å¤–
                    print(f"          âœ“ ãƒ‘ã‚¿ãƒ¼ãƒ³1: /blog/è¨˜äº‹ã‚¹ãƒ©ãƒƒã‚°/")
                    return True
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒ«ãƒ¼ãƒˆç›´ä¸‹ã®è¨˜äº‹ã‚¹ãƒ©ãƒƒã‚°ï¼ˆä»–ã‚µã‚¤ãƒˆã¨åŒæ§˜ï¼‰
        if (path.startswith('/') and len(path) > 1 and 
            '/' not in path[1:].rstrip('/') and
            not path.startswith('/blog')):
            clean_path = path[1:].rstrip('/')
            if clean_path and len(clean_path) > 3:  # çŸ­ã™ãã‚‹ã‚¹ãƒ©ãƒƒã‚°ã¯é™¤å¤–
                # è‹±æ•°å­—ãƒ»ãƒã‚¤ãƒ•ãƒ³ãƒ»ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã®ã¿è¨±å¯
                if all(c.isalnum() or c in '-_' for c in clean_path):
                    print(f"          âœ“ ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒ«ãƒ¼ãƒˆç›´ä¸‹è¨˜äº‹")
                    return True
        
        print(f"          âœ— è¨˜äº‹ãƒšãƒ¼ã‚¸ã§ã¯ãªã„")
        return False

    def is_crawlable(self, url):
        """ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ã‹ã©ã†ã‹åˆ¤å®šï¼ˆAFFINGERæœ€é©åŒ–ï¼‰"""
        path = urlparse(self.normalize_url(url)).path.lower()
        
        # é™¤å¤–ï¼šæ˜ã‚‰ã‹ã«ä¸è¦ãªã‚‚ã®
        exclude_words = ['/site/', '/wp-admin', '/wp-content', '/wp-json', '/feed', '/rss',
                        '.jpg', '.png', '.css', '.js', '.pdf', '.zip', '.svg', '.ico']
        if any(word in path for word in exclude_words):
            return False
        
        # è¨±å¯ï¼šãƒ–ãƒ­ã‚°é–¢é€£ + è¨˜äº‹ãƒšãƒ¼ã‚¸ + ãƒ«ãƒ¼ãƒˆãƒšãƒ¼ã‚¸
        return (path.startswith('/blog') or self.is_article_page(url) or path in ['/', ''])

    def extract_links(self, soup, current_url):
        """ãƒªãƒ³ã‚¯æŠ½å‡ºï¼ˆAFFINGERæœ€é©åŒ–ãƒ»ãƒ‡ãƒãƒƒã‚°å¼·åŒ–ç‰ˆï¼‰"""
        all_links = soup.find_all('a', href=True)
        print(f"    å…¨aã‚¿ã‚°æ•°: {len(all_links)}")
        
        links = []
        crawlable_count = 0
        article_count = 0
        
        for a in all_links:
            href = a.get('href', '').strip()
            if href and not href.startswith('#'):
                absolute = urljoin(current_url, href)
                if 'xgift.jp' in absolute:
                    is_crawl = self.is_crawlable(absolute)
                    is_article = self.is_article_page(absolute)
                    
                    if is_crawl:
                        crawlable_count += 1
                        links.append(self.normalize_url(absolute))
                    if is_article:
                        article_count += 1
                    
                    # è©³ç´°ãƒ‡ãƒãƒƒã‚°ã¯æœ€åˆã®10ãƒªãƒ³ã‚¯ã®ã¿è¡¨ç¤º
                    if len([l for l in [is_crawl, is_article] if l]) > 0 and article_count <= 10:
                        print(f"      ãƒªãƒ³ã‚¯: {href}")
                        print(f"        çµ¶å¯¾URL: {absolute}")
                        print(f"        ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½: {is_crawl}")
                        print(f"        è¨˜äº‹ãƒšãƒ¼ã‚¸: {is_article}")
        
        print(f"    çµæœ: å…¨{len(all_links)}â†’ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½{crawlable_count}â†’è¨˜äº‹{article_count}â†’æ¡ç”¨{len(set(links))}")
        return list(set(links))

    def extract_content_links(self, soup, current_url):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢ã‹ã‚‰ãƒªãƒ³ã‚¯æŠ½å‡ºï¼ˆAFFINGERæœ€é©åŒ–ï¼‰"""
        # AFFINGERç‰¹æœ‰ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’å„ªå…ˆ
        content_selectors = [
            '.entry-content',       # AFFINGERæ¨™æº–
            '.post-content',        # æŠ•ç¨¿ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            '.main-content',        # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            'main',                 # HTML5ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯
            'article',              # è¨˜äº‹è¦ç´ 
            '.content',             # æ±ç”¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            '.single-content',      # å˜ä¸€è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            '[class*="content"]'    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç³»ã‚¯ãƒ©ã‚¹
        ]
        
        content = None
        for selector in content_selectors:
            content = soup.select_one(selector)
            if content:
                print(f"        ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢æ¤œå‡º: {selector}")
                break
        
        if not content:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šbodyå…¨ä½“ï¼ˆãŸã ã—é™¤å¤–ã‚¨ãƒªã‚¢ã‚’å‰Šé™¤ï¼‰
            content = soup.find('body')
            if content:
                # AFFINGERç‰¹æœ‰ã®é™¤å¤–ã‚¨ãƒªã‚¢
                for remove_sel in ['nav', 'header', 'footer', '.navigation', '.menu', 
                                 '.sidebar', '.widget', '.ads', '.related', '.author-info']:
                    for elem in content.select(remove_sel):
                        elem.decompose()
                print(f"        ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: bodyä½¿ç”¨ï¼ˆé™¤å¤–ã‚¨ãƒªã‚¢å‰Šé™¤æ¸ˆã¿ï¼‰")
        
        if not content:
            return []

        links = []
        for a in content.find_all('a', href=True):
            href = a.get('href', '').strip()
            text = a.get_text(strip=True) or ''
            if href and not href.startswith('#') and text:
                absolute = urljoin(current_url, href)
                if 'xgift.jp' in absolute and self.is_article_page(absolute):
                    links.append({
                        'url': self.normalize_url(absolute), 
                        'anchor_text': text[:100]
                    })
        return links

    def auto_start_analysis(self):
        """EXEåŒ–å¯¾å¿œï¼šè‡ªå‹•ã§åˆ†æé–‹å§‹"""
        print("=== è‡ªå‹•åˆ†æé–‹å§‹ ===")
        self.status_label.configure(text="è‡ªå‹•åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
        self.start_analysis()

    def start_analysis(self):
        self.analyze_btn.configure(state="disabled")
        threading.Thread(target=self.analyze, daemon=True).start()

    def analyze(self):
        try:
            self.pages, self.links, self.detailed_links = {}, [], []
            visited, to_visit = set(), [self.normalize_url(self.url_entry.get())]
            
            # AFFINGERå¯¾å¿œã®è¿½åŠ URL
            additional_urls = [
                'https://xgift.jp/blog/page/2/',
                'https://xgift.jp/blog/page/3/',
                'https://xgift.jp/blog/category/',
                'https://xgift.jp/',  # ãƒ«ãƒ¼ãƒˆãƒšãƒ¼ã‚¸ã‚‚ç¢ºèª
            ]
            to_visit.extend(additional_urls)

            session = requests.Session()
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })

            print("=== xgift.jp (AFFINGER) åˆ†æé–‹å§‹ ===")

            # ãƒ•ã‚§ãƒ¼ã‚º1: ãƒšãƒ¼ã‚¸åé›†
            max_pages = 600  # AFTINGERã‚µã‚¤ãƒˆã¯è¨˜äº‹æ•°ãŒå¤šã„å¯èƒ½æ€§
            while to_visit and len(self.pages) < max_pages:
                url = to_visit.pop(0)
                if url in visited: continue
                
                try:
                    print(f"å‡¦ç†ä¸­: {url}")
                    response = session.get(url, timeout=15)
                    if response.status_code != 200: 
                        print(f"  HTTPã‚¨ãƒ©ãƒ¼: {response.status_code}")
                        continue
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # NOINDEXãƒã‚§ãƒƒã‚¯
                    robots = soup.find('meta', attrs={'name': 'robots'})
                    if robots and 'noindex' in robots.get('content', '').lower():
                        print(f"  NOINDEXã‚’ã‚¹ã‚­ãƒƒãƒ—")
                        continue
                    
                    # ãƒ–ãƒ­ã‚°ä¸€è¦§ãƒšãƒ¼ã‚¸ãƒ»ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã¯åé›†ã®ã¿
                    if (('/blog' in url and not self.is_article_page(url)) or 
                        '/page/' in url or '/category/' in url):
                        page_links = self.extract_links(soup, url)
                        new_links = [l for l in page_links if l not in visited and l not in to_visit]
                        to_visit.extend(new_links)
                        visited.add(url)
                        print(f"  ä¸€è¦§ãƒšãƒ¼ã‚¸: {len(new_links)}ä»¶ã®æ–°è¦ãƒªãƒ³ã‚¯")
                        continue
                    
                    # å€‹åˆ¥è¨˜äº‹ãƒšãƒ¼ã‚¸ã‚’ä¿å­˜
                    if self.is_article_page(url):
                        # AFFINGERå¯¾å¿œã®ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
                        title = None
                        for selector in ['h1.entry-title', 'h1', '.post-title', '.entry-title']:
                            title_elem = soup.select_one(selector)
                            if title_elem:
                                title = title_elem.get_text(strip=True)
                                break
                        
                        if not title:
                            title = soup.title.get_text(strip=True) if soup.title else url
                        
                        # ã‚µã‚¤ãƒˆåé™¤å»ï¼ˆXGIFTç”¨ï¼‰
                        title = re.sub(r'\s*[|\-]\s*.*(xgift|XGIFT|ã‚¨ãƒƒã‚¯ã‚¹ã‚®ãƒ•ãƒˆ).*$', '', title, flags=re.IGNORECASE)
                        title = title.strip()
                        
                        self.pages[url] = {'title': title, 'outbound_links': []}
                        
                        # æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹
                        page_links = self.extract_links(soup, url)
                        new_links = [l for l in page_links if l not in visited and l not in to_visit]
                        to_visit.extend(new_links)
                        
                        print(f"  è¨˜äº‹: {title}")
                        print(f"  æ–°è¦: {len(new_links)}ä»¶")
                    
                    visited.add(url)
                    
                    self.root.after(0, lambda: self.status_label.configure(text=f"åé›†ä¸­: {len(self.pages)}è¨˜äº‹"))
                    self.root.after(0, lambda: self.progress.set(len(self.pages) / max_pages))
                    time.sleep(0.2)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                    
                except Exception as e:
                    print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
                    continue

            print(f"=== ãƒ•ã‚§ãƒ¼ã‚º1å®Œäº†: {len(self.pages)}è¨˜äº‹ ===")

            # ãƒ•ã‚§ãƒ¼ã‚º2: ãƒªãƒ³ã‚¯é–¢ä¿‚æ§‹ç¯‰
            print("=== ãƒ•ã‚§ãƒ¼ã‚º2: ãƒªãƒ³ã‚¯é–¢ä¿‚æ§‹ç¯‰ ===")
            processed = set()
            
            for i, url in enumerate(list(self.pages.keys())):
                try:
                    if i % 25 == 0:
                        print(f"ãƒªãƒ³ã‚¯è§£æé€²æ—: {i}/{len(self.pages)}")
                        self.root.after(0, lambda: self.status_label.configure(text=f"ãƒªãƒ³ã‚¯è§£æä¸­: {i}/{len(self.pages)}"))
                    
                    response = session.get(url, timeout=10)
                    if response.status_code != 200: continue
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    content_links = self.extract_content_links(soup, url)
                    
                    for link_data in content_links:
                        target = link_data['url']
                        if target in self.pages and target != url:
                            link_key = (url, target)
                            if link_key not in processed:
                                processed.add(link_key)
                                self.links.append((url, target))
                                self.pages[url]['outbound_links'].append(target)
                                self.detailed_links.append({
                                    'source_url': url, 'source_title': self.pages[url]['title'],
                                    'target_url': target, 'anchor_text': link_data['anchor_text']
                                })
                except Exception as e:
                    print(f"ãƒªãƒ³ã‚¯è§£æã‚¨ãƒ©ãƒ¼: {e}")
                    continue

            # è¢«ãƒªãƒ³ã‚¯æ•°è¨ˆç®—
            for url in self.pages:
                self.pages[url]['inbound_links'] = sum(1 for s, t in self.links if t == url)

            print(f"=== åˆ†æå®Œäº†: {len(self.pages)}è¨˜äº‹, {len(self.links)}ãƒªãƒ³ã‚¯ ===")
            self.root.after(0, self.show_results)
            
            # å…¨è‡ªå‹•åŒ–ï¼šåˆ†æå®Œäº†å¾Œã«è‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            if self.pages:  # ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿
                self.root.after(2000, self.auto_export_csv)  # 2ç§’å¾Œã«è‡ªå‹•ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            
        except Exception as e:
            print(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            self.root.after(0, lambda: messagebox.showerror("ã‚¨ãƒ©ãƒ¼", str(e)))
        finally:
            self.root.after(0, lambda: self.analyze_btn.configure(state="normal"))

    def show_results(self):
        for w in self.result_frame.winfo_children(): w.destroy()
        
        if not self.pages:
            ctk.CTkLabel(self.result_frame, text="ãƒ‡ãƒ¼ã‚¿ãªã—").pack()
            return

        total = len(self.pages)
        isolated = sum(1 for p in self.pages.values() if p['inbound_links'] == 0)
        popular = sum(1 for p in self.pages.values() if p['inbound_links'] >= 5)
        
        ctk.CTkLabel(self.result_frame, 
                    text=f"ğŸ“Š {total}è¨˜äº‹ | {len(self.links)}ãƒªãƒ³ã‚¯ | å­¤ç«‹{isolated}ä»¶ | äººæ°—{popular}ä»¶", 
                    font=ctk.CTkFont(size=16, weight="bold")).pack(pady=10)

        sorted_pages = sorted(self.pages.items(), key=lambda x: x[1]['inbound_links'], reverse=True)
        
        for i, (url, info) in enumerate(sorted_pages):
            inbound = info['inbound_links']
            outbound = len(info['outbound_links'])
            
            if inbound == 0:
                eval_text = "ğŸš¨è¦æ”¹å–„"
            elif inbound >= 10:
                eval_text = "ğŸ†è¶…äººæ°—"
            elif inbound >= 5:
                eval_text = "âœ…äººæ°—"
            else:
                eval_text = "âš ï¸æ™®é€š"
                
            text = f"{i+1}. {info['title'][:50]}...\n   è¢«ãƒªãƒ³ã‚¯:{inbound} | ç™ºãƒªãƒ³ã‚¯:{outbound} | {eval_text}\n   {url}"
            ctk.CTkLabel(self.result_frame, text=text, anchor="w", font=ctk.CTkFont(size=10)).pack(fill="x", padx=10, pady=1)

    def export_csv(self):
        if not self.pages: return messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "ãƒ‡ãƒ¼ã‚¿ãªã—")
        filename = filedialog.asksaveasfilename(defaultextension=".csv", filetypes=[("CSV", "*.csv")])
        if not filename: return
        
        with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow(['é †ä½', 'ã‚¿ã‚¤ãƒˆãƒ«', 'URL', 'è¢«ãƒªãƒ³ã‚¯æ•°', 'ç™ºãƒªãƒ³ã‚¯æ•°', 'è©•ä¾¡'])
            
            sorted_pages = sorted(self.pages.items(), key=lambda x: x[1]['inbound_links'], reverse=True)
            
            for i, (url, info) in enumerate(sorted_pages, 1):
                inbound = info['inbound_links']
                
                if inbound == 0:
                    evaluation = "è¦æ”¹å–„"
                elif inbound >= 10:
                    evaluation = "è¶…äººæ°—"
                elif inbound >= 5:
                    evaluation = "äººæ°—"
                else:
                    evaluation = "æ™®é€š"
                
                writer.writerow([i, info['title'], url, inbound, len(info['outbound_links']), evaluation])
        
        messagebox.showinfo("å®Œäº†", "CSVä¿å­˜å®Œäº†")

    def auto_export_csv(self):
        """å…¨è‡ªå‹•åŒ–ï¼šè‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆEXEåŒ–å¯¾å¿œï¼‰"""
        import os
        from datetime import datetime
        
        try:
            # EXEåŒ–å¯¾å¿œï¼šå®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
            if hasattr(sys, '_MEIPASS'):
                # PyInstallerã§EXEåŒ–ã•ã‚ŒãŸå ´åˆ
                base_dir = os.path.dirname(sys.executable)
            else:
                # é€šå¸¸ã®Pythonå®Ÿè¡Œã®å ´åˆ
                base_dir = os.path.dirname(os.path.abspath(__file__))
            
            # æ—¥æœ¬ã®å½“æ—¥æ—¥ä»˜ã§ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
            today = datetime.now()
            date_folder = today.strftime("%Y-%m-%d")  # 2025-06-20 å½¢å¼
            folder_path = os.path.join(base_dir, date_folder)
            
            # ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
            if not os.path.exists(folder_path):
                os.makedirs(folder_path)
                print(f"æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ: {date_folder}")
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«åã¨ãƒ‘ã‚¹
            csv_filename = f"xgift-{date_folder}.csv"
            filename = os.path.join(folder_path, csv_filename)
            
            if not self.detailed_links:
                print("è©³ç´°ãƒ‡ãƒ¼ã‚¿ãªã— - CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—")
                return
            
            with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                writer.writerow(['ç•ªå·', 'ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'URL', 'è¢«ãƒªãƒ³ã‚¯å…ƒã‚¿ã‚¤ãƒˆãƒ«', 'è¢«ãƒªãƒ³ã‚¯å…ƒURL', 'ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'])
                
                # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
                targets = {}
                for link in self.detailed_links:
                    target = link['target_url']
                    targets.setdefault(target, []).append(link)
                
                # è¢«ãƒªãƒ³ã‚¯æ•°ã§ã‚½ãƒ¼ãƒˆ
                sorted_targets = sorted(targets.items(), key=lambda x: len(x[1]), reverse=True)
                
                row = 1
                for target, links_list in sorted_targets:
                    title = self.pages.get(target, {}).get('title', target)
                    for link in links_list:
                        writer.writerow([row, title, target, link['source_title'], link['source_url'], link['anchor_text']])
                        row += 1
                
                # å­¤ç«‹ãƒšãƒ¼ã‚¸ã‚‚è¿½åŠ 
                for url, info in self.pages.items():
                    if info['inbound_links'] == 0:
                        writer.writerow([row, info['title'], url, '', '', ''])
                        row += 1
            
            print(f"=== è‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {filename} ===")
            self.status_label.configure(text=f"åˆ†æå®Œäº†ï¼CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {os.path.basename(filename)}")
            
        except Exception as e:
            print(f"è‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            self.status_label.configure(text="åˆ†æå®Œäº†ï¼ˆCSVä¿å­˜ã‚¨ãƒ©ãƒ¼ï¼‰")

    def show_results(self):
        for w in self.result_frame.winfo_children(): w.destroy()
        
        if not self.pages:
            ctk.CTkLabel(self.result_frame, text="ãƒ‡ãƒ¼ã‚¿ãªã—").pack()
            return

        total = len(self.pages)
        isolated = sum(1 for p in self.pages.values() if p['inbound_links'] == 0)
        popular = sum(1 for p in self.pages.values() if p['inbound_links'] >= 5)
        
        ctk.CTkLabel(self.result_frame, 
                    text=f"ğŸ“Š {total}è¨˜äº‹ | {len(self.links)}ãƒªãƒ³ã‚¯ | å­¤ç«‹{isolated}ä»¶ | äººæ°—{popular}ä»¶", 
                    font=ctk.CTkFont(size=16, weight="bold")).pack(pady=10)

        sorted_pages = sorted(self.pages.items(), key=lambda x: x[1]['inbound_links'], reverse=True)
        
        for i, (url, info) in enumerate(sorted_pages):
            inbound = info['inbound_links']
            outbound = len(info['outbound_links'])
            
            if inbound == 0:
                eval_text = "ğŸš¨è¦æ”¹å–„"
            elif inbound >= 10:
                eval_text = "ğŸ†è¶…äººæ°—"
            elif inbound >= 5:
                eval_text = "âœ…äººæ°—"
            else:
                eval_text = "âš ï¸æ™®é€š"
                
            text = f"{i+1}. {info['title'][:50]}...\n   è¢«ãƒªãƒ³ã‚¯:{inbound} | ç™ºãƒªãƒ³ã‚¯:{outbound} | {eval_text}\n   {url}"
            ctk.CTkLabel(self.result_frame, text=text, anchor="w", font=ctk.CTkFont(size=10)).pack(fill="x", padx=10, pady=1)

    def export_detailed(self):
        if not self.detailed_links: return messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "è©³ç´°ãƒ‡ãƒ¼ã‚¿ãªã—")
        filename = filedialog.asksaveasfilename(defaultextension=".csv", filetypes=[("CSV", "*.csv")])
        if not filename: return
        
        with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow(['ç•ªå·', 'ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'URL', 'è¢«ãƒªãƒ³ã‚¯å…ƒã‚¿ã‚¤ãƒˆãƒ«', 'è¢«ãƒªãƒ³ã‚¯å…ƒURL', 'ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'])
            
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            targets = {}
            for link in self.detailed_links:
                target = link['target_url']
                targets.setdefault(target, []).append(link)
            
            # è¢«ãƒªãƒ³ã‚¯æ•°ã§ã‚½ãƒ¼ãƒˆ
            sorted_targets = sorted(targets.items(), key=lambda x: len(x[1]), reverse=True)
            
            row = 1
            for target, links_list in sorted_targets:
                title = self.pages.get(target, {}).get('title', target)
                for link in links_list:
                    writer.writerow([row, title, target, link['source_title'], link['source_url'], link['anchor_text']])
                    row += 1
            
            # å­¤ç«‹ãƒšãƒ¼ã‚¸ã‚‚è¿½åŠ 
            for url, info in self.pages.items():
                if info['inbound_links'] == 0:
                    writer.writerow([row, info['title'], url, '', '', ''])
                    row += 1
        
        messagebox.showinfo("å®Œäº†", "è©³ç´°CSVä¿å­˜å®Œäº†")

    def run(self):
        self.root.mainloop()

if __name__ == "__main__":
    XGiftAnalyzer().run()
import tkinter as tk
from tkinter import messagebox, filedialog
import customtkinter as ctk
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import json
import time
import re
import threading
import webbrowser
from datetime import datetime
import csv
import sys
import os

ctk.set_appearance_mode("light")
ctk.set_default_color_theme("blue")

class MorePayAnalyzer:
    def __init__(self):
        self.root = ctk.CTk()
        self.root.title("ğŸ”— more-pay.jpå°‚ç”¨å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆå…¨è‡ªå‹•ç‰ˆï¼‰")
        self.root.geometry("1200x800")

        self.pages = {}
        self.links = []
        self.detailed_links = []
        self.analysis_data = None
        self.is_analyzing = False

        self.setup_ui()
        
        # è‡ªå‹•åˆ†æé–‹å§‹ï¼ˆEXEåŒ–å¯¾å¿œï¼‰
        self.root.after(1000, self.auto_start_analysis)  # 1ç§’å¾Œã«è‡ªå‹•é–‹å§‹

    def setup_ui(self):
        self.main_frame = ctk.CTkScrollableFrame(self.root)
        self.main_frame.pack(fill="both", expand=True, padx=20, pady=20)

        title_label = ctk.CTkLabel(self.main_frame, text="ğŸ”— more-pay.jpå°‚ç”¨å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆå…¨è‡ªå‹•ç‰ˆï¼‰",
                                   font=ctk.CTkFont(size=28, weight="bold"))
        title_label.pack(pady=20)

        self.url_entry = ctk.CTkEntry(self.main_frame, placeholder_text="https://more-pay.jp", width=500)
        self.url_entry.pack(pady=10)
        self.url_entry.insert(0, "https://more-pay.jp")

        self.analyze_button = ctk.CTkButton(self.main_frame, text="åˆ†æé–‹å§‹", command=self.start_analysis)
        self.analyze_button.pack(pady=10)

        self.status_label = ctk.CTkLabel(self.main_frame, text="")
        self.status_label.pack(pady=10)

        self.progress_bar = ctk.CTkProgressBar(self.main_frame, width=400)
        self.progress_bar.pack(pady=5)
        self.progress_bar.stop()

        self.result_frame = ctk.CTkScrollableFrame(self.main_frame, height=400)
        self.result_frame.pack(fill="both", expand=True, pady=20)

        # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒœã‚¿ãƒ³ï¼ˆè©³ç´°ã®ã¿ï¼‰
        ctk.CTkButton(self.main_frame, text="è©³ç´°CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ", command=self.export_detailed_csv).pack(pady=10)

    def auto_start_analysis(self):
        """EXEåŒ–å¯¾å¿œï¼šè‡ªå‹•ã§åˆ†æé–‹å§‹"""
        print("=== è‡ªå‹•åˆ†æé–‹å§‹ ===")
        self.status_label.configure(text="è‡ªå‹•åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
        self.start_analysis()

    def start_analysis(self):
        url = self.url_entry.get().strip()
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url

        self.analyze_button.configure(state="disabled")
        self.status_label.configure(text="ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­...")
        self.progress_bar.start()

        thread = threading.Thread(target=self.analyze_site, args=(url,))
        thread.daemon = True
        thread.start()

    def extract_from_sitemap(self, url):
        """ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‹ã‚‰URLã‚’æŠ½å‡ºï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        urls = set()
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            res = requests.get(url, timeout=10, headers=headers)
            res.raise_for_status()
            
            # XMLã¨ã—ã¦è§£æã‚’è©¦è¡Œ
            try:
                soup = BeautifulSoup(res.content, 'xml')
            except:
                soup = BeautifulSoup(res.content, 'html.parser')
            
            locs = soup.find_all('loc')
            if soup.find('sitemapindex'):
                # ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å ´åˆ
                for loc in locs:
                    child_sitemap_url = loc.text.strip()
                    urls.update(self.extract_from_sitemap(child_sitemap_url))
            else:
                # é€šå¸¸ã®ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®å ´åˆ
                for loc in locs:
                    loc_url = loc.text.strip()
                    if not re.search(r'sitemap.*\.(xml|html)$', loc_url.lower()):
                        normalized_url = self.normalize_url(loc_url)
                        if self.is_content(normalized_url):
                            urls.add(normalized_url)
        except Exception as e:
            print(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—å‡¦ç†ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
        
        return list(urls)

    def generate_seed_urls(self, base_url):
        """ã‚·ãƒ¼ãƒ‰URLã®ç”Ÿæˆï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        seed_urls = [self.normalize_url(base_url)]
        
        # è¤‡æ•°ã®ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
        sitemap_patterns = [
            '/sitemap.xml',
            '/sitemap_index.xml',
            '/wp-sitemap.xml',
            '/sitemap-posts.xml',
            '/post-sitemap.xml'
        ]
        
        for pattern in sitemap_patterns:
            sitemap_url = urljoin(base_url, pattern)
            try:
                sitemap_urls = self.extract_from_sitemap(sitemap_url)
                if sitemap_urls:
                    seed_urls.extend(sitemap_urls)
                    print(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ— {pattern} ã‹ã‚‰ {len(sitemap_urls)} å€‹ã®URLã‚’å–å¾—")
                    break  # æœ€åˆã«æˆåŠŸã—ãŸã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ä½¿ç”¨
            except Exception as e:
                print(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ— {pattern} ã®å–å¾—ã«å¤±æ•—: {e}")
        
        # æ‰‹å‹•ã§é‡è¦ãªãƒšãƒ¼ã‚¸ã‚’è¿½åŠ 
        manual_pages = [
            '/column/',
            '/category/',
        ]
        
        for page in manual_pages:
            manual_url = urljoin(base_url, page)
            normalized_manual = self.normalize_url(manual_url)
            if normalized_manual not in seed_urls:
                seed_urls.append(normalized_manual)
        
        # é‡è¤‡ã‚’é™¤å»
        unique_urls = []
        seen = set()
        for url in seed_urls:
            if url not in seen:
                seen.add(url)
                unique_urls.append(url)
        
        print(f"æœ€çµ‚ã‚·ãƒ¼ãƒ‰URLæ•°: {len(unique_urls)}")
        return unique_urls

    def is_content(self, url):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒšãƒ¼ã‚¸åˆ¤å®šï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        normalized_url = self.normalize_url(url)
        parsed = urlparse(normalized_url)
        path = parsed.path.lower().rstrip('/')
        
        # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯é™¤å¤–
        if parsed.query:
            return False
        
        # more-pay.jpå°‚ç”¨ï¼šè¨±å¯ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚ˆã‚ŠæŸ”è»Ÿã«ï¼‰
        allowed_patterns = [
            r'^/?$',                                    # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸
            r'^/column/?$',                             # ã‚³ãƒ©ãƒ ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸
            r'^/column/[a-z0-9\-_]+/?$',               # ã‚³ãƒ©ãƒ è¨˜äº‹ãƒšãƒ¼ã‚¸
            r'^/column/[a-z0-9\-_]+/[a-z0-9\-_]+/?$',  # ã‚³ãƒ©ãƒ ã‚µãƒ–ãƒšãƒ¼ã‚¸ï¼ˆsite/ãªã©ï¼‰
            r'^/category/[a-z0-9\-_]+/?$',             # ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸
            r'^/category/[a-z0-9\-_]+/page/\d+/?$',    # ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
            r'^/[a-z0-9\-_]+/?$',                      # ç›´ä¸‹ã®è¨˜äº‹ãƒšãƒ¼ã‚¸
            r'^/page/\d+/?$',                          # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
        ]
        
        if any(re.search(pattern, path) for pattern in allowed_patterns):
            # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
            excluded_patterns = [
                r'/sitemap', r'sitemap.*\.(xml|html)$', r'-mg/?$', r'/site/?$',
                r'/wp-', r'/tag/', r'/wp-content', r'/wp-admin', r'/wp-json',
                r'/feed/', r'/privacy', r'/terms', r'/contact', r'/about', 
                r'/company', r'/go-', r'/redirect', r'/exit', r'/out',
                r'/search', r'/author/', r'/date/', r'/\d{4}/', r'/\d{4}/\d{2}/',
                r'\.(jpg|jpeg|png|gif|webp|svg|ico|pdf|zip|rar|doc|docx|xls|xlsx)$'
            ]
            
            if not any(re.search(e, path) for e in excluded_patterns):
                return True
        
        return False

    def is_noindex_page(self, soup):
        """NOINDEXãƒšãƒ¼ã‚¸ã®åˆ¤å®šï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        try:
            # metaã‚¿ã‚°ã®robotsã‚’ãƒã‚§ãƒƒã‚¯
            meta_robots = soup.find('meta', attrs={'name': 'robots'})
            if meta_robots and meta_robots.get('content'):
                content = meta_robots.get('content').lower()
                if 'noindex' in content:
                    return True
            
            # googlebotã®metaã‚¿ã‚°ã‚’ãƒã‚§ãƒƒã‚¯
            noindex_meta = soup.find('meta', attrs={'name': 'googlebot', 'content': lambda x: x and 'noindex' in x.lower()})
            if noindex_meta:
                return True
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰åˆ¤å®š
            title = soup.find('title')
            if title and title.get_text():
                title_text = title.get_text().lower()
                noindex_keywords = ['å¤–éƒ¨ã‚µã‚¤ãƒˆ', 'ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ', 'ç§»å‹•ä¸­', 'å¤–éƒ¨ãƒªãƒ³ã‚¯', 'cushion', '404', 'not found', 'error']
                if any(keyword in title_text for keyword in noindex_keywords):
                    return True
            
            # bodyå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åˆ¤å®š
            body_text = soup.get_text().lower()[:1000]  # æœ€åˆã®1000æ–‡å­—ã®ã¿ãƒã‚§ãƒƒã‚¯
            noindex_phrases = [
                'å¤–éƒ¨ã‚µã‚¤ãƒˆã«ç§»å‹•ã—ã¾ã™', 'ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã—ã¦ã„ã¾ã™', 'å¤–éƒ¨ãƒªãƒ³ã‚¯ã§ã™',
                'åˆ¥ã‚µã‚¤ãƒˆã«ç§»å‹•', 'ã“ã®ãƒªãƒ³ã‚¯ã¯å¤–éƒ¨ã‚µã‚¤ãƒˆ', 'ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'
            ]
            if any(phrase in body_text for phrase in noindex_phrases):
                return True
                
            return False
            
        except Exception as e:
            print(f"NOINDEXãƒã‚§ãƒƒã‚¯ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def analyze_site(self, base_url):
        try:
            pages = {}
            links = []
            detailed_links = []
            visited = set()
            to_visit = self.generate_seed_urls(base_url)
            domain = urlparse(base_url).netloc
            processed_links = set()

            session = requests.Session()
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive'
            })

            print(f"=== more-pay.jp åˆ†æé–‹å§‹ ===")
            print(f"åˆæœŸã‚·ãƒ¼ãƒ‰URLæ•°: {len(to_visit)}")

            while to_visit and len(pages) < 1000:  # ä¸Šé™ã‚’å¢—åŠ 
                url = to_visit.pop(0)
                normalized_url = self.normalize_url(url)
                if normalized_url in visited:
                    continue

                try:
                    print(f"å‡¦ç†ä¸­: {normalized_url}")
                    response = session.get(url, timeout=15)  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’å»¶é•·
                    if response.status_code != 200:
                        print(f"HTTPã‚¨ãƒ©ãƒ¼ {response.status_code}: {url}")
                        continue
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    if self.is_noindex_page(soup):
                        print(f"NOINDEXãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒƒãƒ—: {url}")
                        visited.add(normalized_url)
                        continue
                    
                    extracted_links = self.extract_links(soup)
                    new_links_found = 0

                    title = self.extract_title(soup, normalized_url)
                    
                    pages[normalized_url] = {
                        'title': title,
                        'outbound_links': []
                    }

                    for link_data in extracted_links:
                        link_url = link_data['url']
                        anchor_text = link_data['anchor_text']
                        
                        # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                        try:
                            absolute_url = urljoin(normalized_url, link_url)
                            normalized_link = self.normalize_url(absolute_url)
                        except:
                            continue
                        
                        if (self.is_internal(normalized_link, domain) and 
                            self.is_content(normalized_link)):
                            
                            link_key = (normalized_url, normalized_link)
                            if link_key not in processed_links:
                                processed_links.add(link_key)
                                
                                links.append((normalized_url, normalized_link))
                                pages[normalized_url]['outbound_links'].append(normalized_link)
                                
                                detailed_links.append({
                                    'source_url': normalized_url,
                                    'source_title': title,
                                    'target_url': normalized_link,
                                    'anchor_text': anchor_text
                                })
                            
                            if (normalized_link not in visited and 
                                normalized_link not in to_visit):
                                to_visit.append(normalized_link)
                                new_links_found += 1

                    visited.add(normalized_url)

                    status_text = f"{len(pages)}ä»¶ç›®: {title[:50]}..."
                    if new_links_found > 0:
                        status_text += f" (æ–°è¦ãƒªãƒ³ã‚¯{new_links_found}ä»¶ç™ºè¦‹)"
                    
                    self.root.after(0, lambda text=status_text: self.status_label.configure(text=text))
                    self.root.after(0, lambda: self.progress_bar.set(min(len(pages) / 1000, 1.0)))

                    time.sleep(0.2)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’ç·©å’Œ

                except Exception as e:
                    print(f"ã‚¨ãƒ©ãƒ¼: {url} - {e}")
                    continue

            # è¢«ãƒªãƒ³ã‚¯æ•°ã‚’è¨ˆç®—
            for url in pages:
                pages[url]['inbound_links'] = sum(1 for _, tgt in links if tgt == url)

            self.pages = pages
            self.links = links
            self.detailed_links = detailed_links
            
            print(f"=== åˆ†æå®Œäº†: {len(pages)}ãƒšãƒ¼ã‚¸, {len(links)}ãƒªãƒ³ã‚¯ ===")
            self.root.after(0, self.show_results)
            
            # å…¨è‡ªå‹•åŒ–ï¼šåˆ†æå®Œäº†å¾Œã«è‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            if self.pages:  # ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿
                self.root.after(2000, self.auto_export_csv)  # 2ç§’å¾Œã«è‡ªå‹•ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            
        except Exception as e:
            self.root.after(0, lambda: messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ã‚µã‚¤ãƒˆåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"))
        finally:
            self.root.after(0, lambda: self.analyze_button.configure(state="normal"))
            self.root.after(0, lambda: self.progress_bar.stop())
            self.root.after(0, lambda: self.status_label.configure(text="åˆ†æå®Œäº†"))

    def extract_title(self, soup, url):
        """ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã®æŠ½å‡ºï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        # h1ã‚¿ã‚°ã‚’å„ªå…ˆ
        h1 = soup.find('h1')
        if h1 and h1.get_text(strip=True):
            return h1.get_text(strip=True)
        
        # titleã‚¿ã‚°
        title = soup.find('title')
        if title and title.string:
            title_text = title.string.strip()
            # ã‚µã‚¤ãƒˆåã‚’é™¤å»
            title_text = re.sub(r'\s*[|\-ï½œ]\s*.*$', '', title_text)
            if title_text:
                return title_text
        
        # og:titleã‚’è©¦è¡Œ
        og_title = soup.find('meta', property='og:title')
        if og_title and og_title.get('content'):
            return og_title.get('content').strip()
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return url

    def extract_links(self, soup):
        """ãƒªãƒ³ã‚¯ã®æŠ½å‡ºï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        # WordPressã®ä¸€èˆ¬çš„ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢ã‚»ãƒ¬ã‚¯ã‚¿
        content_selectors = [
            '.entry-content',          # ä¸€èˆ¬çš„ãªWordPressãƒ†ãƒ¼ãƒ
            '.post-content',           # ã‚«ã‚¹ã‚¿ãƒ ãƒ†ãƒ¼ãƒ
            '.article-content',        # è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            '.content',                # æ±ç”¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            'main .content',           # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            '[class*="content"]',      # contentã‚’å«ã‚€ã‚¯ãƒ©ã‚¹
            'main',                    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
            'article',                 # è¨˜äº‹ã‚¨ãƒªã‚¢
            '.single-content',         # å˜ä¸€è¨˜äº‹
            '.post-body',              # æŠ•ç¨¿æœ¬æ–‡
            '.entry-body'              # ã‚¨ãƒ³ãƒˆãƒªãƒ¼æœ¬æ–‡
        ]

        # é™¤å¤–ã™ã‚‹ã‚¨ãƒªã‚¢ã®ã‚»ãƒ¬ã‚¯ã‚¿
        exclude_selectors = [
            'header', 'footer', 'nav', 'aside', '.sidebar', '.widget', 
            '.share', '.related', '.popular-posts', '.breadcrumb', 
            '.author-box', '.navigation', '.sns', '.social-share',
            '.comment', '.comments', '.pagination', '.tags', '.categories',
            '.meta', '.byline', '.date', '.archive'
        ]

        for selector in content_selectors:
            areas = soup.select(selector)
            if areas:
                links = []
                for area in areas:
                    # é™¤å¤–ã‚¨ãƒªã‚¢ã‚’å‰Šé™¤
                    for exclude_selector in exclude_selectors:
                        for exclude_element in area.select(exclude_selector):
                            exclude_element.decompose()

                    # ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
                    found_links = area.find_all('a', href=True)
                    for link in found_links:
                        href = link.get('href', '').strip()
                        if not href or href.startswith('#'):
                            continue
                        
                        anchor_text = link.get_text(strip=True) or link.get('title', '') or ''
                        anchor_text = anchor_text[:200]  # é•·ã•ã‚’åˆ¶é™
                        
                        # ç„¡æ„å‘³ãªã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—
                        if anchor_text.lower() in ['', 'link', 'ãƒªãƒ³ã‚¯', 'click here', 'here', 'read more']:
                            continue
                        
                        links.append({
                            'url': href,
                            'anchor_text': anchor_text
                        })

                if links:
                    print(f"ã‚»ãƒ¬ã‚¯ã‚¿ '{selector}' ã§ {len(links)} å€‹ã®ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹")
                    return links

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
        print("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€å…¨ä½“ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º")
        all_links = []
        for link in soup.find_all('a', href=True):
            href = link.get('href', '').strip()
            if not href or href.startswith('#'):
                continue
            
            anchor_text = link.get_text(strip=True) or link.get('title', '') or ''
            anchor_text = anchor_text[:200]
            
            if anchor_text.lower() not in ['', 'link', 'ãƒªãƒ³ã‚¯', 'click here', 'here', 'read more']:
                all_links.append({
                    'url': href,
                    'anchor_text': anchor_text
                })
        
        return all_links

    def normalize_url(self, url):
        """URLæ­£è¦åŒ–ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        try:
            parsed = urlparse(url)
            scheme = parsed.scheme or 'https'
            netloc = parsed.netloc.replace('www.', '')
            path = parsed.path
            
            # ãƒ‘ã‚¹ã®æ­£è¦åŒ–
            if '/wp/' in path:
                path = path.replace('/wp/', '/')
            
            # é€£ç¶šã™ã‚‹ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’å˜ä¸€ã«
            path = re.sub(r'/+', '/', path)
            
            # æœ«å°¾ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’çµ±ä¸€ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ãŒã‚ã‚‹å ´åˆã¯é™¤ãï¼‰
            if path and not re.search(r'\.[a-zA-Z0-9]+$', path):
                if not path.endswith('/'):
                    path += '/'
            
            # ãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹ã®å ´åˆ
            if path == '':
                path = '/'
            
            return f"{scheme}://{netloc}{path}"
        except Exception as e:
            print(f"URLæ­£è¦åŒ–ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return url

    def is_internal(self, url, domain):
        """å†…éƒ¨ãƒªãƒ³ã‚¯åˆ¤å®š"""
        try:
            link_domain = urlparse(url).netloc.replace('www.', '')
            base_domain = domain.replace('www.', '')
            return link_domain == base_domain
        except:
            return False

    def show_results(self):
        self.result_frame.configure(height=400)
        for widget in self.result_frame.winfo_children():
            widget.destroy()

        total_pages = len(self.pages)
        total_links = len(self.links)
        isolated_pages = sum(1 for p in self.pages.values() if p['inbound_links'] == 0)
        popular_pages = sum(1 for p in self.pages.values() if p['inbound_links'] >= 5)

        stats_label = ctk.CTkLabel(self.result_frame,
                                   text=f"ğŸ“Š åˆ†æçµæœ: {total_pages}ãƒšãƒ¼ã‚¸ | {total_links}å†…éƒ¨ãƒªãƒ³ã‚¯ | å­¤ç«‹è¨˜äº‹{isolated_pages}ä»¶ | äººæ°—è¨˜äº‹{popular_pages}ä»¶",
                                   font=ctk.CTkFont(size=16, weight="bold"))
        stats_label.pack(pady=10)

        sorted_pages = sorted(self.pages.items(), key=lambda x: x[1]['inbound_links'], reverse=True)
        for i, (url, info) in enumerate(sorted_pages):
            if info['inbound_links'] == 0:
                evaluation = "ğŸš¨ è¦æ”¹å–„"
            elif info['inbound_links'] >= 10:
                evaluation = "ğŸ† è¶…äººæ°—"
            elif info['inbound_links'] >= 5:
                evaluation = "âœ… äººæ°—"
            else:
                evaluation = "âš ï¸ æ™®é€š"

            label_text = f"{i+1:3d}. {info['title'][:50]}...\n     è¢«ãƒªãƒ³ã‚¯:{info['inbound_links']:3d} | ç™ºãƒªãƒ³ã‚¯:{len(info['outbound_links']):3d} | {evaluation}\n     {url}"
            
            label = ctk.CTkLabel(self.result_frame,
                                 text=label_text,
                                 anchor="w", justify="left",
                                 font=ctk.CTkFont(size=11))
            label.pack(fill="x", padx=10, pady=2)

    def auto_export_csv(self):
        """å…¨è‡ªå‹•åŒ–ï¼šè‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆEXEåŒ–å¯¾å¿œï¼‰"""
        try:
            # EXEåŒ–å¯¾å¿œï¼šå®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
            if hasattr(sys, '_MEIPASS'):
                # PyInstallerã§EXEåŒ–ã•ã‚ŒãŸå ´åˆ
                base_dir = os.path.dirname(sys.executable)
            else:
                # é€šå¸¸ã®Pythonå®Ÿè¡Œã®å ´åˆ
                base_dir = os.path.dirname(os.path.abspath(__file__))
            
            # æ—¥æœ¬ã®å½“æ—¥æ—¥ä»˜ã§ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
            today = datetime.now()
            date_folder = today.strftime("%Y-%m-%d")  # 2025-06-20 å½¢å¼
            folder_path = os.path.join(base_dir, date_folder)
            
            # ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
            if not os.path.exists(folder_path):
                os.makedirs(folder_path)
                print(f"æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ: {date_folder}")
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«åã¨ãƒ‘ã‚¹
            csv_filename = f"more-pay-{date_folder}.csv"
            filename = os.path.join(folder_path, csv_filename)
            
            if not self.detailed_links:
                print("è©³ç´°ãƒ‡ãƒ¼ã‚¿ãªã— - CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—")
                return
            
            with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                writer.writerow([
                    'A_ç•ªå·', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL',
                    'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL', 'F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'
                ])
                
                target_groups = {}
                for link in self.detailed_links:
                    target_url = link['target_url']
                    if target_url not in target_groups:
                        target_groups[target_url] = []
                    target_groups[target_url].append(link)
                
                sorted_targets = sorted(target_groups.items(), key=lambda x: len(x[1]), reverse=True)
                
                row_number = 1
                for target_url, links_to_target in sorted_targets:
                    target_info = self.pages.get(target_url, {})
                    target_title = target_info.get('title', target_url)
                    
                    if links_to_target:
                        for link in links_to_target:
                            writer.writerow([
                                row_number,
                                target_title,
                                target_url,
                                link['source_title'],
                                link['source_url'],
                                link['anchor_text']
                            ])
                            row_number += 1
                
                for url, info in self.pages.items():
                    if info['inbound_links'] == 0:
                        writer.writerow([
                            row_number,
                            info['title'],
                            url,
                            '',
                            '',
                            ''
                        ])
                        row_number += 1
            
            print(f"=== è‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {folder_path}/{csv_filename} ===")
            self.status_label.configure(text=f"åˆ†æå®Œäº†ï¼CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {csv_filename}")
            
        except Exception as e:
            print(f"è‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            self.status_label.configure(text="åˆ†æå®Œäº†ï¼ˆCSVä¿å­˜ã‚¨ãƒ©ãƒ¼ï¼‰")

    def export_detailed_csv(self):
        if not self.detailed_links:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            return

        filename = filedialog.asksaveasfilename(defaultextension=".csv", filetypes=[("CSV files", "*.csv")])
        if filename:
            try:
                with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.writer(f)
                    writer.writerow([
                        'A_ç•ªå·', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL',
                        'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL', 'F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'
                    ])
                    
                    target_groups = {}
                    for link in self.detailed_links:
                        target_url = link['target_url']
                        if target_url not in target_groups:
                            target_groups[target_url] = []
                        target_groups[target_url].append(link)
                    
                    sorted_targets = sorted(target_groups.items(), key=lambda x: len(x[1]), reverse=True)
                    
                    row_number = 1
                    for target_url, links_to_target in sorted_targets:
                        target_info = self.pages.get(target_url, {})
                        target_title = target_info.get('title', target_url)
                        
                        if links_to_target:
                            for link in links_to_target:
                                writer.writerow([
                                    row_number,
                                    target_title,
                                    target_url,
                                    link['source_title'],
                                    link['source_url'],
                                    link['anchor_text']
                                ])
                                row_number += 1
                    
                    for url, info in self.pages.items():
                        if info['inbound_links'] == 0:
                            writer.writerow([
                                row_number,
                                info['title'],
                                url,
                                '',
                                '',
                                ''
                            ])
                            row_number += 1
                
                messagebox.showinfo("å®Œäº†", f"è©³ç´°CSVã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
            except Exception as e:
                messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ä¿å­˜ã«å¤±æ•—: {e}")

    def run(self):
        self.root.mainloop()

def main():
    app = MorePayAnalyzer()
    app.run()

if __name__ == "__main__":
    main()
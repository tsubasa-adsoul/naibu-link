import tkinter as tk
from tkinter import messagebox, filedialog
import customtkinter as ctk
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import json
import time
import re
import threading
import webbrowser
from datetime import datetime
import csv
import sys
import os

ctk.set_appearance_mode("light")
ctk.set_default_color_theme("blue")

class FriendPayAnalyzer:
    def __init__(self):
        self.root = ctk.CTk()
        self.root.title("ğŸ”— friendpayå°‚ç”¨å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆæ”¹ä¿®ç‰ˆãƒ»onclickå¯¾å¿œï¼‰")
        self.root.geometry("1200x800")
        self.pages = {}
        self.links = []
        self.detailed_links = []  # è©³ç´°ãƒªãƒ³ã‚¯æƒ…å ±ç”¨
        self.analysis_data = None
        self.is_analyzing = False
        self.excluded_links_count = 0  # é™¤å¤–ã•ã‚ŒãŸãƒªãƒ³ã‚¯æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        
        self.setup_ui()
        
        # è‡ªå‹•åˆ†æé–‹å§‹ï¼ˆEXEåŒ–å¯¾å¿œï¼‰
        self.root.after(1000, self.auto_start_analysis)  # 1ç§’å¾Œã«è‡ªå‹•é–‹å§‹

    def setup_ui(self):
        self.main_frame = ctk.CTkScrollableFrame(self.root)
        self.main_frame.pack(fill="both", expand=True, padx=20, pady=20)

        title_label = ctk.CTkLabel(self.main_frame, text="ğŸ”— friendpayå°‚ç”¨å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆæ”¹ä¿®ç‰ˆãƒ»onclickå¯¾å¿œï¼‰",
                                   font=ctk.CTkFont(size=28, weight="bold"))
        title_label.pack(pady=20)

        self.url_entry = ctk.CTkEntry(self.main_frame, placeholder_text="https://friendpay.jp", width=500)
        self.url_entry.pack(pady=10)
        self.url_entry.insert(0, "https://friendpay.jp")

        self.analyze_button = ctk.CTkButton(self.main_frame, text="åˆ†æé–‹å§‹", command=self.start_analysis)
        self.analyze_button.pack(pady=10)

        self.status_label = ctk.CTkLabel(self.main_frame, text="")
        self.status_label.pack(pady=10)

        self.progress_bar = ctk.CTkProgressBar(self.main_frame, width=400)
        self.progress_bar.pack(pady=5)
        self.progress_bar.stop()

        self.result_frame = ctk.CTkScrollableFrame(self.main_frame, height=400)
        self.result_frame.pack(fill="both", expand=True, pady=20)

        # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒœã‚¿ãƒ³
        ctk.CTkButton(self.main_frame, text="è©³ç´°CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ", command=self.export_detailed_csv).pack(pady=10)

    def detect_site_type(self, url):
        """ã‚µã‚¤ãƒˆã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š"""
        domain = urlparse(url).netloc.lower()
        
        if 'friendpay.jp' in domain or 'friend-pay.com' in domain:
            return 'friend_pay'
        elif 'kaitori-life.co.jp' in domain or 'kaitori-life.com' in domain:
            return 'kaitori_life'
        elif 'kau-ru.co.jp' in domain:
            return 'kauru'
        elif 'kurekaeru.com' in domain:
            return 'kurekaeru'
        else:
            return 'unknown'

    def get_exclude_selectors(self, site_type):
        """ã‚µã‚¤ãƒˆã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸé™¤å¤–ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è¿”ã™"""
        base_selectors = ['header', 'footer', 'nav', 'aside', '.sidebar', '.widget', '.share', '.related', '.popular-posts', '.breadcrumb', '.author-box', '.navigation']
        
        site_specific_selectors = {
            'friend_pay': [
                '.l-header__inner.l-container',   # ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒšã‚¤: ãƒ˜ãƒƒãƒ€ãƒ¼å†…éƒ¨
                '.l-footer__nav',                 # ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒšã‚¤: ãƒ•ãƒƒã‚¿ãƒ¼ãƒŠãƒ“
                '.c-tabBody.p-postListTabBody'   # ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒšã‚¤: ã‚¿ãƒ–ãƒœãƒ‡ã‚£
            ],
            'kaitori_life': [
                '#nav-container.header-style4-animate.animate'  # è²·å–LIFE: ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
            ],
            'kauru': [
                '.textwidget',                    # ã‚«ã‚¦ãƒ¼ãƒ«: textwidget
                '.textwidget.custom-html-widget'  # ã‚«ã‚¦ãƒ¼ãƒ«: custom-html-widgetä»˜ãtextwidget
            ],
            'kurekaeru': [
                '#gnav.l-header__gnav.c-gnavWrap'  # ã‚¯ãƒ¬ã‹ãˆã‚‹: ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒŠãƒ“
            ]
        }
        
        return base_selectors + site_specific_selectors.get(site_type, [])

    def auto_start_analysis(self):
        """EXEåŒ–å¯¾å¿œï¼šè‡ªå‹•ã§åˆ†æé–‹å§‹"""
        print("=== friendpay.jp è‡ªå‹•åˆ†æé–‹å§‹ ===")
        self.status_label.configure(text="è‡ªå‹•åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
        self.start_analysis()

    def start_analysis(self):
        url = self.url_entry.get().strip()
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        # ã‚µã‚¤ãƒˆã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
        site_type = self.detect_site_type(url)
        print(f"ã‚µã‚¤ãƒˆã‚¿ã‚¤ãƒ—åˆ¤å®š: {site_type}")
        
        self.analyze_button.configure(state="disabled")
        self.status_label.configure(text="ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­...")
        self.progress_bar.start()
        self.excluded_links_count = 0  # ãƒªã‚»ãƒƒãƒˆ
        
        thread = threading.Thread(target=self.analyze_site, args=(url,))
        thread.daemon = True
        thread.start()

    def extract_from_sitemap(self, url):
        urls = set()
        try:
            res = requests.get(url, timeout=10)
            res.raise_for_status()
            soup = BeautifulSoup(res.content, 'xml')
            locs = soup.find_all('loc')
            if soup.find('sitemapindex'):
                for loc in locs:
                    urls.update(self.extract_from_sitemap(loc.text.strip()))
            else:
                for loc in locs:
                    loc_url = loc.text.strip()
                    # ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«è‡ªä½“ã¯é™¤å¤–
                    if not re.search(r'sitemap.*\.(xml|html)$', loc_url.lower()):
                        urls.add(self.normalize_url(loc_url))
        except Exception as e:
            print(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return list(urls)

    def generate_seed_urls(self, base_url):
        seed_urls = [self.normalize_url(base_url)]
        
        # ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‹ã‚‰URLå–å¾—
        sitemap_root = urljoin(base_url, '/sitemap.xml')
        sitemap_urls = self.extract_from_sitemap(sitemap_root)
        seed_urls.extend(sitemap_urls)
        
        print(f"=== {urlparse(base_url).netloc} åˆ†æé–‹å§‹ ===")
        print(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‹ã‚‰ {len(sitemap_urls)} å€‹ã®URLã‚’å–å¾—")
        print(f"ç·ã‚·ãƒ¼ãƒ‰URLæ•°: {len(seed_urls)}")
        return list(set(seed_urls))

    def is_content(self, url):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒšãƒ¼ã‚¸ã‹ã©ã†ã‹åˆ¤å®š"""
        if not url:
            return False
            
        try:
            # æ­£è¦åŒ–å¾Œã®URLã§åˆ¤å®š
            normalized_url = self.normalize_url(url)
            path = urlparse(normalized_url).path.lower().split('?')[0].rstrip('/')
            
            # ã¾ãšé‡è¦ãªãƒšãƒ¼ã‚¸ã‚’æ˜ç¤ºçš„ã«è¨±å¯
            if any(re.search(pattern, path) for pattern in [
                r'^/category/[a-z0-9\-]+$',           # ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸
                r'^/category/[a-z0-9\-]+/page/\d+$',  # ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
                r'^/[a-z0-9\-]+$',                    # è¨˜äº‹ãƒšãƒ¼ã‚¸
                r'^/$',                               # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸
                r'^$'                                 # ãƒ«ãƒ¼ãƒˆ
            ]):
                return True
            
            # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
            exclude_patterns = [
                r'/sitemap', r'sitemap.*\.(xml|html)$', r'/page/\d+$', r'-mg',
                r'/site/', r'/wp-', r'/tag/', r'/wp-content', r'/wp-admin', r'/wp-json',
                r'#', r'\?utm_', r'/feed/', r'mailto:', r'tel:', r'/privacy', r'/terms',
                r'/contact', r'/go-', r'/redirect', r'/exit', r'/out',
                r'\.(jpg|jpeg|png|gif|webp|svg|ico|pdf|zip|rar|doc|docx|xls|xlsx)$'
            ]
            
            for pattern in exclude_patterns:
                if re.search(pattern, path):
                    return False
            
            return True
            
        except Exception as e:
            print(f"is_contentåˆ¤å®šã‚¨ãƒ©ãƒ¼: {url} -> {e}")
            return False

    def is_noindex_page(self, soup):
        """NOINDEXãƒšãƒ¼ã‚¸ã‹ã©ã†ã‹åˆ¤å®š"""
        try:
            # robots meta ã‚¿ã‚°ã‚’ãƒã‚§ãƒƒã‚¯
            meta_robots = soup.find('meta', attrs={'name': 'robots'})
            if meta_robots and meta_robots.get('content'):
                content = meta_robots.get('content').lower()
                if 'noindex' in content:
                    return True
            
            # ã‚¯ãƒƒã‚·ãƒ§ãƒ³ãƒšãƒ¼ã‚¸ã®å…¸å‹çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
            title = soup.find('title')
            if title and title.get_text():
                title_text = title.get_text().lower()
                if any(keyword in title_text for keyword in ['å¤–éƒ¨ã‚µã‚¤ãƒˆ', 'ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ', 'ç§»å‹•ä¸­', 'å¤–éƒ¨ãƒªãƒ³ã‚¯', 'cushion']):
                    return True
            
            # bodyã«è­¦å‘Šãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆ
            body_text = soup.get_text().lower()
            if any(phrase in body_text for phrase in [
                'å¤–éƒ¨ã‚µã‚¤ãƒˆã«ç§»å‹•ã—ã¾ã™',
                'ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã—ã¦ã„ã¾ã™',
                'å¤–éƒ¨ãƒªãƒ³ã‚¯ã§ã™',
                'åˆ¥ã‚µã‚¤ãƒˆã«ç§»å‹•',
                'ã“ã®ãƒªãƒ³ã‚¯ã¯å¤–éƒ¨ã‚µã‚¤ãƒˆ'
            ]):
                return True
                
            return False
            
        except Exception as e:
            print(f"NOINDEXãƒã‚§ãƒƒã‚¯ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def normalize_url(self, url):
        """URLæ­£è¦åŒ–å‡¦ç†ï¼ˆãƒ•ãƒ¬ãƒ³ãƒ‰ãƒšã‚¤å¯¾å¿œï¼‰"""
        if not url:
            return ""
            
        # ç›¸å¯¾URLã®å‡¦ç†
        if url.startswith('/'):
            try:
                base_domain = f"{urlparse(self.url_entry.get()).scheme}://{urlparse(self.url_entry.get()).netloc}"
                url = base_domain + url
            except:
                return ""
        
        try:
            parsed = urlparse(url)
            scheme = 'https'
            netloc = parsed.netloc.replace('www.', '')
            path = parsed.path.rstrip('/')

            # friendpay.jpå°‚ç”¨ã®æ­£è¦åŒ–
            if '/wp/' in path:
                path = path.replace('/wp/', '/')

            # é‡è¤‡ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’ä¿®æ­£
            path = re.sub(r'/+', '/', path)

            # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ä»¥å¤–ã«ã¯æœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’ä»˜ä¸
            if path and not path.endswith('/'):
                path += '/'

            return f"{scheme}://{netloc}{path}"
        except Exception as e:
            print(f"URLæ­£è¦åŒ–ã‚¨ãƒ©ãƒ¼: {url} -> {e}")
            return ""

    def is_internal(self, url, domain):
        try:
            return urlparse(url).netloc.replace('www.', '') == domain.replace('www.', '')
        except:
            return False

    def analyze_site(self, base_url):
        try:
            pages = {}
            links = []
            detailed_links = []  # è©³ç´°ãƒªãƒ³ã‚¯æƒ…å ±ç”¨
            visited = set()
            to_visit = self.generate_seed_urls(base_url)
            domain = urlparse(base_url).netloc
            processed_links = set()  # é‡è¤‡é™¤å»ç”¨
            site_type = self.detect_site_type(base_url)

            session = requests.Session()
            session.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'})

            # é‡è¤‡é™¤å»
            unique_to_visit = []
            seen = set()
            for url in to_visit:
                normalized = self.normalize_url(url)
                if normalized and normalized not in seen:
                    seen.add(normalized)
                    unique_to_visit.append(normalized)
            
            to_visit = unique_to_visit
            print(f"é‡è¤‡é™¤å»å¾Œã®ã‚·ãƒ¼ãƒ‰URLæ•°: {len(to_visit)}")

            while to_visit and len(pages) < 500:
                url = to_visit.pop(0)
                normalized_url = self.normalize_url(url)
                if not normalized_url or normalized_url in visited:
                    continue

                try:
                    print(f"å‡¦ç†ä¸­: {normalized_url}")
                    response = session.get(url, timeout=15)
                    if response.status_code != 200:
                        print(f"  HTTPã‚¨ãƒ©ãƒ¼: {response.status_code}")
                        continue
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # NOINDEXãƒšãƒ¼ã‚¸ã‚’é™¤å¤–
                    if self.is_noindex_page(soup):
                        print(f"  NOINDEXãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒƒãƒ—: {url}")
                        visited.add(normalized_url)
                        continue
                    
                    extracted_links = self.extract_links(soup, site_type)
                    new_links_found = 0

                    # ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’ä¿å­˜
                    title = soup.title.string.strip() if soup.title and soup.title.string else normalized_url
                    # ã‚µã‚¤ãƒˆåé™¤å»
                    for site_name in ['friendpay', 'ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒšã‚¤', 'friend-pay']:
                        title = re.sub(rf'\s*[|\-]\s*.*{re.escape(site_name)}.*$', '', title, flags=re.IGNORECASE)
                    title = title.strip()
                    
                    pages[normalized_url] = {'title': title, 'outbound_links': []}

                    # ã“ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ã®å†…éƒ¨ãƒªãƒ³ã‚¯ã‚’è¨˜éŒ²
                    for link in extracted_links:
                        normalized_link = self.normalize_url(link['url'])
                        if not normalized_link:
                            continue
                            
                        anchor_text = link['anchor_text']
                        
                        if (self.is_internal(normalized_link, domain) and 
                            self.is_content(normalized_link)):
                            
                            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆåŒã˜ã‚½ãƒ¼ã‚¹â†’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ãƒªãƒ³ã‚¯ã¯1å›ã ã‘ï¼‰
                            link_key = (normalized_url, normalized_link)
                            if link_key not in processed_links:
                                processed_links.add(link_key)
                                
                                links.append((normalized_url, normalized_link))
                                pages[normalized_url]['outbound_links'].append(normalized_link)
                                
                                # è©³ç´°ãƒªãƒ³ã‚¯æƒ…å ±ã‚’ä¿å­˜
                                detailed_links.append({
                                    'source_url': normalized_url,
                                    'source_title': title,
                                    'target_url': normalized_link,
                                    'anchor_text': anchor_text
                                })
                        
                        # æ–°è¦URLã®ç™ºè¦‹
                        if (self.is_internal(normalized_link, domain) and
                            self.is_content(normalized_link) and
                            normalized_link not in visited and
                            normalized_link not in to_visit):
                            to_visit.append(normalized_link)
                            new_links_found += 1

                    visited.add(normalized_url)

                    status_text = f"{len(pages)}ä»¶ç›®: {title[:50]}..."
                    if new_links_found > 0:
                        status_text += f" (æ–°è¦ãƒªãƒ³ã‚¯{new_links_found}ä»¶ç™ºè¦‹)"
                    
                    self.root.after(0, lambda text=status_text: self.status_label.configure(text=text))
                    self.root.after(0, lambda: self.progress_bar.set(min(len(pages) / 500, 1.0)))
                    time.sleep(0.1)

                except Exception as e:
                    print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
                    continue

            # è¢«ãƒªãƒ³ã‚¯æ•°ã‚’è¨ˆç®—
            for url in pages:
                pages[url]['inbound_links'] = len(set([src for src, tgt in links if tgt == url]))

            self.pages = pages
            self.links = links
            self.detailed_links = detailed_links
            
            print(f"=== åˆ†æå®Œäº†: {len(pages)}ãƒšãƒ¼ã‚¸, {len(links)}ãƒªãƒ³ã‚¯, é™¤å¤–{self.excluded_links_count}ãƒªãƒ³ã‚¯ ===")
            self.root.after(0, self.show_results)
            
            # å…¨è‡ªå‹•åŒ–ï¼šåˆ†æå®Œäº†å¾Œã«è‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            if self.pages:  # ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿
                self.root.after(2000, self.auto_export_csv)  # 2ç§’å¾Œã«è‡ªå‹•ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            
        except Exception as e:
            self.root.after(0, lambda: messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ã‚µã‚¤ãƒˆåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"))
        finally:
            self.root.after(0, lambda: self.analyze_button.configure(state="normal"))
            self.root.after(0, lambda: self.progress_bar.stop())
            self.root.after(0, lambda: self.status_label.configure(text="åˆ†æå®Œäº†"))

    def extract_links(self, soup, site_type):
        """ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒšã‚¤å°‚ç”¨ã®æœ€é©åŒ–ã•ã‚ŒãŸãƒªãƒ³ã‚¯æŠ½å‡ºï¼ˆã‚µã‚¤ãƒˆåˆ¥é™¤å¤–å¯¾å¿œãƒ»onclickå¯¾å¿œç‰ˆï¼‰"""
        selectors = [
            '.post_content',           # ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒšã‚¤å°‚ç”¨
            '.entry-content',          # WordPressä¸€èˆ¬
            '.article-content',        # è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            'main .content',           # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            '[class*="content"]',      # content ã‚’å«ã‚€ã‚¯ãƒ©ã‚¹
            'main',                    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
            'article'                  # è¨˜äº‹ã‚¨ãƒªã‚¢
        ]
        
        exclude_selectors = self.get_exclude_selectors(site_type)
        
        print(f"  ã‚µã‚¤ãƒˆã‚¿ã‚¤ãƒ— {site_type} ã®é™¤å¤–ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼: {exclude_selectors}")
        
        for selector in selectors:
            areas = soup.select(selector)
            if areas:
                links = []
                for area in areas:
                    # ã‚µã‚¤ãƒˆåˆ¥é™¤å¤–å‡¦ç†
                    excluded_count = 0
                    for exclude_selector in exclude_selectors:
                        excluded_elements = area.select(exclude_selector)
                        for elem in excluded_elements:
                            excluded_count += len(elem.find_all('a', href=True))
                            excluded_count += len(elem.find_all(attrs={'onclick': True}))
                            elem.decompose()
                    
                    if excluded_count > 0:
                        print(f"    é™¤å¤–ã—ãŸãƒªãƒ³ã‚¯æ•°: {excluded_count}")
                        self.excluded_links_count += excluded_count
                    
                    # 1. é€šå¸¸ã®aã‚¿ã‚°ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
                    found_links = area.find_all('a', href=True)
                    for link in found_links:
                        anchor_text = link.get_text(strip=True) or link.get('title', '') or '[ãƒªãƒ³ã‚¯]'
                        links.append({
                            'url': link['href'],
                            'anchor_text': anchor_text[:100]
                        })
                    
                    # 2. onclickå±æ€§ã®ã‚ã‚‹ã‚¿ã‚°ã‚’æŠ½å‡ºï¼ˆãƒ•ãƒ¬ãƒ³ãƒ‰ãƒšã‚¤ç‰¹æœ‰ï¼‰
                    onclick_elements = area.find_all(attrs={'onclick': True})
                    for element in onclick_elements:
                        onclick_attr = element.get('onclick', '')
                        # "window.location.href='URL'" ã‹ã‚‰URLã‚’æŠ½å‡º
                        url_match = re.search(r"window\.location\.href\s*=\s*['\"]([^'\"]+)['\"]", onclick_attr)
                        if url_match:
                            extracted_url = url_match.group(1)
                            anchor_text = element.get_text(strip=True) or element.get('title', '') or '[onclick ãƒªãƒ³ã‚¯]'
                            links.append({
                                'url': extracted_url,
                                'anchor_text': anchor_text[:100]
                            })
                            print(f"    onclick ãƒªãƒ³ã‚¯æ¤œå‡º: {extracted_url} (ã‚¢ãƒ³ã‚«ãƒ¼: {anchor_text})")
                
                if links:
                    print(f"  ã‚»ãƒ¬ã‚¯ã‚¿ '{selector}' ã§ {len(links)} å€‹ã®ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹")
                    return links
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        print("  ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã§ãƒªãƒ³ã‚¯æŠ½å‡º")
        all_links = []
        
        # é€šå¸¸ã®aã‚¿ã‚°
        for link in soup.find_all('a', href=True):
            anchor_text = link.get_text(strip=True) or link.get('title', '') or '[ãƒªãƒ³ã‚¯]'
            all_links.append({
                'url': link['href'],
                'anchor_text': anchor_text[:100]
            })
        
        # onclickå±æ€§ã®ã‚¿ã‚°
        onclick_elements = soup.find_all(attrs={'onclick': True})
        for element in onclick_elements:
            onclick_attr = element.get('onclick', '')
            url_match = re.search(r"window\.location\.href\s*=\s*['\"]([^'\"]+)['\"]", onclick_attr)
            if url_match:
                extracted_url = url_match.group(1)
                anchor_text = element.get_text(strip=True) or element.get('title', '') or '[onclick ãƒªãƒ³ã‚¯]'
                all_links.append({
                    'url': extracted_url,
                    'anchor_text': anchor_text[:100]
                })
        
        return all_links

    def show_results(self):
        for widget in self.result_frame.winfo_children():
            widget.destroy()

        total_pages = len(self.pages)
        total_links = len(self.links)
        isolated_pages = sum(1 for p in self.pages.values() if p['inbound_links'] == 0)
        popular_pages = sum(1 for p in self.pages.values() if p['inbound_links'] >= 5)

        stats_label = ctk.CTkLabel(self.result_frame,
                                   text=f"ğŸ“Š åˆ†æçµæœ: {total_pages}ãƒšãƒ¼ã‚¸ | {total_links}å†…éƒ¨ãƒªãƒ³ã‚¯ | é™¤å¤–{self.excluded_links_count}ãƒªãƒ³ã‚¯ | å­¤ç«‹è¨˜äº‹{isolated_pages}ä»¶ | äººæ°—è¨˜äº‹{popular_pages}ä»¶",
                                   font=ctk.CTkFont(size=16, weight="bold"))
        stats_label.pack(pady=10)

        sorted_pages = sorted(self.pages.items(), key=lambda x: x[1]['inbound_links'], reverse=True)
        for i, (url, info) in enumerate(sorted_pages):
            if info['inbound_links'] == 0:
                evaluation = "ğŸš¨ è¦æ”¹å–„"
            elif info['inbound_links'] >= 10:
                evaluation = "ğŸ† è¶…äººæ°—"
            elif info['inbound_links'] >= 5:
                evaluation = "âœ… äººæ°—"
            else:
                evaluation = "âš ï¸ æ™®é€š"

            label_text = f"{i+1:3d}. {info['title'][:50]}...\n     è¢«ãƒªãƒ³ã‚¯:{info['inbound_links']:3d} | ç™ºãƒªãƒ³ã‚¯:{len(info['outbound_links']):3d} | {evaluation}\n     {url}"
            
            label = ctk.CTkLabel(self.result_frame, text=label_text, anchor="w", justify="left", font=ctk.CTkFont(size=11))
            label.pack(fill="x", padx=10, pady=2)

    def auto_export_csv(self):
        """å…¨è‡ªå‹•åŒ–ï¼šè‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆEXEåŒ–å¯¾å¿œï¼‰"""
        try:
            # EXEåŒ–å¯¾å¿œï¼šå®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
            if hasattr(sys, '_MEIPASS'):
                # PyInstallerã§EXEåŒ–ã•ã‚ŒãŸå ´åˆ
                base_dir = os.path.dirname(sys.executable)
            else:
                # é€šå¸¸ã®Pythonå®Ÿè¡Œã®å ´åˆ
                base_dir = os.path.dirname(os.path.abspath(__file__))
            
            # æ—¥æœ¬ã®å½“æ—¥æ—¥ä»˜ã§ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
            today = datetime.now()
            date_folder = today.strftime("%Y-%m-%d")  # 2025-06-20 å½¢å¼
            folder_path = os.path.join(base_dir, date_folder)
            
            # ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
            if not os.path.exists(folder_path):
                os.makedirs(folder_path)
                print(f"æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ: {date_folder}")
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«åã¨ãƒ‘ã‚¹
            csv_filename = f"friendpay-{date_folder}.csv"
            filename = os.path.join(folder_path, csv_filename)
            
            if not self.detailed_links:
                print("è©³ç´°ãƒ‡ãƒ¼ã‚¿ãªã— - CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—")
                return
            
            with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œï¼ˆ6åˆ—æ§‹æˆï¼‰
                writer.writerow([
                    'A_ç•ªå·', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL',
                    'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL', 'F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'
                ])
                
                # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURLåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
                target_groups = {}
                for link in self.detailed_links:
                    target_url = link['target_url']
                    if target_url not in target_groups:
                        target_groups[target_url] = []
                    target_groups[target_url].append(link)
                
                # è¢«ãƒªãƒ³ã‚¯æ•°ã§ã‚½ãƒ¼ãƒˆ
                sorted_targets = sorted(target_groups.items(), key=lambda x: len(x[1]), reverse=True)
                
                # ãƒšãƒ¼ã‚¸ç•ªå·ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆï¼ˆåŒã˜ã‚¿ã‚¤ãƒˆãƒ«+URLã«ã¯åŒã˜ç•ªå·ï¼‰
                page_numbers = {}
                current_page_number = 1
                
                # è¢«ãƒªãƒ³ã‚¯ã‚ã‚Šãƒšãƒ¼ã‚¸ã®ç•ªå·å‰²ã‚Šå½“ã¦
                for target_url, links_to_target in sorted_targets:
                    target_info = self.pages.get(target_url, {})
                    target_title = target_info.get('title', target_url)
                    page_key = (target_title, target_url)
                    
                    if page_key not in page_numbers:
                        page_numbers[page_key] = current_page_number
                        current_page_number += 1
                
                # å­¤ç«‹ãƒšãƒ¼ã‚¸ã®ç•ªå·å‰²ã‚Šå½“ã¦
                for url, info in self.pages.items():
                    if info['inbound_links'] == 0:
                        page_key = (info['title'], url)
                        if page_key not in page_numbers:
                            page_numbers[page_key] = current_page_number
                            current_page_number += 1
                
                # è¢«ãƒªãƒ³ã‚¯ã‚ã‚Šãƒšãƒ¼ã‚¸ã®å‡ºåŠ›
                for target_url, links_to_target in sorted_targets:
                    target_info = self.pages.get(target_url, {})
                    target_title = target_info.get('title', target_url)
                    page_key = (target_title, target_url)
                    page_number = page_numbers[page_key]
                    
                    if links_to_target:
                        # è¢«ãƒªãƒ³ã‚¯ãŒã‚ã‚‹å ´åˆã€å„è¢«ãƒªãƒ³ã‚¯ã‚’1è¡Œãšã¤å‡ºåŠ›ï¼ˆåŒã˜ãƒšãƒ¼ã‚¸ç•ªå·ï¼‰
                        for link in links_to_target:
                            writer.writerow([
                                page_number,                     # A_ç•ªå·ï¼ˆçµ±ä¸€ï¼‰
                                target_title,                    # B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                                target_url,                      # C_URL
                                link['source_title'],            # D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                                link['source_url'],              # E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL
                                link['anchor_text']              # F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
                            ])
                
                # å­¤ç«‹ãƒšãƒ¼ã‚¸ï¼ˆè¢«ãƒªãƒ³ã‚¯ãŒ0ã®ï¼‰ã®å‡ºåŠ›
                for url, info in self.pages.items():
                    if info['inbound_links'] == 0:
                        page_key = (info['title'], url)
                        page_number = page_numbers[page_key]
                        
                        writer.writerow([
                            page_number,        # A_ç•ªå·
                            info['title'],      # B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                            url,               # C_URL
                            '',                # D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                            '',                # E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL
                            ''                 # F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
                        ])
            
            print(f"=== è‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {folder_path}/{csv_filename} ===")
            print(f"ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒšãƒ¼ã‚¸æ•°: {len(page_numbers)}")
            self.status_label.configure(text=f"åˆ†æå®Œäº†ï¼CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {csv_filename}")
            
        except Exception as e:
            print(f"è‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            self.status_label.configure(text="åˆ†æå®Œäº†ï¼ˆCSVä¿å­˜ã‚¨ãƒ©ãƒ¼ï¼‰")

    def export_detailed_csv(self):
        """è©³ç´°CSVå‡ºåŠ›ï¼ˆè¢«ãƒªãƒ³ã‚¯è©³ç´°ï¼‰"""
        if not self.detailed_links:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            return

        today = datetime.now().strftime("%Y-%m-%d")
        default_filename = f"friendpay-{today}.csv"

        filename = filedialog.asksaveasfilename(
            defaultextension=".csv", 
            filetypes=[("CSV files", "*.csv")],
            initialvalue=default_filename
        )
        
        if filename:
            try:
                with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.writer(f)
                    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œï¼ˆ6åˆ—æ§‹æˆï¼‰
                    writer.writerow([
                        'A_ç•ªå·', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL',
                        'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL', 'F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'
                    ])
                    
                    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURLåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
                    target_groups = {}
                    for link in self.detailed_links:
                        target_url = link['target_url']
                        if target_url not in target_groups:
                            target_groups[target_url] = []
                        target_groups[target_url].append(link)
                    
                    # è¢«ãƒªãƒ³ã‚¯æ•°ã§ã‚½ãƒ¼ãƒˆ
                    sorted_targets = sorted(target_groups.items(), key=lambda x: len(x[1]), reverse=True)
                    
                    # ãƒšãƒ¼ã‚¸ç•ªå·ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆï¼ˆåŒã˜ã‚¿ã‚¤ãƒˆãƒ«+URLã«ã¯åŒã˜ç•ªå·ï¼‰
                    page_numbers = {}
                    current_page_number = 1
                    
                    # è¢«ãƒªãƒ³ã‚¯ã‚ã‚Šãƒšãƒ¼ã‚¸ã®ç•ªå·å‰²ã‚Šå½“ã¦
                    for target_url, links_to_target in sorted_targets:
                        target_info = self.pages.get(target_url, {})
                        target_title = target_info.get('title', target_url)
                        page_key = (target_title, target_url)
                        
                        if page_key not in page_numbers:
                            page_numbers[page_key] = current_page_number
                            current_page_number += 1
                    
                    # å­¤ç«‹ãƒšãƒ¼ã‚¸ã®ç•ªå·å‰²ã‚Šå½“ã¦
                    for url, info in self.pages.items():
                        if info['inbound_links'] == 0:
                            page_key = (info['title'], url)
                            if page_key not in page_numbers:
                                page_numbers[page_key] = current_page_number
                                current_page_number += 1
                    
                    # è¢«ãƒªãƒ³ã‚¯ã‚ã‚Šãƒšãƒ¼ã‚¸ã®å‡ºåŠ›
                    for target_url, links_to_target in sorted_targets:
                        target_info = self.pages.get(target_url, {})
                        target_title = target_info.get('title', target_url)
                        page_key = (target_title, target_url)
                        page_number = page_numbers[page_key]
                        
                        if links_to_target:
                            # è¢«ãƒªãƒ³ã‚¯ãŒã‚ã‚‹å ´åˆã€å„è¢«ãƒªãƒ³ã‚¯ã‚’1è¡Œãšã¤å‡ºåŠ›ï¼ˆåŒã˜ãƒšãƒ¼ã‚¸ç•ªå·ï¼‰
                            for link in links_to_target:
                                writer.writerow([
                                    page_number,                     # A_ç•ªå·ï¼ˆçµ±ä¸€ï¼‰
                                    target_title,                    # B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                                    target_url,                      # C_URL
                                    link['source_title'],            # D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                                    link['source_url'],              # E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL
                                    link['anchor_text']              # F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
                                ])
                    
                    # å­¤ç«‹ãƒšãƒ¼ã‚¸ï¼ˆè¢«ãƒªãƒ³ã‚¯ãŒ0ã®ï¼‰ã®å‡ºåŠ›
                    for url, info in self.pages.items():
                        if info['inbound_links'] == 0:
                            page_key = (info['title'], url)
                            page_number = page_numbers[page_key]
                            
                            writer.writerow([
                                page_number,        # A_ç•ªå·
                                info['title'],      # B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                                url,               # C_URL
                                '',                # D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                                '',                # E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL
                                ''                 # F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
                            ])
                
                messagebox.showinfo("å®Œäº†", f"è©³ç´°CSVã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}\nãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒšãƒ¼ã‚¸æ•°: {len(page_numbers)}")
            except Exception as e:
                messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ä¿å­˜ã«å¤±æ•—: {e}")

    def run(self):
        self.root.mainloop()

def main():
    app = FriendPayAnalyzer()
    app.run()

if __name__ == "__main__":
    main()
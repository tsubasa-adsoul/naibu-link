import tkinter as tk
from tkinter import messagebox, filedialog
import customtkinter as ctk
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import time
import re
import threading
import csv
import sys
import os
from datetime import datetime

ctk.set_appearance_mode("light")
ctk.set_default_color_theme("blue")

class PayfulAnalyzer:
    def __init__(self):
        self.root = ctk.CTk()
        self.root.title("ğŸ”— pay-ful.jpå°‚ç”¨å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆå…¨è‡ªå‹•ç‰ˆï¼‰")
        self.root.geometry("1200x800")
        
        self.pages = {}
        self.links = []
        self.detailed_links = []
        
        self.setup_ui()
        
        # è‡ªå‹•åˆ†æé–‹å§‹ï¼ˆEXEåŒ–å¯¾å¿œï¼‰
        self.root.after(1000, self.auto_start_analysis)  # 1ç§’å¾Œã«è‡ªå‹•é–‹å§‹

    def setup_ui(self):
        main_frame = ctk.CTkScrollableFrame(self.root)
        main_frame.pack(fill="both", expand=True, padx=20, pady=20)

        # ã‚¿ã‚¤ãƒˆãƒ«
        ctk.CTkLabel(main_frame, text="ğŸ”— pay-ful.jpå°‚ç”¨å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆå…¨è‡ªå‹•ç‰ˆï¼‰", 
                    font=ctk.CTkFont(size=28, weight="bold")).pack(pady=20)

        # URLå…¥åŠ›
        self.url_entry = ctk.CTkEntry(main_frame, placeholder_text="https://pay-ful.jp/media/", width=500)
        self.url_entry.pack(pady=10)
        self.url_entry.insert(0, "https://pay-ful.jp/media/")

        # åˆ†æãƒœã‚¿ãƒ³
        self.analyze_btn = ctk.CTkButton(main_frame, text="åˆ†æé–‹å§‹", command=self.start_analysis)
        self.analyze_btn.pack(pady=10)

        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
        self.status_label = ctk.CTkLabel(main_frame, text="")
        self.status_label.pack(pady=10)

        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
        self.progress = ctk.CTkProgressBar(main_frame, width=400)
        self.progress.pack(pady=5)

        # çµæœã‚¨ãƒªã‚¢
        self.result_frame = ctk.CTkScrollableFrame(main_frame, height=400)
        self.result_frame.pack(fill="both", expand=True, pady=20)

        # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒœã‚¿ãƒ³ï¼ˆè©³ç´°ã®ã¿ï¼‰
        ctk.CTkButton(main_frame, text="è©³ç´°CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ", command=self.export_detailed).pack(pady=10)

    def auto_start_analysis(self):
        """EXEåŒ–å¯¾å¿œï¼šè‡ªå‹•ã§åˆ†æé–‹å§‹"""
        print("=== è‡ªå‹•åˆ†æé–‹å§‹ ===")
        self.status_label.configure(text="è‡ªå‹•åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
        self.start_analysis()

    def normalize_url(self, url):
        if not url: return ""
        try:
            parsed = urlparse(url)
            return f"https://{parsed.netloc.replace('www.', '')}{parsed.path.rstrip('/')}"
        except:
            return url

    def is_valid_page(self, url):
        """å€‹åˆ¥è¨˜äº‹ãƒšãƒ¼ã‚¸ã®ã¿ã‚’è¨±å¯ï¼ˆè¨˜äº‹ä¸€è¦§ãƒšãƒ¼ã‚¸ã¯é™¤å¤–ï¼‰"""
        path = urlparse(self.normalize_url(url)).path.lower()
        
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
        if any(x in path for x in ['/site/', '/wp-', '/feed', '/page/', '.', '/media']):
            return False
        
        # è¨±å¯ï¼šãƒ«ãƒ¼ãƒˆç›´ä¸‹ã®è¨˜äº‹ãƒšãƒ¼ã‚¸ã®ã¿ï¼ˆ/è¨˜äº‹å ã®å½¢å¼ï¼‰
        return path.startswith('/') and '/' not in path[1:] and len(path) > 1

    def is_crawlable_page(self, url):
        """ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ãƒšãƒ¼ã‚¸åˆ¤å®šï¼ˆè¨˜äº‹ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚‚å«ã‚€ï¼‰"""
        path = urlparse(self.normalize_url(url)).path.lower()
        
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
        if any(x in path for x in ['/site/', '/wp-', '/feed', '.']):
            return False
        
        # è¨±å¯ï¼šè¨˜äº‹ä¸€è¦§ãƒšãƒ¼ã‚¸ + å€‹åˆ¥è¨˜äº‹ãƒšãƒ¼ã‚¸
        return (path == '/media' or                                    # è¨˜äº‹ä¸€è¦§ãƒšãƒ¼ã‚¸
                path.startswith('/media/page/') or                     # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
                (path.startswith('/') and '/' not in path[1:] and len(path) > 1))  # å€‹åˆ¥è¨˜äº‹

    def extract_links(self, soup, current_url):
        """ã‚¯ãƒ­ãƒ¼ãƒ«ç”¨ãƒªãƒ³ã‚¯æŠ½å‡ºï¼ˆè¨˜äº‹ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚‚å«ã‚€ï¼‰"""
        links = []
        for a in soup.find_all('a', href=True):
            href = a.get('href', '').strip()
            if href and not href.startswith('#'):
                absolute = urljoin(current_url, href)
                if 'pay-ful.jp' in absolute and self.is_crawlable_page(absolute):
                    links.append(self.normalize_url(absolute))
        return list(set(links))  # é‡è¤‡é™¤å»

    def extract_content_links(self, soup, current_url):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢ã‹ã‚‰ã®ãƒªãƒ³ã‚¯æŠ½å‡ºï¼ˆå€‹åˆ¥è¨˜äº‹ãƒšãƒ¼ã‚¸ã®ã¿ï¼‰"""
        content = soup.select_one('.entry-content, .post-content, main, article')
        if not content:
            return []
        
        links = []
        for a in content.find_all('a', href=True):
            href = a.get('href', '').strip()
            text = a.get_text(strip=True) or ''
            if href and not href.startswith('#') and text and '/site/' not in href:
                absolute = urljoin(current_url, href)
                if 'pay-ful.jp' in absolute and self.is_valid_page(absolute):  # å€‹åˆ¥è¨˜äº‹ã®ã¿
                    links.append({'url': self.normalize_url(absolute), 'anchor_text': text[:100]})
        return links

    def start_analysis(self):
        self.analyze_btn.configure(state="disabled")
        threading.Thread(target=self.analyze, daemon=True).start()

    def analyze(self):
        try:
            self.pages, self.links, self.detailed_links = {}, [], []
            visited, to_visit = set(), [self.normalize_url(self.url_entry.get())]
            
            # åˆæœŸãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
            for i in range(2, 5):
                to_visit.append(f"https://pay-ful.jp/media/page/{i}/")

            session = requests.Session()
            session.headers.update({'User-Agent': 'Mozilla/5.0'})

            print("=== pay-ful.jp åˆ†æé–‹å§‹ ===")

            # ãƒ•ã‚§ãƒ¼ã‚º1: ãƒšãƒ¼ã‚¸åé›†
            while to_visit and len(self.pages) < 500:
                url = to_visit.pop(0)
                if url in visited: continue
                
                try:
                    print(f"å‡¦ç†ä¸­: {url}")
                    response = session.get(url, timeout=10)
                    if response.status_code != 200: 
                        print(f"  HTTPã‚¨ãƒ©ãƒ¼: {response.status_code}")
                        continue
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # NOINDEXãƒã‚§ãƒƒã‚¯
                    robots = soup.find('meta', attrs={'name': 'robots'})
                    if robots and 'noindex' in robots.get('content', '').lower():
                        print(f"  NOINDEXã‚’ã‚¹ã‚­ãƒƒãƒ—")
                        continue
                    
                    # è¨˜äº‹ä¸€è¦§ãƒšãƒ¼ã‚¸ã¨ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã¯åé›†ã®ã¿ã€ä¿å­˜ã—ãªã„
                    if '/page/' in url or url.endswith('/media'):
                        page_links = self.extract_links(soup, url)
                        new_links = [l for l in page_links if l not in visited and l not in to_visit]
                        to_visit.extend(new_links)
                        visited.add(url)
                        print(f"  ä¸€è¦§ãƒšãƒ¼ã‚¸: {len(new_links)}ä»¶ã®æ–°è¦ãƒªãƒ³ã‚¯")
                        continue
                    
                    # è¨˜äº‹ãƒšãƒ¼ã‚¸ã‚’ä¿å­˜
                    if self.is_valid_page(url):
                        title = soup.find('h1')
                        title = title.get_text(strip=True) if title else url
                        # ã‚µã‚¤ãƒˆåé™¤å»
                        title = re.sub(r'\s*[|\-]\s*.*(pay-ful|ãƒšã‚¤ãƒ•ãƒ«).*$', '', title, flags=re.IGNORECASE)
                        title = title.strip()
                        
                        self.pages[url] = {'title': title, 'outbound_links': []}
                        
                        # æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹
                        page_links = self.extract_links(soup, url)
                        new_links = [l for l in page_links if l not in visited and l not in to_visit]
                        to_visit.extend(new_links)
                        
                        print(f"  è¨˜äº‹: {title}")
                        print(f"  æ–°è¦: {len(new_links)}ä»¶")
                    
                    visited.add(url)
                    
                    self.root.after(0, lambda: self.status_label.configure(text=f"åé›†ä¸­: {len(self.pages)}è¨˜äº‹"))
                    self.root.after(0, lambda: self.progress.set(len(self.pages) / 500))
                    time.sleep(0.1)
                    
                except Exception as e:
                    print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
                    continue

            print(f"=== ãƒ•ã‚§ãƒ¼ã‚º1å®Œäº†: {len(self.pages)}è¨˜äº‹ ===")

            # ãƒ•ã‚§ãƒ¼ã‚º2: ãƒªãƒ³ã‚¯é–¢ä¿‚æ§‹ç¯‰ï¼ˆå€‹åˆ¥è¨˜äº‹ãƒšãƒ¼ã‚¸é–“ã®ã¿ï¼‰
            print("=== ãƒ•ã‚§ãƒ¼ã‚º2: ãƒªãƒ³ã‚¯é–¢ä¿‚æ§‹ç¯‰ ===")
            processed = set()
            for i, url in enumerate(list(self.pages.keys())):
                # å€‹åˆ¥è¨˜äº‹ãƒšãƒ¼ã‚¸ã‹ã‚‰ã®ãƒªãƒ³ã‚¯ã®ã¿åˆ†æ
                if not self.is_valid_page(url):
                    continue
                    
                try:
                    if i % 20 == 0:
                        print(f"ãƒªãƒ³ã‚¯è§£æé€²æ—: {i}/{len(self.pages)}")
                        self.root.after(0, lambda: self.status_label.configure(text=f"ãƒªãƒ³ã‚¯è§£æä¸­: {i}/{len(self.pages)}"))
                    
                    response = session.get(url, timeout=10)
                    if response.status_code != 200: continue
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    content_links = self.extract_content_links(soup, url)
                    
                    for link_data in content_links:
                        target = link_data['url']
                        if target in self.pages and target != url:
                            link_key = (url, target)
                            if link_key not in processed:
                                processed.add(link_key)
                                self.links.append((url, target))
                                self.pages[url]['outbound_links'].append(target)
                                self.detailed_links.append({
                                    'source_url': url, 'source_title': self.pages[url]['title'],
                                    'target_url': target, 'anchor_text': link_data['anchor_text']
                                })
                except Exception as e:
                    print(f"ãƒªãƒ³ã‚¯è§£æã‚¨ãƒ©ãƒ¼: {e}")
                    continue

            # è¢«ãƒªãƒ³ã‚¯æ•°è¨ˆç®—
            for url in self.pages:
                self.pages[url]['inbound_links'] = sum(1 for s, t in self.links if t == url)

            print(f"=== åˆ†æå®Œäº†: {len(self.pages)}è¨˜äº‹, {len(self.links)}ãƒªãƒ³ã‚¯ ===")
            self.root.after(0, self.show_results)
            
            # å…¨è‡ªå‹•åŒ–ï¼šåˆ†æå®Œäº†å¾Œã«è‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            if self.pages:  # ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿
                self.root.after(2000, self.auto_export_csv)  # 2ç§’å¾Œã«è‡ªå‹•ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            
        except Exception as e:
            print(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            self.root.after(0, lambda: messagebox.showerror("ã‚¨ãƒ©ãƒ¼", str(e)))
        finally:
            self.root.after(0, lambda: self.analyze_btn.configure(state="normal"))

    def show_results(self):
        for w in self.result_frame.winfo_children(): w.destroy()
        
        if not self.pages:
            ctk.CTkLabel(self.result_frame, text="ãƒ‡ãƒ¼ã‚¿ãªã—").pack()
            return

        total, isolated = len(self.pages), sum(1 for p in self.pages.values() if p['inbound_links'] == 0)
        ctk.CTkLabel(self.result_frame, text=f"ğŸ“Š {total}è¨˜äº‹ | {len(self.links)}ãƒªãƒ³ã‚¯ | å­¤ç«‹{isolated}ä»¶", 
                    font=ctk.CTkFont(size=16, weight="bold")).pack(pady=10)

        for i, (url, info) in enumerate(sorted(self.pages.items(), key=lambda x: x[1]['inbound_links'], reverse=True)):
            eval_text = "ğŸš¨è¦æ”¹å–„" if info['inbound_links'] == 0 else "ğŸ†è¶…äººæ°—" if info['inbound_links'] >= 10 else "âœ…äººæ°—" if info['inbound_links'] >= 5 else "âš ï¸æ™®é€š"
            text = f"{i+1}. {info['title'][:50]}...\n   è¢«ãƒªãƒ³ã‚¯:{info['inbound_links']} | ç™ºãƒªãƒ³ã‚¯:{len(info['outbound_links'])} | {eval_text}\n   {url}"
            ctk.CTkLabel(self.result_frame, text=text, anchor="w", font=ctk.CTkFont(size=10)).pack(fill="x", padx=10, pady=1)

    def auto_export_csv(self):
        """å…¨è‡ªå‹•åŒ–ï¼šè‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆEXEåŒ–å¯¾å¿œï¼‰"""
        try:
            # EXEåŒ–å¯¾å¿œï¼šå®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
            if hasattr(sys, '_MEIPASS'):
                # PyInstallerã§EXEåŒ–ã•ã‚ŒãŸå ´åˆ
                base_dir = os.path.dirname(sys.executable)
            else:
                # é€šå¸¸ã®Pythonå®Ÿè¡Œã®å ´åˆ
                base_dir = os.path.dirname(os.path.abspath(__file__))
            
            # æ—¥æœ¬ã®å½“æ—¥æ—¥ä»˜ã§ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
            today = datetime.now()
            date_folder = today.strftime("%Y-%m-%d")  # 2025-06-20 å½¢å¼
            folder_path = os.path.join(base_dir, date_folder)
            
            # ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
            if not os.path.exists(folder_path):
                os.makedirs(folder_path)
                print(f"æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ: {date_folder}")
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«åã¨ãƒ‘ã‚¹
            csv_filename = f"pay-ful-{date_folder}.csv"
            filename = os.path.join(folder_path, csv_filename)
            
            if not self.detailed_links:
                print("è©³ç´°ãƒ‡ãƒ¼ã‚¿ãªã— - CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—")
                return
            
            with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                writer.writerow(['ç•ªå·', 'ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'URL', 'è¢«ãƒªãƒ³ã‚¯å…ƒã‚¿ã‚¤ãƒˆãƒ«', 'è¢«ãƒªãƒ³ã‚¯å…ƒURL', 'ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'])
                
                targets = {}
                for link in self.detailed_links:
                    target = link['target_url']
                    targets.setdefault(target, []).append(link)
                
                row = 1
                for target, links in sorted(targets.items(), key=lambda x: len(x[1]), reverse=True):
                    title = self.pages.get(target, {}).get('title', target)
                    for link in links:
                        writer.writerow([row, title, target, link['source_title'], link['source_url'], link['anchor_text']])
                        row += 1
                
                for url, info in self.pages.items():
                    if info['inbound_links'] == 0:
                        writer.writerow([row, info['title'], url, '', '', ''])
                        row += 1
            
            print(f"=== è‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {folder_path}/{csv_filename} ===")
            self.status_label.configure(text=f"åˆ†æå®Œäº†ï¼CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {csv_filename}")
            
        except Exception as e:
            print(f"è‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            self.status_label.configure(text="åˆ†æå®Œäº†ï¼ˆCSVä¿å­˜ã‚¨ãƒ©ãƒ¼ï¼‰")

    def export_detailed(self):
        if not self.detailed_links: return messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "è©³ç´°ãƒ‡ãƒ¼ã‚¿ãªã—")
        filename = filedialog.asksaveasfilename(defaultextension=".csv", filetypes=[("CSV", "*.csv")])
        if not filename: return
        
        with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow(['ç•ªå·', 'ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'URL', 'è¢«ãƒªãƒ³ã‚¯å…ƒã‚¿ã‚¤ãƒˆãƒ«', 'è¢«ãƒªãƒ³ã‚¯å…ƒURL', 'ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'])
            
            targets = {}
            for link in self.detailed_links:
                target = link['target_url']
                targets.setdefault(target, []).append(link)
            
            row = 1
            for target, links in sorted(targets.items(), key=lambda x: len(x[1]), reverse=True):
                title = self.pages.get(target, {}).get('title', target)
                for link in links:
                    writer.writerow([row, title, target, link['source_title'], link['source_url'], link['anchor_text']])
                    row += 1
            
            for url, info in self.pages.items():
                if info['inbound_links'] == 0:
                    writer.writerow([row, info['title'], url, '', '', ''])
                    row += 1
        
        messagebox.showinfo("å®Œäº†", "è©³ç´°CSVä¿å­˜å®Œäº†")

    def run(self):
        self.root.mainloop()

if __name__ == "__main__":
    PayfulAnalyzer().run()
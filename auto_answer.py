# Streamlitç‰ˆï¼ˆå®Œæˆç‰ˆï¼‰

```python
# streamlit_app.py
# ------------------------------------------------------------
# ğŸ”— answer-genkinka.jpå°‚ç”¨å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆStreamlitãƒ»å®Ÿã‚¯ãƒ­ãƒ¼ãƒ«ç‰ˆï¼‰
# - æ—¢å­˜Tk/CTkç‰ˆã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’Streamlitã«ç§»æ¤
# - ãƒ€ãƒŸãƒ¼ãªã—ã€‚å®Ÿéš›ã«requests+BeautifulSoupã§ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¾ã™
# - åé›†ä¸Šé™: 500è¨˜äº‹ / è‡ªå‹•ã§ /blog/page/2/ ã‚‚å·¡å›
# - åˆ†æå®Œäº†å¾Œï¼šç”»é¢è¡¨ç¤º + è©³ç´°CSVã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ + è‡ªå‹•ä¿å­˜ï¼ˆä»»æ„ï¼‰
# ------------------------------------------------------------

import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
from datetime import datetime
import re
import csv
import io
import os
import sys
import time

st.set_page_config(page_title="answer-genkinka.jp å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æï¼ˆStreamlitï¼‰", layout="wide")

# =========================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================
def normalize_url(url: str) -> str:
    if not url:
        return ""
    try:
        parsed = urlparse(url)
        return f"https://{parsed.netloc.replace('www.', '')}{parsed.path.rstrip('/')}"
    except Exception:
        return url

def is_article_page(url: str) -> bool:
    """å€‹åˆ¥è¨˜äº‹ãƒšãƒ¼ã‚¸ã‹ã©ã†ã‹åˆ¤å®šï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆ/Tkç‰ˆæº–æ‹ ï¼‰"""
    path = urlparse(normalize_url(url)).path.lower()

    # é™¤å¤–ï¼šæ˜ã‚‰ã‹ã«è¨˜äº‹ã§ã¯ãªã„ã‚‚ã®
    exclude_words = ['/site/', '/blog/', '/page/', '/feed', '/wp-', '.']
    if any(word in path for word in exclude_words):
        return False

    # è¨±å¯ï¼šãƒ«ãƒ¼ãƒˆç›´ä¸‹ã®ã‚¹ãƒ©ãƒƒã‚°ï¼ˆ/è¨˜äº‹åï¼‰
    if path.startswith('/') and len(path) > 1:
        clean_path = path[1:].rstrip('/')
        if clean_path and '/' not in clean_path:
            return True

    return False

def is_crawlable(url: str) -> bool:
    """ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ã‹ã©ã†ã‹åˆ¤å®š"""
    path = urlparse(normalize_url(url)).path.lower()

    # é™¤å¤–ï¼šæ˜ã‚‰ã‹ã«ä¸è¦ãªã‚‚ã®
    exclude_words = ['/site/', '/wp-admin', '/feed', '.jpg', '.png', '.css', '.js']
    if any(word in path for word in exclude_words):
        return False

    # è¨±å¯ï¼šãƒ–ãƒ­ã‚°é–¢é€£ + è¨˜äº‹ãƒšãƒ¼ã‚¸
    return (path.startswith('/blog') or is_article_page(url))

def extract_links(soup: BeautifulSoup, current_url: str) -> list[str]:
    """ãƒªãƒ³ã‚¯æŠ½å‡ºï¼ˆã‚µã‚¤ãƒˆå†… & ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ã®ã¿ï¼‰"""
    links = []
    for a in soup.find_all('a', href=True):
        href = a.get('href', '').strip()
        if href and not href.startswith('#'):
            absolute = urljoin(current_url, href)
            if 'answer-genkinka.jp' in absolute and is_crawlable(absolute):
                links.append(normalize_url(absolute))
    return list(set(links))

def extract_content_links(soup: BeautifulSoup, current_url: str) -> list[dict]:
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢ã‹ã‚‰ãƒªãƒ³ã‚¯æŠ½å‡ºï¼ˆæœ¬æ–‡å†…ãƒªãƒ³ã‚¯ã®ã¿ï¼‰"""
    content = soup.select_one('.entry-content, .post-content, main, article')
    if not content:
        return []

    links = []
    for a in content.find_all('a', href=True):
        href = a.get('href', '').strip()
        text = a.get_text(strip=True) or ''
        if href and not href.startswith('#') and text:
            absolute = urljoin(current_url, href)
            if 'answer-genkinka.jp' in absolute and is_article_page(absolute):
                links.append({'url': normalize_url(absolute), 'anchor_text': text[:100]})
    return links

def sanitize_title(title: str, url: str) -> str:
    """ã‚µã‚¤ãƒˆåãªã©ã‚’é™¤å»"""
    if not title:
        return url
    # ã€Œanswer-genkinka | ã‚¢ãƒ³ã‚µãƒ¼ã€ç­‰ã®ã‚µã‚¤ãƒˆåé™¤å»
    return re.sub(r'\s*[|\-]\s*.*(answer-genkinka|ã‚¢ãƒ³ã‚µãƒ¼).*$', '', title, flags=re.IGNORECASE)

# =========================
# åˆ†ææœ¬ä½“
# =========================
def analyze_site(start_url: str, max_pages: int = 500, status_cb=None, progress_cb=None):
    """
    - start_url ã‹ã‚‰å·¡å›ã—ã€/blog/page/2/ ã‚‚ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
    - è¨˜äº‹ãƒšãƒ¼ã‚¸ã®è¢«/ç™ºãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
    - return: pages(dict), links(list[tuple]), detailed_links(list[dict])
    """
    pages = {}
    links = []
    detailed_links = []

    visited = set()
    to_visit = [normalize_url(start_url)]
    # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚‚å·¡å›
    to_visit.append('https://answer-genkinka.jp/blog/page/2/')

    session = requests.Session()
    session.headers.update({'User-Agent': 'Mozilla/5.0'})

    if status_cb:
        status_cb("åé›†ä¸­: 0è¨˜äº‹")

    # ===== ãƒ•ã‚§ãƒ¼ã‚º1: ãƒšãƒ¼ã‚¸åé›† =====
    while to_visit and len(pages) < max_pages:
        url = to_visit.pop(0)
        if url in visited:
            continue

        try:
            resp = session.get(url, timeout=10)
            if resp.status_code != 200:
                visited.add(url)
                continue

            soup = BeautifulSoup(resp.text, 'html.parser')

            # NOINDEXãƒã‚§ãƒƒã‚¯
            robots = soup.find('meta', attrs={'name': 'robots'})
            if robots and 'noindex' in robots.get('content', '').lower():
                visited.add(url)
                continue

            # è¨˜äº‹ä¸€è¦§ãƒšãƒ¼ã‚¸ã¯åé›†ã®ã¿
            if '/blog' in url:
                page_links = extract_links(soup, url)
                new_links = [l for l in page_links if l not in visited and l not in to_visit]
                to_visit.extend(new_links)
                visited.add(url)
                if status_cb:
                    status_cb(f"åé›†ä¸­: {len(pages)}è¨˜äº‹ï¼ˆä¸€è¦§: æ–°è¦{len(new_links)}ä»¶ï¼‰")
                if progress_cb:
                    progress_cb(len(pages) / max_pages)
                time.sleep(0.05)
                continue

            # å€‹åˆ¥è¨˜äº‹
            if is_article_page(url):
                title_el = soup.find('h1')
                title = sanitize_title(title_el.get_text(strip=True) if title_el else url, url)

                pages[url] = {'title': title, 'outbound_links': []}

                # æ–°è¦ãƒªãƒ³ã‚¯ç™ºè¦‹
                page_links = extract_links(soup, url)
                new_links = [l for l in page_links if l not in visited and l not in to_visit]
                to_visit.extend(new_links)

            visited.add(url)

            if status_cb:
                status_cb(f"åé›†ä¸­: {len(pages)}è¨˜äº‹")
            if progress_cb:
                progress_cb(len(pages) / max_pages)
            time.sleep(0.05)

        except Exception:
            # å¤±æ•—ã¯ã‚¹ã‚­ãƒƒãƒ—
            visited.add(url)
            continue

    # ===== ãƒ•ã‚§ãƒ¼ã‚º2: ãƒªãƒ³ã‚¯é–¢ä¿‚æ§‹ç¯‰ =====
    processed = set()
    for url in list(pages.keys()):
        try:
            resp = session.get(url, timeout=10)
            if resp.status_code != 200:
                continue

            soup = BeautifulSoup(resp.text, 'html.parser')
            content_links = extract_content_links(soup, url)

            for link_data in content_links:
                target = link_data['url']
                if target in pages and target != url:
                    link_key = (url, target)
                    if link_key not in processed:
                        processed.add(link_key)
                        links.append((url, target))
                        pages[url]['outbound_links'].append(target)
                        detailed_links.append({
                            'source_url': url,
                            'source_title': pages[url]['title'],
                            'target_url': target,
                            'anchor_text': link_data['anchor_text']
                        })
        except Exception:
            continue

    # è¢«ãƒªãƒ³ã‚¯æ•°è¨ˆç®—
    for u in pages:
        pages[u]['inbound_links'] = sum(1 for s, t in links if t == u)

    return pages, links, detailed_links

# =========================
# CSVå‡ºåŠ›
# =========================
def build_csv_bytes(pages: dict, detailed_links: list[dict]) -> bytes:
    """
    ç”»é¢ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ï¼ˆUTF-8 BOMï¼‰
    """
    buf = io.StringIO()
    writer = csv.writer(buf)
    writer.writerow(['ç•ªå·', 'ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'URL', 'è¢«ãƒªãƒ³ã‚¯å…ƒã‚¿ã‚¤ãƒˆãƒ«', 'è¢«ãƒªãƒ³ã‚¯å…ƒURL', 'ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'])

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    targets = {}
    for link in detailed_links:
        target = link['target_url']
        targets.setdefault(target, []).append(link)

    # è¢«ãƒªãƒ³ã‚¯æ•°ã§ã‚½ãƒ¼ãƒˆ
    sorted_targets = sorted(targets.items(), key=lambda x: len(x[1]), reverse=True)

    row = 1
    for target, links_list in sorted_targets:
        title = pages.get(target, {}).get('title', target)
        for link in links_list:
            writer.writerow([row, title, target, link['source_title'], link['source_url'], link['anchor_text']])
            row += 1

    # å­¤ç«‹ãƒšãƒ¼ã‚¸ã‚‚è¿½åŠ 
    for url, info in pages.items():
        if info.get('inbound_links', 0) == 0:
            writer.writerow([row, info.get('title', url), url, '', '', ''])
            row += 1

    data = buf.getvalue().encode('utf-8-sig')
    buf.close()
    return data

def auto_save_csv_to_disk(pages: dict, detailed_links: list[dict]) -> str | None:
    """
    ãƒ­ãƒ¼ã‚«ãƒ«/ã‚µãƒ¼ãƒãƒ¼ã«æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ã§è‡ªå‹•ä¿å­˜ï¼ˆä»»æ„ï¼‰
    """
    try:
        # å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«åŸºæº–ï¼ˆPyInstallerå¯¾å¿œï¼‰
        if hasattr(sys, '_MEIPASS'):
            base_dir = os.path.dirname(sys.executable)
        else:
            base_dir = os.path.dirname(os.path.abspath(__file__))

        today = datetime.now().strftime("%Y-%m-%d")
        folder_path = os.path.join(base_dir, today)
        os.makedirs(folder_path, exist_ok=True)

        filename = f"answer-genkinka-{today}.csv"
        filepath = os.path.join(folder_path, filename)

        with open(filepath, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow(['ç•ªå·', 'ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'URL', 'è¢«ãƒªãƒ³ã‚¯å…ƒã‚¿ã‚¤ãƒˆãƒ«', 'è¢«ãƒªãƒ³ã‚¯å…ƒURL', 'ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'])

            targets = {}
            for link in detailed_links:
                target = link['target_url']
                targets.setdefault(target, []).append(link)

            sorted_targets = sorted(targets.items(), key=lambda x: len(x[1]), reverse=True)
            row = 1
            for target, links_list in sorted_targets:
                title = pages.get(target, {}).get('title', target)
                for link in links_list:
                    writer.writerow([row, title, target, link['source_title'], link['source_url'], link['anchor_text']])
                    row += 1

            for url, info in pages.items():
                if info.get('inbound_links', 0) == 0:
                    writer.writerow([row, info.get('title', url), url, '', '', ''])
                    row += 1

        return filepath
    except Exception:
        return None

# =========================
# UI / å®Ÿè¡Œ
# =========================
DEFAULT_URL = "https://answer-genkinka.jp/blog/"

st.title("ğŸ”— answer-genkinka.jpå°‚ç”¨å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆStreamlitãƒ»å…¨è‡ªå‹•ç‰ˆï¼‰")

col_url, col_opts = st.columns([0.7, 0.3])
with col_url:
    start_url = st.text_input("é–‹å§‹URL", value=DEFAULT_URL, help="ä¾‹: https://answer-genkinka.jp/blog/")

with col_opts:
    max_pages = st.number_input("åé›†ä¸Šé™ï¼ˆè¨˜äº‹ï¼‰", min_value=50, max_value=2000, value=500, step=50)
    do_autosave = st.checkbox("åˆ†æå®Œäº†å¾Œã«è‡ªå‹•ä¿å­˜ï¼ˆå®Ÿè¡Œç’°å¢ƒã«CSVå‡ºåŠ›ï¼‰", value=True)

# è‡ªå‹•å®Ÿè¡Œï¼ˆåˆå›ã®ã¿ï¼‰
if "auto_run_done" not in st.session_state:
    st.session_state.auto_run_done = False

run_now = False
c1, c2 = st.columns([0.3, 0.7])
with c1:
    if st.button("åˆ†æé–‹å§‹", type="primary"):
        run_now = True
with c2:
    st.write(" ")

if not st.session_state.auto_run_done and start_url.strip():
    run_now = True
    st.session_state.auto_run_done = True

status = st.empty()
progress = st.progress(0)

def status_cb(msg: str):
    status.info(msg)

def progress_cb(val: float):
    progress.progress(min(max(val, 0.0), 1.0))

if run_now:
    try:
        status_cb("æº–å‚™ä¸­â€¦")
        pages, links, detailed_links = analyze_site(start_url, int(max_pages), status_cb=status_cb, progress_cb=progress_cb)

        total = len(pages)
        isolated = sum(1 for p in pages.values() if p.get('inbound_links', 0) == 0)
        popular5 = sum(1 for p in pages.values() if p.get('inbound_links', 0) >= 5)
        st.success(f"åˆ†æå®Œäº†: {total}è¨˜äº‹ / {len(links)}ãƒªãƒ³ã‚¯ / å­¤ç«‹{isolated}ä»¶ / äººæ°—(5+) {popular5}ä»¶")

        # çµæœãƒ†ãƒ¼ãƒ–ãƒ«
        sorted_pages = sorted(pages.items(), key=lambda x: x[1].get('inbound_links', 0), reverse=True)

        # å·¦: ã‚µãƒãƒª / å³: ãƒªã‚¹ãƒˆ
        left, right = st.columns([0.35, 0.65])
        with left:
            st.subheader("ã‚µãƒãƒª")
            st.metric("è¨˜äº‹æ•°", total)
            st.metric("ãƒªãƒ³ã‚¯æ•°", len(links))
            st.metric("å­¤ç«‹ãƒšãƒ¼ã‚¸", isolated)
            st.metric("äººæ°—(è¢«10+)", sum(1 for p in pages.values() if p.get('inbound_links', 0) >= 10))

            # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            csv_bytes = build_csv_bytes(pages, detailed_links)
            st.download_button(
                label="è©³ç´°CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv_bytes,
                file_name=f"answer-genkinka-{datetime.now().strftime('%Y-%m-%d')}.csv",
                mime="text/csv"
            )

            # è‡ªå‹•ä¿å­˜
            saved_path = None
            if do_autosave:
                saved_path = auto_save_csv_to_disk(pages, detailed_links)
            if saved_path:
                st.caption(f"è‡ªå‹•ä¿å­˜: {saved_path}")

        with right:
            st.subheader("è¨˜äº‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆè¢«ãƒªãƒ³ã‚¯é™é †ï¼‰")
            for i, (u, info) in enumerate(sorted_pages, start=1):
                inbound = info.get('inbound_links', 0)
                outbound = len(info.get('outbound_links', []))

                if inbound == 0:
                    eval_text = "ğŸš¨è¦æ”¹å–„"
                elif inbound >= 10:
                    eval_text = "ğŸ†è¶…äººæ°—"
                elif inbound >= 5:
                    eval_text = "âœ…äººæ°—"
                else:
                    eval_text = "âš ï¸æ™®é€š"

                st.markdown(
                    f"**{i}. {info.get('title','')[:50]}...**  \n"
                    f"è¢«ãƒªãƒ³ã‚¯: **{inbound}** / ç™ºãƒªãƒ³ã‚¯: {outbound} / {eval_text}  \n"
                    f"[{u}]({u})"
                )

        status_cb("å®Œäº†")
        progress_cb(1.0)

        # è©³ç´°ãƒªãƒ³ã‚¯ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        with st.expander("æœ¬æ–‡å†…ãƒªãƒ³ã‚¯ï¼ˆè©³ç´°ãƒ»ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ¥ï¼‰"):
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            targets = {}
            for link in detailed_links:
                targets.setdefault(link['target_url'], []).append(link)
            sorted_targets = sorted(targets.items(), key=lambda x: len(x[1]), reverse=True)

            for tgt, lst in sorted_targets[:200]:  # è¡¨ç¤ºéå¤šé˜²æ­¢ã§æœ€å¤§200ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
                st.markdown(f"**{pages.get(tgt,{}).get('title',tgt)}**  \n[{tgt}]({tgt})  \nè¢«ãƒªãƒ³ã‚¯: **{len(lst)}**")
                for lk in lst[:50]:  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã”ã¨æœ€å¤§50ä»¶è¡¨ç¤º
                    st.markdown(
                        f"- from: [{lk['source_title']}]({lk['source_url']})  \n"
                        f"  ã‚¢ãƒ³ã‚«ãƒ¼: `{lk['anchor_text']}`"
                    )
                st.divider()

    except Exception as e:
        st.error(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
else:
    st.info("ã€Œåˆ†æé–‹å§‹ã€ã‚’æŠ¼ã™ã‹ã€ãƒšãƒ¼ã‚¸è¡¨ç¤ºç›´å¾Œã®è‡ªå‹•å®Ÿè¡Œã‚’ãŠå¾…ã¡ãã ã•ã„ã€‚")
```

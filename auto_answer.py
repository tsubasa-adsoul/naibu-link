import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import time
import re
import csv
import io
from datetime import datetime
import pandas as pd

class AnswerAnalyzer:
    def __init__(self):
        self.pages = {}
        self.links = []
        self.detailed_links = []

    def normalize_url(self, url):
        if not url: 
            return ""
        try:
            parsed = urlparse(url)
            return f"https://{parsed.netloc.replace('www.', '')}{parsed.path.rstrip('/')}"
        except:
            return url

    def is_article_page(self, url):
        path = urlparse(self.normalize_url(url)).path.lower()
        
        exclude_words = ['/site/', '/blog/', '/page/', '/feed', '/wp-', '.']
        if any(word in path for word in exclude_words):
            return False
        
        if path.startswith('/') and len(path) > 1:
            clean_path = path[1:].rstrip('/')
            if clean_path and '/' not in clean_path:
                return True
        
        return False

    def is_crawlable(self, url):
        path = urlparse(self.normalize_url(url)).path.lower()
        
        exclude_words = ['/site/', '/wp-admin', '/feed', '.jpg', '.png', '.css', '.js']
        if any(word in path for word in exclude_words):
            return False
        
        return (path.startswith('/blog') or self.is_article_page(url))

    def extract_links(self, soup, current_url):
        links = []
        for a in soup.find_all('a', href=True):
            href = a.get('href', '').strip()
            if href and not href.startswith('#'):
                absolute = urljoin(current_url, href)
                if 'answer-genkinka.jp' in absolute and self.is_crawlable(absolute):
                    links.append(self.normalize_url(absolute))
        return list(set(links))

    def extract_content_links(self, soup, current_url):
        content = soup.select_one('.entry-content, .post-content, main, article')
        if not content:
            return []
        
        links = []
        for a in content.find_all('a', href=True):
            href = a.get('href', '').strip()
            text = a.get_text(strip=True) or ''
            if href and not href.startswith('#') and text:
                absolute = urljoin(current_url, href)
                if 'answer-genkinka.jp' in absolute and self.is_article_page(absolute):
                    links.append({'url': self.normalize_url(absolute), 'anchor_text': text[:100]})
        return links

    def analyze(self, start_url):
        self.pages, self.links, self.detailed_links = {}, [], []
        visited, to_visit = set(), [self.normalize_url(start_url)]
        
        to_visit.append('https://answer-genkinka.jp/blog/page/2/')

        session = requests.Session()
        session.headers.update({'User-Agent': 'Mozilla/5.0'})

        progress_bar = st.progress(0)
        status_text = st.empty()

        status_text.text("ãƒ•ã‚§ãƒ¼ã‚º1: ãƒšãƒ¼ã‚¸åé›†ä¸­...")
        
        while to_visit and len(self.pages) < 100:
            url = to_visit.pop(0)
            if url in visited: 
                continue
            
            try:
                status_text.text(f"å‡¦ç†ä¸­: {url}")
                response = session.get(url, timeout=10)
                if response.status_code != 200: 
                    continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                robots = soup.find('meta', attrs={'name': 'robots'})
                if robots and 'noindex' in robots.get('content', '').lower():
                    continue
                
                if '/blog' in url:
                    page_links = self.extract_links(soup, url)
                    new_links = [l for l in page_links if l not in visited and l not in to_visit]
                    to_visit.extend(new_links)
                    visited.add(url)
                    continue
                
                if self.is_article_page(url):
                    title = soup.find('h1')
                    title = title.get_text(strip=True) if title else url
                    
                    title = re.sub(r'\s*[|\-]\s*.*(answer-genkinka|ã‚¢ãƒ³ã‚µãƒ¼).*$', '', title, flags=re.IGNORECASE)
                    
                    self.pages[url] = {'title': title, 'outbound_links': []}
                    
                    page_links = self.extract_links(soup, url)
                    new_links = [l for l in page_links if l not in visited and l not in to_visit]
                    to_visit.extend(new_links)
                
                visited.add(url)
                progress_bar.progress(len(self.pages) / 100)
                time.sleep(0.1)
                
            except Exception as e:
                continue

        status_text.text("ãƒ•ã‚§ãƒ¼ã‚º2: ãƒªãƒ³ã‚¯é–¢ä¿‚æ§‹ç¯‰ä¸­...")
        
        processed = set()
        for i, url in enumerate(list(self.pages.keys())):
            try:
                response = session.get(url, timeout=10)
                if response.status_code != 200: 
                    continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                content_links = self.extract_content_links(soup, url)
                
                for link_data in content_links:
                    target = link_data['url']
                    if target in self.pages and target != url:
                        link_key = (url, target)
                        if link_key not in processed:
                            processed.add(link_key)
                            self.links.append((url, target))
                            self.pages[url]['outbound_links'].append(target)
                            self.detailed_links.append({
                                'source_url': url, 
                                'source_title': self.pages[url]['title'],
                                'target_url': target, 
                                'anchor_text': link_data['anchor_text']
                            })
                            
                progress_bar.progress(0.5 + 0.5 * i / len(self.pages))
                            
            except Exception as e:
                continue

        for url in self.pages:
            self.pages[url]['inbound_links'] = sum(1 for s, t in self.links if t == url)

        progress_bar.progress(1.0)
        status_text.text("åˆ†æå®Œäº†!")
        
        return self.pages, self.links, self.detailed_links

    def export_detailed_csv(self):
        if not self.detailed_links:
            return None
            
        csv_data = []
        csv_data.append(['ç•ªå·', 'ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'URL', 'è¢«ãƒªãƒ³ã‚¯å…ƒã‚¿ã‚¤ãƒˆãƒ«', 'è¢«ãƒªãƒ³ã‚¯å…ƒURL', 'ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'])
        
        targets = {}
        for link in self.detailed_links:
            target = link['target_url']
            targets.setdefault(target, []).append(link)
        
        sorted_targets = sorted(targets.items(), key=lambda x: len(x[1]), reverse=True)
        
        page_number = 1
        for target, links_list in sorted_targets:
            title = self.pages.get(target, {}).get('title', target)
            for link in links_list:
                csv_data.append([page_number, title, target, link['source_title'], link['source_url'], link['anchor_text']])
            page_number += 1
        
        for url, info in self.pages.items():
            if info['inbound_links'] == 0:
                csv_data.append([page_number, info['title'], url, '', '', ''])
                page_number += 1
        
        output = io.StringIO()
        writer = csv.writer(output)
        writer.writerows(csv_data)
        return output.getvalue()

def main():
    st.set_page_config(
        page_title="Answerç¾é‡‘åŒ– å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æ",
        page_icon="ğŸ”—",
        layout="wide"
    )
    
    st.title("ğŸ”— answer-genkinka.jpå°‚ç”¨å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æãƒ„ãƒ¼ãƒ«")
    st.markdown("**CustomTkinterå®Œå…¨ç§»æ¤ç‰ˆ**")
    
    start_url = st.text_input(
        "åˆ†æé–‹å§‹URL",
        value="https://answer-genkinka.jp/blog/",
        help="answer-genkinka.jpã®åˆ†æé–‹å§‹URL"
    )
    
    if st.button("ğŸš€ åˆ†æé–‹å§‹", type="primary"):
        if not start_url:
            st.error("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            return
            
        analyzer = AnswerAnalyzer()
        
        with st.spinner("åˆ†æä¸­..."):
            pages, links, detailed_links = analyzer.analyze(start_url)
        
        if pages:
            st.session_state['analyzer_data'] = {
                'pages': pages,
                'links': links,
                'detailed_links': detailed_links,
                'analyzer': analyzer
            }
    
    if 'analyzer_data' in st.session_state:
        data = st.session_state['analyzer_data']
        pages = data['pages']
        links = data['links']
        analyzer = data['analyzer']
        
        total = len(pages)
        isolated = sum(1 for p in pages.values() if p['inbound_links'] == 0)
        popular = sum(1 for p in pages.values() if p['inbound_links'] >= 5)
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ç·è¨˜äº‹æ•°", total)
        with col2:
            st.metric("ç·ãƒªãƒ³ã‚¯æ•°", len(links))
        with col3:
            st.metric("å­¤ç«‹ãƒšãƒ¼ã‚¸", isolated)
        with col4:
            st.metric("äººæ°—ãƒšãƒ¼ã‚¸", popular)
        
        st.subheader("ğŸ“Š åˆ†æçµæœï¼ˆè¢«ãƒªãƒ³ã‚¯æ•°é †ï¼‰")
        
        results_data = []
        for url, info in pages.items():
            results_data.append({
                'ã‚¿ã‚¤ãƒˆãƒ«': info['title'],
                'URL': url,
                'è¢«ãƒªãƒ³ã‚¯æ•°': info['inbound_links'],
                'ç™ºãƒªãƒ³ã‚¯æ•°': len(info['outbound_links'])
            })
        
        df = pd.DataFrame(results_data)
        df_sorted = df.sort_values('è¢«ãƒªãƒ³ã‚¯æ•°', ascending=False)
        
        if len(df_sorted) > 0:
            st.subheader("è¢«ãƒªãƒ³ã‚¯æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆä¸Šä½10ä»¶ï¼‰")
            top_10 = df_sorted.head(10)
            chart_data = top_10.set_index('ã‚¿ã‚¤ãƒˆãƒ«')['è¢«ãƒªãƒ³ã‚¯æ•°']
            st.bar_chart(chart_data)
        
        st.subheader("è©³ç´°ãƒ‡ãƒ¼ã‚¿")
        st.dataframe(df_sorted, use_container_width=True)
        
        csv_content = analyzer.export_detailed_csv()
        if csv_content:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"answer-genkinka-{timestamp}.csv"
            
            st.download_button(
                "ğŸ“¥ è©³ç´°CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                csv_content,
                filename,
                "text/csv",
                help="è¢«ãƒªãƒ³ã‚¯æ•°é †ã§ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸè©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ"
            )
        
        st.success("âœ… åˆ†æå®Œäº†!")

if __name__ == "__main__":
    main()

import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import time
import re
import csv
import pandas as pd
from datetime import datetime
from io import StringIO
import threading
import queue

class AnswerAnalyzer:
    def __init__(self):
        self.pages = {}
        self.links = []
        self.detailed_links = []
        
    def normalize_url(self, url):
        if not url: return ""
        try:
            parsed = urlparse(url)
            return f"https://{parsed.netloc.replace('www.', '')}{parsed.path.rstrip('/')}"
        except:
            return url

    def is_article_page(self, url):
        """å€‹åˆ¥è¨˜äº‹ãƒšãƒ¼ã‚¸ã‹ã©ã†ã‹åˆ¤å®šï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰"""
        path = urlparse(self.normalize_url(url)).path.lower()
        
        # é™¤å¤–ï¼šæ˜ã‚‰ã‹ã«è¨˜äº‹ã§ã¯ãªã„ã‚‚ã®
        exclude_words = ['/site/', '/blog/', '/page/', '/feed', '/wp-', '.']
        if any(word in path for word in exclude_words):
            return False
        
        # è¨±å¯ï¼šãƒ«ãƒ¼ãƒˆç›´ä¸‹ã®ã‚¹ãƒ©ãƒƒã‚°ï¼ˆ/è¨˜äº‹åï¼‰
        if path.startswith('/') and len(path) > 1:
            # ãƒ‘ã‚¹ã‹ã‚‰ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’é™¤å»
            clean_path = path[1:].rstrip('/')
            # ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ãŒå«ã¾ã‚Œã¦ã„ãªãã¦ã€æ–‡å­—ãŒã‚ã‚‹
            if clean_path and '/' not in clean_path:
                return True
        
        return False

    def is_crawlable(self, url):
        """ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ã‹ã©ã†ã‹åˆ¤å®š"""
        path = urlparse(self.normalize_url(url)).path.lower()
        
        # é™¤å¤–ï¼šæ˜ã‚‰ã‹ã«ä¸è¦ãªã‚‚ã®
        exclude_words = ['/site/', '/wp-admin', '/feed', '.jpg', '.png', '.css', '.js']
        if any(word in path for word in exclude_words):
            return False
        
        # è¨±å¯ï¼šãƒ–ãƒ­ã‚°é–¢é€£ + è¨˜äº‹ãƒšãƒ¼ã‚¸
        return (path.startswith('/blog') or self.is_article_page(url))

    def extract_links(self, soup, current_url):
        """ãƒªãƒ³ã‚¯æŠ½å‡º"""
        links = []
        for a in soup.find_all('a', href=True):
            href = a.get('href', '').strip()
            if href and not href.startswith('#'):
                absolute = urljoin(current_url, href)
                if 'answer-genkinka.jp' in absolute and self.is_crawlable(absolute):
                    links.append(self.normalize_url(absolute))
        return list(set(links))

    def extract_content_links(self, soup, current_url):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢ã‹ã‚‰ãƒªãƒ³ã‚¯æŠ½å‡º"""
        content = soup.select_one('.entry-content, .post-content, main, article')
        if not content:
            return []
        
        links = []
        for a in content.find_all('a', href=True):
            href = a.get('href', '').strip()
            text = a.get_text(strip=True) or ''
            if href and not href.startswith('#') and text:
                absolute = urljoin(current_url, href)
                if 'answer-genkinka.jp' in absolute and self.is_article_page(absolute):
                    links.append({'url': self.normalize_url(absolute), 'anchor_text': text[:100]})
        return links

    def analyze_site(self, url, progress_callback=None, status_callback=None):
        """ã‚µã‚¤ãƒˆåˆ†æãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        try:
            self.pages, self.links, self.detailed_links = {}, [], []
            visited, to_visit = set(), [self.normalize_url(url)]
            
            # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
            to_visit.append('https://answer-genkinka.jp/blog/page/2/')

            session = requests.Session()
            session.headers.update({'User-Agent': 'Mozilla/5.0'})

            if status_callback:
                status_callback("=== answer-genkinka.jp åˆ†æé–‹å§‹ ===")

            # ãƒ•ã‚§ãƒ¼ã‚º1: ãƒšãƒ¼ã‚¸åé›†
            max_pages = 500
            while to_visit and len(self.pages) < max_pages:
                url_to_process = to_visit.pop(0)
                if url_to_process in visited: 
                    continue
                
                try:
                    if status_callback:
                        status_callback(f"å‡¦ç†ä¸­: {url_to_process}")
                    
                    response = session.get(url_to_process, timeout=10)
                    if response.status_code != 200: 
                        continue
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # NOINDEXãƒã‚§ãƒƒã‚¯
                    robots = soup.find('meta', attrs={'name': 'robots'})
                    if robots and 'noindex' in robots.get('content', '').lower():
                        continue
                    
                    # è¨˜äº‹ä¸€è¦§ãƒšãƒ¼ã‚¸ã¯åé›†ã®ã¿
                    if '/blog' in url_to_process:
                        page_links = self.extract_links(soup, url_to_process)
                        new_links = [l for l in page_links if l not in visited and l not in to_visit]
                        to_visit.extend(new_links)
                        visited.add(url_to_process)
                        continue
                    
                    # å€‹åˆ¥è¨˜äº‹ãƒšãƒ¼ã‚¸ã‚’ä¿å­˜
                    if self.is_article_page(url_to_process):
                        title = soup.find('h1')
                        title = title.get_text(strip=True) if title else url_to_process
                        
                        # answer-genkinka | ã‚¢ãƒ³ã‚µãƒ¼ ãªã©ã®ã‚µã‚¤ãƒˆåã‚’é™¤å»
                        title = re.sub(r'\s*[|\-]\s*.*(answer-genkinka|ã‚¢ãƒ³ã‚µãƒ¼).*$', '', title, flags=re.IGNORECASE)
                        
                        self.pages[url_to_process] = {'title': title, 'outbound_links': []}
                        
                        # æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹
                        page_links = self.extract_links(soup, url_to_process)
                        new_links = [l for l in page_links if l not in visited and l not in to_visit]
                        to_visit.extend(new_links)
                    
                    visited.add(url_to_process)
                    
                    if progress_callback:
                        progress_callback(len(self.pages) / max_pages)
                    
                    time.sleep(0.1)
                    
                except Exception as e:
                    continue

            if status_callback:
                status_callback(f"=== ãƒ•ã‚§ãƒ¼ã‚º1å®Œäº†: {len(self.pages)}è¨˜äº‹ ===")

            # ãƒ•ã‚§ãƒ¼ã‚º2: ãƒªãƒ³ã‚¯é–¢ä¿‚æ§‹ç¯‰
            if status_callback:
                status_callback("=== ãƒ•ã‚§ãƒ¼ã‚º2: ãƒªãƒ³ã‚¯é–¢ä¿‚æ§‹ç¯‰ ===")
            
            processed = set()
            for i, url_to_process in enumerate(list(self.pages.keys())):
                try:
                    if progress_callback and i % 10 == 0:
                        progress_callback(0.5 + (i / len(self.pages)) * 0.5)
                    
                    response = session.get(url_to_process, timeout=10)
                    if response.status_code != 200: 
                        continue
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    content_links = self.extract_content_links(soup, url_to_process)
                    
                    for link_data in content_links:
                        target = link_data['url']
                        if target in self.pages and target != url_to_process:
                            link_key = (url_to_process, target)
                            if link_key not in processed:
                                processed.add(link_key)
                                self.links.append((url_to_process, target))
                                self.pages[url_to_process]['outbound_links'].append(target)
                                self.detailed_links.append({
                                    'source_url': url_to_process, 
                                    'source_title': self.pages[url_to_process]['title'],
                                    'target_url': target, 
                                    'anchor_text': link_data['anchor_text']
                                })
                except Exception:
                    continue

            # è¢«ãƒªãƒ³ã‚¯æ•°è¨ˆç®—
            for url_key in self.pages:
                self.pages[url_key]['inbound_links'] = sum(1 for s, t in self.links if t == url_key)

            if status_callback:
                status_callback(f"=== åˆ†æå®Œäº†: {len(self.pages)}è¨˜äº‹, {len(self.links)}ãƒªãƒ³ã‚¯ ===")
            
            return True
            
        except Exception as e:
            if status_callback:
                status_callback(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def get_results_summary(self):
        """åˆ†æçµæœã®ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        if not self.pages:
            return None
        
        total_pages = len(self.pages)
        total_links = len(self.links)
        isolated_pages = sum(1 for p in self.pages.values() if p['inbound_links'] == 0)
        popular_pages = sum(1 for p in self.pages.values() if p['inbound_links'] >= 5)
        
        return {
            'total_pages': total_pages,
            'total_links': total_links,
            'isolated_pages': isolated_pages,
            'popular_pages': popular_pages
        }

    def get_detailed_results(self):
        """è©³ç´°çµæœã‚’DataFrameå½¢å¼ã§å–å¾—"""
        if not self.detailed_links:
            return None
        
        # è¢«ãƒªãƒ³ã‚¯ã‚ã‚Šãƒšãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿
        data = []
        for link in self.detailed_links:
            data.append({
                'ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«': link['target_url'] if link['target_url'] in self.pages else link['target_url'],
                'URL': link['target_url'],
                'è¢«ãƒªãƒ³ã‚¯å…ƒã‚¿ã‚¤ãƒˆãƒ«': link['source_title'],
                'è¢«ãƒªãƒ³ã‚¯å…ƒURL': link['source_url'],
                'ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ': link['anchor_text']
            })
        
        # å­¤ç«‹ãƒšãƒ¼ã‚¸ã‚‚è¿½åŠ 
        for url, info in self.pages.items():
            if info['inbound_links'] == 0:
                data.append({
                    'ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«': info['title'],
                    'URL': url,
                    'è¢«ãƒªãƒ³ã‚¯å…ƒã‚¿ã‚¤ãƒˆãƒ«': '',
                    'è¢«ãƒªãƒ³ã‚¯å…ƒURL': '',
                    'ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ': ''
                })
        
        return pd.DataFrame(data)

def main():
    st.set_page_config(
        page_title="Answer-Genkinka å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æ",
        page_icon="ğŸ”—",
        layout="wide"
    )
    
    st.title("ğŸ”— answer-genkinka.jpå°‚ç”¨å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æãƒ„ãƒ¼ãƒ«")
    st.markdown("**å…¨è‡ªå‹•ç‰ˆ - Streamlitå¯¾å¿œ**")
    
    # URLå…¥åŠ›
    col1, col2 = st.columns([3, 1])
    with col1:
        url = st.text_input(
            "åˆ†æURL", 
            value="https://answer-genkinka.jp/blog/",
            placeholder="https://answer-genkinka.jp/blog/"
        )
    
    with col2:
        st.markdown("**å¯¾è±¡ã‚µã‚¤ãƒˆ:**")
        st.info("answer-genkinka.jp")
    
    # åˆ†æå®Ÿè¡Œ
    if st.button("ğŸš€ åˆ†æé–‹å§‹", type="primary"):
        if not url:
            st.error("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            return
        
        analyzer = AnswerAnalyzer()
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤ºç”¨
        progress_bar = st.progress(0)
        status_placeholder = st.empty()
        
        def update_progress(value):
            progress_bar.progress(value)
        
        def update_status(message):
            status_placeholder.text(message)
        
        # åˆ†æå®Ÿè¡Œ
        with st.spinner('åˆ†æä¸­...'):
            success = analyzer.analyze_site(url, update_progress, update_status)
        
        if success:
            # çµæœè¡¨ç¤º
            summary = analyzer.get_results_summary()
            
            if summary:
                st.success("âœ… åˆ†æå®Œäº†ï¼")
                
                # çµ±è¨ˆæƒ…å ±
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("ç·è¨˜äº‹æ•°", summary['total_pages'])
                with col2:
                    st.metric("å†…éƒ¨ãƒªãƒ³ã‚¯æ•°", summary['total_links'])
                with col3:
                    st.metric("å­¤ç«‹è¨˜äº‹", summary['isolated_pages'])
                with col4:
                    st.metric("äººæ°—è¨˜äº‹", summary['popular_pages'])
                
                # è©³ç´°çµæœ
                df = analyzer.get_detailed_results()
                if df is not None and not df.empty:
                    st.subheader("ğŸ“Š è©³ç´°åˆ†æçµæœ")
                    
                    # ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
                    st.dataframe(df, use_container_width=True, height=400)
                    
                    # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                    csv_data = df.to_csv(index=False).encode('utf-8-sig')
                    filename = f"answer-genkinka-{datetime.now().strftime('%Y%m%d')}.csv"
                    
                    st.download_button(
                        label="ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=csv_data,
                        file_name=filename,
                        mime="text/csv",
                        type="primary"
                    )
                    
                    # ä¸Šä½çµæœè¡¨ç¤º
                    st.subheader("ğŸ† è¢«ãƒªãƒ³ã‚¯æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆä¸Šä½10ä»¶ï¼‰")
                    
                    # è¢«ãƒªãƒ³ã‚¯æ•°ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
                    inbound_counts = {}
                    for _, row in df.iterrows():
                        if row['è¢«ãƒªãƒ³ã‚¯å…ƒURL']:  # è¢«ãƒªãƒ³ã‚¯ãŒã‚ã‚‹å ´åˆã®ã¿
                            url_key = row['URL']
                            if url_key not in inbound_counts:
                                inbound_counts[url_key] = {
                                    'title': row['ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«'],
                                    'url': url_key,
                                    'count': 0
                                }
                            inbound_counts[url_key]['count'] += 1
                    
                    # ä¸Šä½10ä»¶è¡¨ç¤º
                    sorted_pages = sorted(inbound_counts.values(), key=lambda x: x['count'], reverse=True)[:10]
                    
                    for i, page_info in enumerate(sorted_pages, 1):
                        count = page_info['count']
                        if count >= 10:
                            eval_text = "ğŸ† è¶…äººæ°—"
                        elif count >= 5:
                            eval_text = "âœ… äººæ°—"
                        elif count >= 2:
                            eval_text = "âš ï¸ æ™®é€š"
                        else:
                            eval_text = "ğŸ”¹ å°‘æ•°"
                        
                        st.write(f"**{i}ä½** | è¢«ãƒªãƒ³ã‚¯æ•°: **{count}** | {eval_text}")
                        st.write(f"ğŸ“„ {page_info['title'][:80]}...")
                        st.write(f"ğŸ”— {page_info['url']}")
                        st.divider()
                
                else:
                    st.warning("è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            else:
                st.warning("åˆ†æçµæœãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        else:
            st.error("åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ã‚¯ãƒªã‚¢
        progress_bar.empty()
        status_placeholder.empty()

if __name__ == "__main__":
    main()

import streamlit as st
import requests
from bs4 import BeautifulSoup
import csv
import io
import time
from urllib.parse import urljoin, urlparse
import pandas as pd
from datetime import datetime
import re

class AnswerGenkinkaAnalyzer:
    def __init__(self):
        self.base_url = "https://answer-genkinka.jp"
        self.seed_urls = [
            "https://answer-genkinka.jp/blog/",
            "https://answer-genkinka.jp/sitemap.xml"
        ]
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
    def extract_from_sitemap(self, sitemap_url):
        """ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‹ã‚‰URLæŠ½å‡ºï¼ˆå…ƒã‚³ãƒ¼ãƒ‰ã®ç§»æ¤ï¼‰"""
        urls = []
        try:
            response = self.session.get(sitemap_url, timeout=10)
            soup = BeautifulSoup(response.content, 'xml')
            
            for loc in soup.find_all('loc'):
                url = loc.get_text()
                if self.is_article_page(url):
                    urls.append(url)
                    
        except Exception as e:
            st.warning(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            
        return list(set(urls))
    
    def is_article_page(self, url):
        """è¨˜äº‹ãƒšãƒ¼ã‚¸åˆ¤å®šï¼ˆå…ƒã‚³ãƒ¼ãƒ‰ã®ç§»æ¤ï¼‰"""
        if not url.startswith(self.base_url):
            return False
            
        path = urlparse(url).path
        
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
        exclude_patterns = [
            '/wp-admin', '/wp-content', '/wp-includes',
            '/feed', '/rss', '/sitemap', '/category', '/tag',
            '/author', '/search', '/page', '/privacy', '/terms'
        ]
        
        if any(pattern in path for pattern in exclude_patterns):
            return False
            
        # è¨˜äº‹ãƒ‘ã‚¿ãƒ¼ãƒ³
        article_patterns = [
            r'/blog/[^/]+/?$',  # /blog/è¨˜äº‹ã‚¹ãƒ©ãƒƒã‚°
            r'/\d{4}/\d{2}/[^/]+/?$',  # /YYYY/MM/è¨˜äº‹ã‚¹ãƒ©ãƒƒã‚°
            r'/[^/]+/?$'  # ãƒ«ãƒ¼ãƒˆç›´ä¸‹
        ]
        
        return any(re.match(pattern, path) for pattern in article_patterns)
    
    def extract_links(self, soup, current_url):
        """ãƒªãƒ³ã‚¯æŠ½å‡ºï¼ˆonclickå±æ€§å¯¾å¿œãƒ»å…ƒã‚³ãƒ¼ãƒ‰ã®ç§»æ¤ï¼‰"""
        links = []
        
        # é€šå¸¸ã®aã‚¿ã‚°
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if href:
                full_url = urljoin(current_url, href)
                if self.is_article_page(full_url):
                    links.append(full_url)
        
        # onclickå±æ€§å¯¾å¿œ
        for element in soup.find_all(attrs={'onclick': True}):
            onclick = element.get('onclick', '')
            if 'location.href' in onclick or 'window.open' in onclick:
                # onclick="location.href='URL'" ãƒ‘ã‚¿ãƒ¼ãƒ³
                match = re.search(r"location\.href\s*=\s*['\"]([^'\"]+)['\"]", onclick)
                if not match:
                    # onclick="window.open('URL')" ãƒ‘ã‚¿ãƒ¼ãƒ³
                    match = re.search(r"window\.open\s*\(\s*['\"]([^'\"]+)['\"]", onclick)
                
                if match:
                    href = match.group(1)
                    full_url = urljoin(current_url, href)
                    if self.is_article_page(full_url):
                        links.append(full_url)
        
        return list(set(links))
    
    def analyze_page(self, url):
        """å€‹åˆ¥ãƒšãƒ¼ã‚¸åˆ†æï¼ˆå…ƒã‚³ãƒ¼ãƒ‰ã®ç§»æ¤ï¼‰"""
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
            title_elem = soup.find('title')
            title = title_elem.get_text().strip() if title_elem else url
            
            # è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å†…ã®ãƒªãƒ³ã‚¯æŠ½å‡º
            content_selectors = [
                '.entry-content',
                '.post-content', 
                '.article-content',
                'article',
                'main'
            ]
            
            content_links = []
            for selector in content_selectors:
                content = soup.select_one(selector)
                if content:
                    content_links = self.extract_links(content, url)
                    break
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å†…ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å…¨ä½“ã‹ã‚‰æŠ½å‡º
            if not content_links:
                content_links = self.extract_links(soup, url)
            
            return {
                'url': url,
                'title': title,
                'outgoing_links': content_links,
                'status': 'success'
            }
            
        except Exception as e:
            return {
                'url': url,
                'title': f"å–å¾—ã‚¨ãƒ©ãƒ¼: {url}",
                'outgoing_links': [],
                'status': f'error: {str(e)}'
            }
    
    def discover_pagination(self, url):
        """ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³è‡ªå‹•æ¢ç´¢ï¼ˆå…ƒã‚³ãƒ¼ãƒ‰ã®ç§»æ¤ï¼‰"""
        discovered_urls = []
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # rel="next" ãƒªãƒ³ã‚¯
            next_link = soup.find('link', rel='next')
            if next_link and next_link.get('href'):
                next_url = urljoin(url, next_link.get('href'))
                discovered_urls.append(next_url)
            
            # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³è¦ç´ 
            pagination_selectors = [
                '.pagination a',
                '.page-numbers a',
                '.wp-pagenavi a',
                '.nav-links a'
            ]
            
            for selector in pagination_selectors:
                for link in soup.select(selector):
                    href = link.get('href')
                    if href:
                        full_url = urljoin(url, href)
                        if full_url.startswith(self.base_url):
                            discovered_urls.append(full_url)
            
        except Exception as e:
            st.warning(f"ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¢ç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        
        return list(set(discovered_urls))
    
    def analyze_site(self):
        """ã‚µã‚¤ãƒˆå…¨ä½“åˆ†æï¼ˆå…ƒã‚³ãƒ¼ãƒ‰ã®ç§»æ¤ãƒ»ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼‰"""
        st.info("ğŸš€ answer-genkinka.jp ã®åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: URLåé›†
        progress = st.progress(0)
        status = st.empty()
        
        status.text("URLåé›†ä¸­...")
        all_urls = set()
        
        # ã‚·ãƒ¼ãƒ‰URLã‹ã‚‰é–‹å§‹
        for seed_url in self.seed_urls:
            if seed_url.endswith('.xml'):
                sitemap_urls = self.extract_from_sitemap(seed_url)
                all_urls.update(sitemap_urls)
                st.success(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‹ã‚‰ {len(sitemap_urls)} URL ã‚’å–å¾—")
            else:
                # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¢ç´¢
                pagination_urls = self.discover_pagination(seed_url)
                all_urls.update(pagination_urls)
                all_urls.add(seed_url)
        
        progress.progress(20)
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: å„ãƒšãƒ¼ã‚¸ã‚’åˆ†æ
        status.text(f"ãƒšãƒ¼ã‚¸åˆ†æä¸­... ({len(all_urls)} ãƒšãƒ¼ã‚¸)")
        
        results = []
        for i, url in enumerate(all_urls):
            if i % 10 == 0:
                progress.progress(20 + int(60 * i / len(all_urls)))
                status.text(f"åˆ†æä¸­... {i+1}/{len(all_urls)}")
            
            result = self.analyze_page(url)
            results.append(result)
            time.sleep(0.1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
        
        progress.progress(80)
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: è¢«ãƒªãƒ³ã‚¯æ•°è¨ˆç®—
        status.text("è¢«ãƒªãƒ³ã‚¯æ•°è¨ˆç®—ä¸­...")
        
        link_counts = {}
        for result in results:
            url = result['url']
            link_counts[url] = link_counts.get(url, 0)
            
            for outgoing_url in result['outgoing_links']:
                link_counts[outgoing_url] = link_counts.get(outgoing_url, 0) + 1
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: æœ€çµ‚ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
        final_data = []
        for result in results:
            url = result['url']
            final_data.append({
                'ã‚¿ã‚¤ãƒˆãƒ«': result['title'],
                'URL': url,
                'è¢«ãƒªãƒ³ã‚¯æ•°': link_counts.get(url, 0),
                'ç™ºãƒªãƒ³ã‚¯æ•°': len(result['outgoing_links']),
                'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': result['status']
            })
        
        progress.progress(100)
        status.text("åˆ†æå®Œäº†ï¼")
        
        return final_data

def main():
    st.set_page_config(
        page_title="Answerç¾é‡‘åŒ– å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æ",
        page_icon="ğŸ”—",
        layout="wide"
    )
    
    st.title("ğŸ”— Answerç¾é‡‘åŒ– å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æ")
    st.markdown("**answer-genkinka.jpå°‚ç”¨åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆCustomTkinterå®Œå…¨ç§»æ¤ç‰ˆï¼‰**")
    
    analyzer = AnswerGenkinkaAnalyzer()
    
    # åˆ†æå®Ÿè¡Œ
    if st.button("ğŸš€ åˆ†æé–‹å§‹", type="primary"):
        
        # å®Ÿéš›ã®åˆ†æå®Ÿè¡Œ
        with st.spinner("åˆ†æä¸­..."):
            data = analyzer.analyze_site()
        
        if data:
            # è¢«ãƒªãƒ³ã‚¯æ•°ã§ã‚½ãƒ¼ãƒˆ
            df = pd.DataFrame(data)
            df_sorted = df.sort_values('è¢«ãƒªãƒ³ã‚¯æ•°', ascending=False)
            
            # çµ±è¨ˆè¡¨ç¤º
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ç·ãƒšãƒ¼ã‚¸æ•°", len(df))
            with col2:
                st.metric("ç·å†…éƒ¨ãƒªãƒ³ã‚¯æ•°", df['ç™ºãƒªãƒ³ã‚¯æ•°'].sum())
            with col3:
                st.metric("å­¤ç«‹ãƒšãƒ¼ã‚¸", len(df[df['è¢«ãƒªãƒ³ã‚¯æ•°'] == 0]))
            with col4:
                st.metric("æœ€å¤šè¢«ãƒªãƒ³ã‚¯", df['è¢«ãƒªãƒ³ã‚¯æ•°'].max())
            
            # è¢«ãƒªãƒ³ã‚¯æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°
            st.subheader("ğŸ“Š è¢«ãƒªãƒ³ã‚¯æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
            
            # ã‚°ãƒ©ãƒ•è¡¨ç¤º
            top_10 = df_sorted.head(10)
            if not top_10.empty:
                chart_data = top_10.set_index('ã‚¿ã‚¤ãƒˆãƒ«')['è¢«ãƒªãƒ³ã‚¯æ•°']
                st.bar_chart(chart_data)
            
            # è©³ç´°ãƒ†ãƒ¼ãƒ–ãƒ«
            st.subheader("ğŸ“‹ è©³ç´°ãƒ‡ãƒ¼ã‚¿")
            st.dataframe(df_sorted, use_container_width=True)
            
            # CSVå‡ºåŠ›
            csv_buffer = io.StringIO()
            df_sorted.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
            csv_data = csv_buffer.getvalue()
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"answer-genkinka_analysis_{timestamp}.csv"
            
            st.download_button(
                "ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                csv_data,
                filename,
                "text/csv",
                help="è¢«ãƒªãƒ³ã‚¯æ•°é †ã§ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸè©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ"
            )
            
            st.success("âœ… åˆ†æå®Œäº†ï¼")
        
        else:
            st.error("âŒ åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ")
    
    # è¨­å®šè¡¨ç¤º
    with st.expander("âš™ï¸ åˆ†æè¨­å®š"):
        st.markdown("""
        **å¯¾è±¡ã‚µã‚¤ãƒˆ:** answer-genkinka.jp  
        **åˆ†æç¯„å›²:** /blog/ é…ä¸‹ + ã‚µã‚¤ãƒˆãƒãƒƒãƒ—  
        **ç‰¹æ®Šæ©Ÿèƒ½:** onclickå±æ€§å¯¾å¿œã€ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³è‡ªå‹•æ¢ç´¢  
        **å‡ºåŠ›å½¢å¼:** CSVï¼ˆè¢«ãƒªãƒ³ã‚¯æ•°é †ã‚½ãƒ¼ãƒˆï¼‰
        """)

if __name__ == "__main__":
    main()

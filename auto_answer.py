# streamlit_app.py
# ------------------------------------------------------------
# ğŸ”— answer-genkinka.jpå°‚ç”¨ å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æï¼ˆStreamlitãƒ»å®Ÿã‚¯ãƒ­ãƒ¼ãƒ«å®Œå…¨ç‰ˆï¼‰
# - æ—¢å­˜CTkç‰ˆã®ç§»æ¤ + ä¸å…·åˆä¿®æ­£
# - å€‹åˆ¥è¨˜äº‹åˆ¤å®šã‚’å¼·åŒ–ï¼ˆ/blog/<slug>ã€/YYYY/MM/<slug>ã€/ç›´ä¸‹ã‚¹ãƒ©ãƒƒã‚°ï¼‰
# - ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³è‡ªå‹•æ¢ç´¢ï¼ˆrel=next, .paginationï¼‰
# - sitemap.xml ã‚‚ã‚·ãƒ¼ãƒ‰ã«è¿½åŠ ï¼ˆ/blog/é…ä¸‹å„ªå…ˆï¼‰
# - ãƒ€ãƒŸãƒ¼/ã‚µãƒ³ãƒ—ãƒ«ä¸€åˆ‡ãªã—
# ------------------------------------------------------------

import streamlit as st
import requests
from bs4 import BeautifulSoup, SoupStrainer
from urllib.parse import urlparse, urljoin, urlunparse
from datetime import datetime
import re
import csv
import io
import os
import sys
import time
from collections import deque

st.set_page_config(page_title="answer-genkinka.jp å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æ", layout="wide")

DOMAIN = "answer-genkinka.jp"
BASE = f"https://{DOMAIN}"

# ===== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====
def normalize_url(url: str) -> str:
    if not url:
        return ""
    try:
        p = urlparse(url)
        # wwwé™¤å»ãƒ»ã‚¯ã‚¨ãƒª/ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆé™¤å»ãƒ»æœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥é™¤å»
        clean = urlunparse(("https", p.netloc.replace("www.", ""), p.path.rstrip("/"), "", "", ""))
        return clean
    except Exception:
        return url

def same_site(url: str) -> bool:
    try:
        return urlparse(url).netloc.replace("www.", "") == DOMAIN
    except Exception:
        return False

# å€‹åˆ¥è¨˜äº‹ãƒ‘ã‚¿ãƒ¼ãƒ³:
ARTICLE_PATTERNS = [
    re.compile(r"^/blog/[^/]+/?$", re.IGNORECASE),            # /blog/slug
    re.compile(r"^/\d{4}/\d{1,2}/[^/]+/?$", re.IGNORECASE),   # /2025/09/slug
    re.compile(r"^/[^/]+/?$", re.IGNORECASE),                 # /slugï¼ˆãƒ«ãƒ¼ãƒˆç›´ä¸‹ï¼‰
]

def is_article_page(url: str) -> bool:
    path = urlparse(normalize_url(url)).path
    # é™¤å¤–æ‹¡å¼µå­/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    if any(path.endswith(ext) for ext in (".jpg",".jpeg",".png",".gif",".webp",".svg",".pdf",".css",".js",".zip",".rar",".7z",".mp4",".mp3",".webm",".ico")):
        return False
    if any(seg in path.lower() for seg in ["/wp-admin","/wp-json","/feed","/page/","/tag/","/category/","/author/","/site/","/search"]):
        return False
    # å€‹åˆ¥è¨˜äº‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã„ãšã‚Œã‹ã«ä¸€è‡´
    return any(pat.match(path) for pat in ARTICLE_PATTERNS)

def is_crawlable(url: str) -> bool:
    path = urlparse(normalize_url(url)).path.lower()
    if any(path.endswith(ext) for ext in (".jpg",".jpeg",".png",".gif",".webp",".svg",".pdf",".css",".js",".zip",".rar",".7z",".mp4",".mp3",".webm",".ico")):
        return False
    if any(bad in path for bad in ["/wp-admin","/wp-login","/feed","/tag/","/author/","/site/","/search"]):
        return False
    # /blog ä¸€è¦§ã‚„è¨˜äº‹ã€å¹´/æœˆã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãªã©ã¯è¨±å®¹
    return True

def get_session():
    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                      "AppleWebKit/537.36 (KHTML, like Gecko) "
                      "Chrome/120.0.0.0 Safari/537.36"
    })
    return s

def fetch(session, url, timeout=12, retries=2, sleep=0.6):
    last_exc = None
    for i in range(retries+1):
        try:
            r = session.get(url, timeout=timeout)
            # 2xxã®ã¿æ¡ç”¨
            if 200 <= r.status_code < 300:
                return r
            # 429/5xx â†’å¾…ã£ã¦å†è©¦è¡Œ
            if r.status_code in (429, 500, 502, 503, 504):
                time.sleep(sleep * (i+1))
            else:
                return r  # 404ç­‰ã¯ãã®ã¾ã¾è¿”ã™
        except Exception as e:
            last_exc = e
            time.sleep(sleep * (i+1))
    if last_exc:
        raise last_exc
    return None

CONTENT_SELECTORS = [
    ".entry-content",
    ".post-content",
    "article .entry-content",
    "article .content",
    ".single-post",
    ".p-entry__body",
    ".c-entry__content",
    ".article-body",
    "main article",
    "main .content",
    "#content article",
    "article",
    "main",
]

def extract_content_links(soup: BeautifulSoup, current_url: str):
    # æœ¬æ–‡ã«è¿‘ã„é ˜åŸŸã‚’å„ªå…ˆ
    content = None
    for sel in CONTENT_SELECTORS:
        content = soup.select_one(sel)
        if content:
            break
    if not content:
        return []

    out = []
    for a in content.find_all("a", href=True):
        href = a.get("href","").strip()
        text = (a.get_text(strip=True) or "").strip()
        if not href or href.startswith("#") or not text:
            continue
        absu = urljoin(current_url, href)
        if same_site(absu) and is_article_page(absu):
            out.append({"url": normalize_url(absu), "anchor_text": text[:120]})
    # é‡è¤‡æ’é™¤ï¼ˆURL+anchorï¼‰
    seen = set()
    uniq = []
    for d in out:
        key = (d["url"], d["anchor_text"])
        if key not in seen:
            seen.add(key)
            uniq.append(d)
    return uniq

def sanitize_title(title: str, url: str) -> str:
    if not title:
        return url
    return re.sub(r"\s*[|\-]\s*.*(answer-genkinka|ã‚¢ãƒ³ã‚µãƒ¼).*?$","", title, flags=re.IGNORECASE)

def collect_links_on_page(soup: BeautifulSoup, current_url: str):
    links = []
    for a in soup.find_all("a", href=True):
        href = a.get("href","").strip()
        if not href or href.startswith("#"):
            continue
        absu = urljoin(current_url, href)
        if same_site(absu) and is_crawlable(absu):
            links.append(normalize_url(absu))
    return list(dict.fromkeys(links))  # é †åºä¿æŒã§é‡è¤‡é™¤å»

def find_pagination_urls(soup: BeautifulSoup, current_url: str):
    """rel=next, .pagination, .nav-links ã‹ã‚‰æ¬¡ãƒšãƒ¼ã‚¸ã‚’æ¢ç´¢"""
    urls = set()
    for ln in soup.find_all("link", rel=lambda x: x and "next" in x):
        absu = urljoin(current_url, ln.get("href",""))
        if same_site(absu):
            urls.add(normalize_url(absu))
    for a in soup.select("a[rel='next'], .pagination a, .nav-links a"):
        absu = urljoin(current_url, a.get("href",""))
        if same_site(absu):
            urls.add(normalize_url(absu))
    return list(urls)

def seed_from_sitemap(session):
    """sitemap.xml ã‹ã‚‰ /blog/é…ä¸‹ã‚„è¨˜äº‹ã£ã½ã„URLã‚’ã‚·ãƒ¼ãƒ‰ã«è¿½åŠ """
    seeds = set()
    for path in ["/sitemap.xml", "/sitemap_index.xml", "/sitemap1.xml"]:
        url = BASE + path
        try:
            r = fetch(session, url, timeout=10, retries=1)
            if not r or r.status_code != 200 or "xml" not in r.headers.get("Content-Type",""):
                continue
            # è»½é‡ãƒ‘ãƒ¼ã‚¹
            only_loc = SoupStrainer("loc")
            xsoup = BeautifulSoup(r.text, "xml", parse_only=only_loc)
            for loc in xsoup.find_all("loc"):
                locu = (loc.text or "").strip()
                if not locu:
                    continue
                if same_site(locu):
                    nloc = normalize_url(locu)
                    # /blog/é…ä¸‹ or è¨˜äº‹åˆ¤å®šã«åˆè‡´
                    if "/blog/" in urlparse(nloc).path or is_article_page(nloc):
                        seeds.add(nloc)
        except Exception:
            continue
    return list(seeds)

# ===== åˆ†ææœ¬ä½“ =====
def analyze_site(start_url: str, max_pages: int, status_cb=None, progress_cb=None):
    pages = {}           # url -> {title, outbound_links[], inbound_links}
    links = []           # (source, target)
    detailed_links = []  # dict

    start_url = normalize_url(start_url)
    session = get_session()

    visited = set()
    q = deque()

    # åˆæœŸã‚­ãƒ¥ãƒ¼ï¼šå…¥åŠ›URL + ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå¾Œã§è‡ªå‹•æ¤œå‡ºï¼‰ + sitemap
    q.append(start_url)
    # sitemapã‹ã‚‰ç¨®ã¾ã
    for u in seed_from_sitemap(session)[:1000]:
        q.append(u)

    processed_count = 0

    # ãƒ•ã‚§ãƒ¼ã‚º1ï¼šåé›†ï¼ˆä¸€è¦§/è¨˜äº‹å•ã‚ãšæ‹¾ã„ã€è¨˜äº‹ã¯pagesã«ç™»éŒ²ï¼‰
    while q and len(pages) < max_pages:
        url = q.popleft()
        if url in visited or not same_site(url):
            continue
        visited.add(url)

        try:
            r = fetch(session, url, timeout=12, retries=2)
            if not r or r.status_code != 200:
                continue

            soup = BeautifulSoup(r.text, "html.parser")

            # robots noindex å›é¿
            robots = soup.find("meta", attrs={"name":"robots"})
            if robots and "noindex" in (robots.get("content","").lower()):
                continue

            # è¨˜äº‹ãªã‚‰ç™»éŒ²
            if is_article_page(url):
                h1 = soup.find("h1")
                title = sanitize_title(h1.get_text(strip=True) if h1 else url, url)
                if url not in pages:
                    pages[url] = {"title": title, "outbound_links": [], "inbound_links": 0}

            # ãã®ãƒšãƒ¼ã‚¸ã‹ã‚‰è¦‹ã¤ã‹ã‚‹ãƒªãƒ³ã‚¯ã‚’åé›†ï¼ˆä¸€è¦§ãƒ»è¨˜äº‹ã‚’å•ã‚ãšã‚¯ãƒ­ãƒ¼ãƒ«æ‹¡å¤§ï¼‰
            new_links = collect_links_on_page(soup, url)
            # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚‚åé›†
            new_links += find_pagination_urls(soup, url)
            # ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ï¼ˆæœªè¨ªå•ã®ã¿ï¼‰
            for nl in new_links:
                if nl not in visited and same_site(nl):
                    q.append(nl)

            processed_count += 1
            if status_cb:
                status_cb(f"åé›†ä¸­: è¨˜äº‹ {len(pages)} / ã‚­ãƒ¥ãƒ¼ {len(q)} / å‡¦ç† {processed_count}")
            if progress_cb:
                # è¨˜äº‹ã®é€²æ—ãƒ™ãƒ¼ã‚¹ï¼ˆæœ€å¤§å€¤max_pagesã«å¯¾ã™ã‚‹æ¯”ç‡ï¼‰
                progress_cb(min(len(pages)/max_pages, 0.98))
            time.sleep(0.03)

        except Exception:
            continue

    # ãƒ•ã‚§ãƒ¼ã‚º2ï¼šæœ¬æ–‡å†…ãƒªãƒ³ã‚¯æ§‹ç¯‰ï¼ˆè¨˜äº‹ã®ã¿ã‚’å¯¾è±¡ã«æœ¬æ–‡ã‚’å†è§£æï¼‰
    for url in list(pages.keys()):
        try:
            r = fetch(session, url, timeout=12, retries=2)
            if not r or r.status_code != 200:
                continue
            soup = BeautifulSoup(r.text, "html.parser")
            content_links = extract_content_links(soup, url)
            for ld in content_links:
                tgt = ld["url"]
                if tgt in pages and tgt != url:
                    links.append((url, tgt))
                    pages[url]["outbound_links"].append(tgt)
                    detailed_links.append({
                        "source_url": url,
                        "source_title": pages[url]["title"],
                        "target_url": tgt,
                        "anchor_text": ld["anchor_text"],
                    })
            if status_cb:
                status_cb(f"ãƒªãƒ³ã‚¯è§£æä¸­: {pages[url]['title'][:28]}â€¦")
            time.sleep(0.02)
        except Exception:
            continue

    # è¢«ãƒªãƒ³ã‚¯æ•°
    for u in pages:
        pages[u]["inbound_links"] = sum(1 for s,t in links if t == u)

    if progress_cb:
        progress_cb(1.0)

    return pages, links, detailed_links

# ===== CSV =====
def build_csv_bytes(pages: dict, detailed_links: list[dict]) -> bytes:
    buf = io.StringIO()
    w = csv.writer(buf)
    w.writerow(["ç•ªå·","ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«","URL","è¢«ãƒªãƒ³ã‚¯å…ƒã‚¿ã‚¤ãƒˆãƒ«","è¢«ãƒªãƒ³ã‚¯å…ƒURL","ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ"])

    targets = {}
    for d in detailed_links:
        targets.setdefault(d["target_url"], []).append(d)

    for i, (tgt, lst) in enumerate(sorted(targets.items(), key=lambda x: len(x[1]), reverse=True), start=1):
        ttitle = pages.get(tgt,{}).get("title", tgt)
        for d in lst:
            w.writerow([i, ttitle, tgt, d["source_title"], d["source_url"], d["anchor_text"]])

    # å­¤ç«‹ãƒšãƒ¼ã‚¸
    row = len(detailed_links) + 1
    for u, info in pages.items():
        if info.get("inbound_links",0) == 0:
            w.writerow([row, info.get("title",u), u, "", "", ""])
            row += 1

    data = buf.getvalue().encode("utf-8-sig")
    buf.close()
    return data

def auto_save_csv(pages: dict, detailed_links: list[dict]) -> str | None:
    try:
        if hasattr(sys, "_MEIPASS"):
            base_dir = os.path.dirname(sys.executable)
        else:
            base_dir = os.path.dirname(os.path.abspath(__file__))
        d = datetime.now().strftime("%Y-%m-%d")
        folder = os.path.join(base_dir, d)
        os.makedirs(folder, exist_ok=True)
        path = os.path.join(folder, f"answer-genkinka-{d}.csv")
        with open(path, "wb") as f:
            f.write(build_csv_bytes(pages, detailed_links))
        return path
    except Exception:
        return None

# ===== UI =====
st.title("ğŸ”— answer-genkinka.jp å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æï¼ˆStreamlitãƒ»å®Ÿã‚¯ãƒ­ãƒ¼ãƒ«ï¼‰")

col1, col2 = st.columns([0.7, 0.3])
with col1:
    start_url = st.text_input("é–‹å§‹URL", value=f"{BASE}/blog/", help="ä¾‹: https://answer-genkinka.jp/blog/")
with col2:
    max_pages = st.number_input("è¨˜äº‹åé›†ä¸Šé™", 50, 5000, 800, 50)
    do_autosave = st.checkbox("å®Œäº†å¾Œã«è‡ªå‹•ä¿å­˜ï¼ˆCSVï¼‰", value=True)

status = st.empty()
prog = st.progress(0.0)

def status_cb(msg): status.info(msg)
def progress_cb(v): prog.progress(min(max(v,0.0),1.0))

run = st.button("åˆ†æé–‹å§‹", type="primary")

# åˆå›è‡ªå‹•å®Ÿè¡Œ
if "auto_done" not in st.session_state:
    st.session_state.auto_done = False
if not st.session_state.auto_done and start_url.strip():
    run = True
    st.session_state.auto_done = True

if run:
    try:
        status_cb("é–‹å§‹ã—ã¾ã™â€¦")
        pages, links, detailed = analyze_site(start_url=start_url, max_pages=int(max_pages),
                                              status_cb=status_cb, progress_cb=progress_cb)
        total = len(pages)
        isolated = sum(1 for p in pages.values() if p.get("inbound_links",0) == 0)
        popular5 = sum(1 for p in pages.values() if p.get("inbound_links",0) >= 5)
        st.success(f"åˆ†æå®Œäº†: è¨˜äº‹ {total} / ãƒªãƒ³ã‚¯ {len(links)} / å­¤ç«‹ {isolated} / äººæ°—(5+) {popular5}")

        left, right = st.columns([0.35, 0.65])
        with left:
            st.subheader("ã‚µãƒãƒª")
            st.metric("è¨˜äº‹æ•°", total)
            st.metric("ãƒªãƒ³ã‚¯æ•°", len(links))
            st.metric("å­¤ç«‹", isolated)
            st.metric("äººæ°—(è¢«10+)", sum(1 for p in pages.values() if p.get("inbound_links",0) >= 10))

            csv_bytes = build_csv_bytes(pages, detailed)
            st.download_button(
                "è©³ç´°CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv_bytes,
                file_name=f"answer-genkinka-{datetime.now().strftime('%Y-%m-%d')}.csv",
                mime="text/csv",
            )
            if do_autosave:
                pth = auto_save_csv(pages, detailed)
                if pth:
                    st.caption(f"è‡ªå‹•ä¿å­˜: {pth}")

        with right:
            st.subheader("è¨˜äº‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆè¢«ãƒªãƒ³ã‚¯é™é †ï¼‰")
            for i, (u, info) in enumerate(sorted(pages.items(), key=lambda x: x[1].get("inbound_links",0), reverse=True), start=1):
                inbound = info.get("inbound_links",0)
                outbound = len(info.get("outbound_links",[]))
                if inbound == 0: tag = "ğŸš¨è¦æ”¹å–„"
                elif inbound >= 10: tag = "ğŸ†è¶…äººæ°—"
                elif inbound >= 5: tag = "âœ…äººæ°—"
                else: tag = "âš ï¸æ™®é€š"
                st.markdown(
                    f"**{i}. {info.get('title','')[:60]}**  \n"
                    f"è¢«ãƒªãƒ³ã‚¯: **{inbound}** / ç™ºãƒªãƒ³ã‚¯: {outbound} / {tag}  \n"
                    f"[{u}]({u})"
                )

        with st.expander("æœ¬æ–‡å†…ãƒªãƒ³ã‚¯ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ¥ è©³ç´°ï¼‰", expanded=False):
            targets = {}
            for d in detailed:
                targets.setdefault(d["target_url"], []).append(d)
            for tgt, lst in sorted(targets.items(), key=lambda x: len(x[1]), reverse=True)[:300]:
                st.markdown(f"**{pages.get(tgt,{}).get('title',tgt)}**  \n[{tgt}]({tgt})  \nè¢«ãƒªãƒ³ã‚¯: **{len(lst)}**")
                for dd in lst[:80]:
                    st.markdown(f"- from: [{dd['source_title']}]({dd['source_url']})  \n  ã‚¢ãƒ³ã‚«ãƒ¼: `{dd['anchor_text']}`")
                st.divider()

        status_cb("å®Œäº†")
        progress_cb(1.0)

    except Exception as e:
        st.error(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
else:
    st.info("ã€Œåˆ†æé–‹å§‹ã€ã‚’æŠ¼ã™ã‹ã€ãƒšãƒ¼ã‚¸è¡¨ç¤ºç›´å¾Œã®è‡ªå‹•å®Ÿè¡Œã§ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

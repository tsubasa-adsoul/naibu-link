import tkinter as tk
from tkinter import messagebox, filedialog
import customtkinter as ctk
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import json
import time
import re
import threading
import webbrowser
from datetime import datetime
import csv
import sys
import os

ctk.set_appearance_mode("light")
ctk.set_default_color_theme("blue")

class LinkAnalyzerApp:
    def __init__(self):
        self.root = ctk.CTk()
        self.root.title("ğŸ”— arigatayaå°‚ç”¨å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆonclickå¯¾å¿œãƒ»å…¨è‡ªå‹•ç‰ˆï¼‰")
        self.root.geometry("1200x800")

        self.pages = {}
        self.links = []
        self.detailed_links = []  # è©³ç´°ãƒªãƒ³ã‚¯æƒ…å ±ç”¨
        self.analysis_data = None
        self.is_analyzing = False

        self.setup_ui()
        
        # å…¨è‡ªå‹•åŒ–ï¼šèµ·å‹•1ç§’å¾Œã«è‡ªå‹•ã§åˆ†æé–‹å§‹
        self.root.after(1000, self.auto_start_analysis)

    def setup_ui(self):
        self.main_frame = ctk.CTkScrollableFrame(self.root)
        self.main_frame.pack(fill="both", expand=True, padx=20, pady=20)

        title_label = ctk.CTkLabel(self.main_frame, text="ğŸ”— arigatayaå°‚ç”¨å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆonclickå¯¾å¿œãƒ»å…¨è‡ªå‹•ç‰ˆï¼‰",
                                   font=ctk.CTkFont(size=28, weight="bold"))
        title_label.pack(pady=20)

        self.url_entry = ctk.CTkEntry(self.main_frame, placeholder_text="https://example.com", width=500)
        self.url_entry.pack(pady=10)
        self.url_entry.insert(0, "https://arigataya.co.jp")

        self.analyze_button = ctk.CTkButton(self.main_frame, text="åˆ†æé–‹å§‹", command=self.start_analysis)
        self.analyze_button.pack(pady=10)

        self.status_label = ctk.CTkLabel(self.main_frame, text="")
        self.status_label.pack(pady=10)

        self.progress_bar = ctk.CTkProgressBar(self.main_frame, width=400)
        self.progress_bar.pack(pady=5)
        self.progress_bar.stop()

        self.result_frame = ctk.CTkScrollableFrame(self.main_frame, height=400)
        self.result_frame.pack(fill="both", expand=True, pady=20)

        # è©³ç´°CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒœã‚¿ãƒ³ã®ã¿ï¼ˆå¾“æ¥CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã¯å‰Šé™¤ï¼‰
        button_frame = ctk.CTkFrame(self.main_frame)
        button_frame.pack(pady=10)
        
        self.detailed_export_btn = ctk.CTkButton(button_frame, text="è©³ç´°CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ", command=self.export_detailed_csv)
        self.detailed_export_btn.pack(side="left", padx=5)

    def auto_start_analysis(self):
        """å…¨è‡ªå‹•åŒ–ï¼šè‡ªå‹•åˆ†æé–‹å§‹"""
        print("=== arigataya.co.jp è‡ªå‹•åˆ†æé–‹å§‹ ===")
        self.start_analysis()

    def start_analysis(self):
        url = self.url_entry.get().strip()
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url

        self.analyze_button.configure(state="disabled")
        self.status_label.configure(text="ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­...")
        self.progress_bar.start()

        thread = threading.Thread(target=self.analyze_site, args=(url,))
        thread.daemon = True
        thread.start()

    def extract_from_sitemap(self, url):
        urls = set()
        try:
            res = requests.get(url, timeout=10)
            res.raise_for_status()
            soup = BeautifulSoup(res.content, 'xml')
            locs = soup.find_all('loc')
            if soup.find('sitemapindex'):
                for loc in locs:
                    urls.update(self.extract_from_sitemap(loc.text.strip()))
            else:
                for loc in locs:
                    loc_url = loc.text.strip()
                    # ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«è‡ªä½“ã¯é™¤å¤–
                    if not re.search(r'sitemap.*\.(xml|html)$', loc_url.lower()):
                        urls.add(self.normalize_url(loc_url))
        except:
            pass
        return list(urls)

    def generate_seed_urls(self, base_url):
        sitemap_root = urljoin(base_url, '/sitemap.xml')
        sitemap_urls = self.extract_from_sitemap(sitemap_root)
        print(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‹ã‚‰ {len(sitemap_urls)} å€‹ã®URLã‚’å–å¾—")
        return list(set([self.normalize_url(base_url)] + sitemap_urls))

    def is_content(self, url):
        # æ­£è¦åŒ–å¾Œã®URLã§åˆ¤å®š
        normalized_url = self.normalize_url(url)
        path = urlparse(normalized_url).path.lower().split('?')[0].rstrip('/')
        
        # ã¾ãšé‡è¦ãªãƒšãƒ¼ã‚¸ã‚’æ˜ç¤ºçš„ã«è¨±å¯
        if any(re.search(pattern, path) for pattern in [
            r'^/category/[a-z0-9\-]+$',           # ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸
            r'^/category/[a-z0-9\-]+/page/\d+$',  # ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
            r'^/[a-z0-9\-]+$',                    # è¨˜äº‹ãƒšãƒ¼ã‚¸
            r'^/$',                               # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸
            r'^$'                                 # ãƒ«ãƒ¼ãƒˆ
        ]):
            return True
        
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
        if any(re.search(e, path) for e in [
            r'/sitemap',                    # sitemap ã‚’å«ã‚€ãƒ‘ã‚¹å…¨ã¦
            r'sitemap.*\.(xml|html)$',      # sitemap.xml, sitemap-*.html ç­‰
            r'/page/\d+$',                  # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆã‚«ãƒ†ã‚´ãƒªä»¥å¤–ï¼‰
            r'-mg',                         # -mg ã‚’å«ã‚€URL
            r'/site/',                      # /site/ ã‚’å«ã‚€URL
            r'/wp-',                        # WordPressé–¢é€£
            r'/tag/',                       # ã‚¿ã‚°ãƒšãƒ¼ã‚¸
            r'/wp-content',                 # WordPress ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            r'/wp-admin',                   # WordPress ç®¡ç†ç”»é¢
            r'/wp-json',                    # WordPress JSON API
            r'#',                           # ã‚¢ãƒ³ã‚«ãƒ¼ãƒªãƒ³ã‚¯
            r'\?utm_',                      # UTMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            r'/feed/',                      # RSS/Atom ãƒ•ã‚£ãƒ¼ãƒ‰
            r'mailto:',                     # ãƒ¡ãƒ¼ãƒ«ãƒªãƒ³ã‚¯
            r'tel:',                        # é›»è©±ãƒªãƒ³ã‚¯
            r'/privacy',                    # ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒãƒªã‚·ãƒ¼
            r'/terms',                      # åˆ©ç”¨è¦ç´„
            r'/contact',                    # ãŠå•ã„åˆã‚ã›
            r'/go-',                        # ã‚¯ãƒƒã‚·ãƒ§ãƒ³ãƒšãƒ¼ã‚¸ï¼ˆgo-ã§å§‹ã¾ã‚‹ï¼‰
            r'/redirect',                   # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆãƒšãƒ¼ã‚¸
            r'/exit',                       # é›¢è„±ãƒšãƒ¼ã‚¸
            r'/out',                        # å¤–éƒ¨ãƒªãƒ³ã‚¯ãƒšãƒ¼ã‚¸
            r'\.(jpg|jpeg|png|gif|webp|svg|ico|pdf|zip|rar|doc|docx|xls|xlsx)$'  # ãƒ•ã‚¡ã‚¤ãƒ«
        ]):
            return False
        
        # ãã®ä»–ã¯åŸºæœ¬çš„ã«è¨±å¯ï¼ˆè¨˜äº‹ãƒšãƒ¼ã‚¸ã®å¯èƒ½æ€§ãŒé«˜ã„ï¼‰
        return True

    def is_noindex_page(self, soup):
        """NOINDEXãƒšãƒ¼ã‚¸ã‹ã©ã†ã‹åˆ¤å®š"""
        try:
            # robots meta ã‚¿ã‚°ã‚’ãƒã‚§ãƒƒã‚¯
            meta_robots = soup.find('meta', attrs={'name': 'robots'})
            if meta_robots and meta_robots.get('content'):
                content = meta_robots.get('content').lower()
                if 'noindex' in content:
                    return True
            
            # å€‹åˆ¥ã®noindex meta ã‚¿ã‚°ã‚‚ãƒã‚§ãƒƒã‚¯
            noindex_meta = soup.find('meta', attrs={'name': 'googlebot', 'content': lambda x: x and 'noindex' in x.lower()})
            if noindex_meta:
                return True
                
            # ã‚¯ãƒƒã‚·ãƒ§ãƒ³ãƒšãƒ¼ã‚¸ã®å…¸å‹çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
            title = soup.find('title')
            if title and title.get_text():
                title_text = title.get_text().lower()
                if any(keyword in title_text for keyword in ['å¤–éƒ¨ã‚µã‚¤ãƒˆ', 'ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ', 'ç§»å‹•ä¸­', 'å¤–éƒ¨ãƒªãƒ³ã‚¯', 'cushion']):
                    return True
            
            # bodyã«è­¦å‘Šãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆï¼ˆã‚¯ãƒƒã‚·ãƒ§ãƒ³ãƒšãƒ¼ã‚¸ã®å…¸å‹ï¼‰
            body_text = soup.get_text().lower()
            if any(phrase in body_text for phrase in [
                'å¤–éƒ¨ã‚µã‚¤ãƒˆã«ç§»å‹•ã—ã¾ã™',
                'ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã—ã¦ã„ã¾ã™',
                'å¤–éƒ¨ãƒªãƒ³ã‚¯ã§ã™',
                'åˆ¥ã‚µã‚¤ãƒˆã«ç§»å‹•',
                'ã“ã®ãƒªãƒ³ã‚¯ã¯å¤–éƒ¨ã‚µã‚¤ãƒˆ'
            ]):
                return True
                
            return False
            
        except Exception as e:
            print(f"NOINDEXãƒã‚§ãƒƒã‚¯ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def analyze_site(self, base_url):
        try:
            pages = {}
            links = []
            detailed_links = []  # è©³ç´°ãƒªãƒ³ã‚¯æƒ…å ±ç”¨
            visited = set()
            to_visit = self.generate_seed_urls(base_url)
            domain = urlparse(base_url).netloc
            processed_links = set()  # é‡è¤‡é™¤å»ç”¨

            session = requests.Session()
            session.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'})

            print(f"åˆæœŸã‚·ãƒ¼ãƒ‰URLæ•°: {len(to_visit)}")
            
            # é‡è¤‡ã‚’é™¤å»
            unique_to_visit = []
            seen = set()
            for url in to_visit:
                normalized = self.normalize_url(url)
                if normalized not in seen:
                    seen.add(normalized)
                    unique_to_visit.append(normalized)
            
            to_visit = unique_to_visit
            print(f"é‡è¤‡é™¤å»å¾Œã®ã‚·ãƒ¼ãƒ‰URLæ•°: {len(to_visit)}")

            while to_visit and len(pages) < 500:
                url = to_visit.pop(0)
                normalized_url = self.normalize_url(url)
                if normalized_url in visited:
                    continue

                try:
                    response = session.get(url, timeout=10)
                    if response.status_code != 200:
                        continue
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # NOINDEXãƒšãƒ¼ã‚¸ã‚’é™¤å¤–
                    if self.is_noindex_page(soup):
                        print(f"NOINDEXãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒƒãƒ—: {url}")
                        visited.add(normalized_url)
                        continue
                    
                    # æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹
                    extracted_links = self.extract_links(soup)
                    new_links_found = 0

                    # ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’ä¿å­˜
                    title = soup.title.string.strip() if soup.title and soup.title.string else normalized_url
                    
                    # arigataya | ã‚ã‚ŠãŒãŸã‚„ ãªã©ã®ã‚µã‚¤ãƒˆåã‚’é™¤å»
                    title = re.sub(r'\s*[|\-]\s*.*(arigataya|ã‚ã‚ŠãŒãŸã‚„).*$', '', title, flags=re.IGNORECASE)
                    
                    pages[normalized_url] = {
                        'title': title,
                        'outbound_links': []
                    }

                    # ã“ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ã®å†…éƒ¨ãƒªãƒ³ã‚¯ã‚’è¨˜éŒ²
                    for link_data in extracted_links:
                        link_url = link_data['url']
                        anchor_text = link_data['anchor_text']
                        normalized_link = self.normalize_url(link_url)
                        
                        if (self.is_internal(normalized_link, domain) and 
                            self.is_content(normalized_link)):
                            
                            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆåŒã˜ã‚½ãƒ¼ã‚¹â†’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ãƒªãƒ³ã‚¯ã¯1å›ã ã‘ï¼‰
                            link_key = (normalized_url, normalized_link)
                            if link_key not in processed_links:
                                processed_links.add(link_key)
                                
                                links.append((normalized_url, normalized_link))
                                pages[normalized_url]['outbound_links'].append(normalized_link)
                                
                                # è©³ç´°ãƒªãƒ³ã‚¯æƒ…å ±ã‚’ä¿å­˜
                                detailed_links.append({
                                    'source_url': normalized_url,
                                    'source_title': title,
                                    'target_url': normalized_link,
                                    'anchor_text': anchor_text
                                })
                            
                            # æ–°è¦URLã®ç™ºè¦‹
                            if (normalized_link not in visited and 
                                normalized_link not in to_visit):
                                to_visit.append(normalized_link)
                                new_links_found += 1

                    visited.add(normalized_url)

                    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
                    status_text = f"{len(pages)}ä»¶ç›®: {normalized_url[:60]}..."
                    if new_links_found > 0:
                        status_text += f" (æ–°è¦ãƒªãƒ³ã‚¯{new_links_found}ä»¶ç™ºè¦‹)"
                    
                    self.root.after(0, lambda text=status_text: self.status_label.configure(text=text))
                    self.root.after(0, lambda: self.progress_bar.set(min(len(pages) / 500, 1.0)))

                    time.sleep(0.1)

                except Exception as e:
                    print(f"ã‚¨ãƒ©ãƒ¼: {url} - {e}")
                    continue

            # è¢«ãƒªãƒ³ã‚¯æ•°ã‚’è¨ˆç®—
            for url in pages:
                pages[url]['inbound_links'] = sum(1 for _, tgt in links if tgt == url)

            self.pages = pages
            self.links = links
            self.detailed_links = detailed_links
            
            print(f"æœ€çµ‚çµæœ: {len(pages)}ãƒšãƒ¼ã‚¸, {len(links)}ãƒªãƒ³ã‚¯")
            self.root.after(0, self.show_results)
            
        except Exception as e:
            self.root.after(0, lambda: messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ã‚µã‚¤ãƒˆåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"))
        finally:
            self.root.after(0, lambda: self.analyze_button.configure(state="normal"))
            self.root.after(0, lambda: self.progress_bar.stop())
            self.root.after(0, lambda: self.status_label.configure(text="åˆ†æå®Œäº†"))
            
            # å…¨è‡ªå‹•åŒ–ï¼šåˆ†æå®Œäº†2ç§’å¾Œã«è‡ªå‹•ã§CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            self.root.after(2000, self.auto_export_csv)

    def auto_export_csv(self):
        """å…¨è‡ªå‹•åŒ–ï¼šè‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆEXEåŒ–å¯¾å¿œãƒ»ã‚¨ãƒ©ãƒ¼å¯¾ç­–å¼·åŒ–ç‰ˆï¼‰"""
        try:
            print("=== è‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆé–‹å§‹ ===")
            
            # ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ç¢ºèª
            if not self.detailed_links and not self.pages:
                print("ã‚¨ãƒ©ãƒ¼: åˆ†æãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                self.status_label.configure(text="åˆ†æå®Œäº†ï¼ˆãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰")
                return
            
            # EXEåŒ–å¯¾å¿œï¼šå®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
            if hasattr(sys, '_MEIPASS'):
                # PyInstaller ã§EXEåŒ–ã•ã‚ŒãŸå ´åˆ
                base_dir = os.path.dirname(sys.executable)
                print(f"EXEåŒ–ç’°å¢ƒæ¤œå‡º: {base_dir}")
            else:
                # é€šå¸¸ã®Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œã®å ´åˆ
                base_dir = os.path.dirname(os.path.abspath(__file__))
                print(f"é€šå¸¸ç’°å¢ƒæ¤œå‡º: {base_dir}")
            
            # æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
            today = datetime.now()
            date_folder = today.strftime("%Y-%m-%d")  # 2025-09-06 å½¢å¼
            folder_path = os.path.join(base_dir, date_folder)
            
            print(f"ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹: {folder_path}")
            
            # ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
            try:
                os.makedirs(folder_path, exist_ok=True)
                print(f"æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆå®Œäº†: {date_folder}")
            except Exception as e:
                print(f"ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
                # ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆã«å¤±æ•—ã—ãŸå ´åˆã¯ã€ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç›´æ¥ä¿å­˜
                folder_path = base_dir
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«åè¨­å®šï¼ˆå®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åï¼‰
            timestamp = today.strftime("%Y%m%d_%H%M%S")
            csv_filename = f"arigataya_{timestamp}.csv"
            filename = os.path.join(folder_path, csv_filename)
            
            print(f"CSVãƒ•ã‚¡ã‚¤ãƒ«å: {csv_filename}")
            print(f"å®Œå…¨ãƒ‘ã‚¹: {filename}")
            
            # è©³ç´°CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Ÿè¡Œ
            success = self.export_detailed_csv_to_file(filename)
            
            if success:
                print(f"=== è‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {csv_filename} ===")
                self.status_label.configure(text=f"åˆ†æå®Œäº†ï¼CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {csv_filename}")
            else:
                print("=== è‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¤±æ•— ===")
                self.status_label.configure(text="åˆ†æå®Œäº†ï¼ˆCSVä¿å­˜å¤±æ•—ï¼‰")
            
        except Exception as e:
            print(f"è‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            self.status_label.configure(text="åˆ†æå®Œäº†ï¼ˆCSVä¿å­˜ã‚¨ãƒ©ãƒ¼ï¼‰")

    def extract_links(self, soup):
        """arigatayaå°‚ç”¨ã®æœ€é©åŒ–ã•ã‚ŒãŸãƒªãƒ³ã‚¯æŠ½å‡ºï¼ˆonclickå¯¾å¿œç‰ˆï¼‰"""
        selectors = [
            '.post_content',           # crecaeruå°‚ç”¨
            '.entry-content',          # WordPressä¸€èˆ¬
            '.article-content',        # è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            'main .content',           # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            '[class*="content"]',      # content ã‚’å«ã‚€ã‚¯ãƒ©ã‚¹
            'main',                    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
            'article'                  # è¨˜äº‹ã‚¨ãƒªã‚¢
        ]
        
        for selector in selectors:
            areas = soup.select(selector)
            if areas:
                links = []
                for area in areas:
                    # é™¤å¤–è¦ç´ ã‚’å‰Šé™¤ï¼ˆé‡è¦ï¼šã‚µã‚¤ãƒ‰ãƒãƒ¼ã€ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ç­‰ï¼‰
                    for exclude in area.select('header, footer, nav, aside, .sidebar, .widget, .share, .related, .popular-posts, .breadcrumb, .author-box, .navigation'):
                        exclude.decompose()
                    
                    # 1. é€šå¸¸ã®aã‚¿ã‚°ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
                    found_links = area.find_all('a', href=True)
                    for link in found_links:
                        anchor_text = link.get_text(strip=True) or link.get('title', '') or '[ãƒªãƒ³ã‚¯]'
                        links.append({
                            'url': link['href'],
                            'anchor_text': anchor_text[:100]
                        })
                    
                    # 2. onclickå±æ€§ã®ã‚ã‚‹ã‚¿ã‚°ã‚’æŠ½å‡ºï¼ˆarigatayaç‰¹æœ‰ï¼‰
                    onclick_elements = area.find_all(attrs={'onclick': True})
                    for element in onclick_elements:
                        onclick_attr = element.get('onclick', '')
                        # "window.location.href='URL'" ã‹ã‚‰URLã‚’æŠ½å‡º
                        url_match = re.search(r"window\.location\.href\s*=\s*['\"]([^'\"]+)['\"]", onclick_attr)
                        if url_match:
                            extracted_url = url_match.group(1)
                            anchor_text = element.get_text(strip=True) or element.get('title', '') or '[onclick ãƒªãƒ³ã‚¯]'
                            links.append({
                                'url': extracted_url,
                                'anchor_text': anchor_text[:100]
                            })
                            print(f"onclick ãƒªãƒ³ã‚¯æ¤œå‡º: {extracted_url} (ã‚¢ãƒ³ã‚«ãƒ¼: {anchor_text})")
                    
                if links:
                    print(f"ã‚»ãƒ¬ã‚¯ã‚¿ '{selector}' ã§ {len(links)} å€‹ã®ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹")
                    return links
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå…¨ä½“ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’å–å¾—ï¼ˆãŸã ã—é™¤å¤–ã‚¨ãƒªã‚¢ã¯é¿ã‘ã‚‹ï¼‰
        print("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…¨ä½“ã‹ã‚‰ãƒªãƒ³ã‚¯æŠ½å‡º")
        all_links = []
        
        # é€šå¸¸ã®aã‚¿ã‚°
        for link in soup.find_all('a', href=True):
            anchor_text = link.get_text(strip=True) or link.get('title', '') or '[ãƒªãƒ³ã‚¯]'
            all_links.append({
                'url': link['href'],
                'anchor_text': anchor_text[:100]
            })
        
        # onclickå±æ€§ã®ã‚¿ã‚°
        onclick_elements = soup.find_all(attrs={'onclick': True})
        for element in onclick_elements:
            onclick_attr = element.get('onclick', '')
            url_match = re.search(r"window\.location\.href\s*=\s*['\"]([^'\"]+)['\"]", onclick_attr)
            if url_match:
                extracted_url = url_match.group(1)
                anchor_text = element.get_text(strip=True) or element.get('title', '') or '[onclick ãƒªãƒ³ã‚¯]'
                all_links.append({
                    'url': extracted_url,
                    'anchor_text': anchor_text[:100]
                })
        
        return all_links

    def normalize_url(self, url):
        parsed = urlparse(url)
        scheme = 'https'
        netloc = parsed.netloc.replace('www.', '')
        path = parsed.path.rstrip('/')

        # arigataya.co.jpå°‚ç”¨ã®æ­£è¦åŒ–
        if '/wp/' in path:
            path = path.replace('/wp/', '/')

        # é‡è¤‡ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’ä¿®æ­£
        path = re.sub(r'/+', '/', path)

        # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ä»¥å¤–ã«ã¯æœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’ä»˜ä¸
        if path and not path.endswith('/'):
            path += '/'

        return f"{scheme}://{netloc}{path}"

    def is_internal(self, url, domain):
        return urlparse(url).netloc.replace('www.', '') == domain.replace('www.', '')

    def show_results(self):
        self.result_frame.configure(height=400)
        for widget in self.result_frame.winfo_children():
            widget.destroy()

        # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
        total_pages = len(self.pages)
        total_links = len(self.links)
        isolated_pages = sum(1 for p in self.pages.values() if p['inbound_links'] == 0)
        popular_pages = sum(1 for p in self.pages.values() if p['inbound_links'] >= 5)

        stats_label = ctk.CTkLabel(self.result_frame,
                                   text=f"åˆ†æçµæœ: {total_pages}ãƒšãƒ¼ã‚¸ | {total_links}å†…éƒ¨ãƒªãƒ³ã‚¯ | å­¤ç«‹è¨˜äº‹{isolated_pages}ä»¶ | äººæ°—è¨˜äº‹{popular_pages}ä»¶",
                                   font=ctk.CTkFont(size=16, weight="bold"))
        stats_label.pack(pady=10)

        # çµæœä¸€è¦§ã‚’è¡¨ç¤º
        sorted_pages = sorted(self.pages.items(), key=lambda x: x[1]['inbound_links'], reverse=True)
        for i, (url, info) in enumerate(sorted_pages):
            # SEOè©•ä¾¡
            if info['inbound_links'] == 0:
                evaluation = "è¦æ”¹å–„"
            elif info['inbound_links'] >= 10:
                evaluation = "è¶…äººæ°—"
            elif info['inbound_links'] >= 5:
                evaluation = "äººæ°—"
            else:
                evaluation = "æ™®é€š"

            label_text = f"{i+1:3d}. {info['title'][:50]}...\n     è¢«ãƒªãƒ³ã‚¯:{info['inbound_links']:3d} | ç™ºãƒªãƒ³ã‚¯:{len(info['outbound_links']):3d} | {evaluation}\n     {url}"
            
            label = ctk.CTkLabel(self.result_frame,
                                 text=label_text,
                                 anchor="w", justify="left",
                                 font=ctk.CTkFont(size=11))
            label.pack(fill="x", padx=10, pady=2)

    def export_detailed_csv(self):
        """è©³ç´°CSVå‡ºåŠ›ï¼ˆè¢«ãƒªãƒ³ã‚¯è©³ç´°ï¼‰"""
        if not self.detailed_links and not self.pages:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            return

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚¡ã‚¤ãƒ«åã‚’å®‰å…¨ãªå½¢å¼ã§ç”Ÿæˆ
        today = datetime.now().strftime("%Y%m%d_%H%M%S")
        default_filename = f"arigataya_{today}.csv"

        filename = filedialog.asksaveasfilename(
            defaultextension=".csv", 
            filetypes=[("CSV files", "*.csv")],
            initialvalue=default_filename
        )
        
        if filename:
            success = self.export_detailed_csv_to_file(filename)
            if success:
                messagebox.showinfo("å®Œäº†", f"è©³ç´°CSVã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
            else:
                messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "CSVä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")

    def export_detailed_csv_to_file(self, filename):
        """ä¿®æ­£ç‰ˆï¼šåŒä¸€ãƒšãƒ¼ã‚¸ã«ã¯åŒä¸€ç•ªå·ã‚’å‰²ã‚Šå½“ã¦ï¼ˆã‚¨ãƒ©ãƒ¼å¯¾ç­–å¼·åŒ–ç‰ˆï¼‰"""
        try:
            print(f"CSVä¿å­˜é–‹å§‹: {filename}")
            
            # ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ç¢ºèª
            if not self.pages:
                print("ã‚¨ãƒ©ãƒ¼: ãƒšãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                return False
            
            print(f"ãƒšãƒ¼ã‚¸æ•°: {len(self.pages)}")
            print(f"è©³ç´°ãƒªãƒ³ã‚¯æ•°: {len(self.detailed_links)}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ãƒ†ã‚¹ãƒˆ
            try:
                with open(filename, 'w', newline='', encoding='utf-8-sig') as test_file:
                    test_file.write("test")
                print("ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ãƒ†ã‚¹ãƒˆæˆåŠŸ")
            except Exception as e:
                print(f"ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
                return False
            
            with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                
                # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œï¼ˆ6åˆ—æ§‹æˆï¼‰
                writer.writerow([
                    'A_ç•ªå·', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL',
                    'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL', 'F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'
                ])
                print("ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œæ›¸ãè¾¼ã¿å®Œäº†")
                
                # **ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆï¼šåŒä¸€ãƒšãƒ¼ã‚¸ã«ã¯åŒä¸€ç•ªå·ã‚’å‰²ã‚Šå½“ã¦**
                
                # 1. å…¨ãƒšãƒ¼ã‚¸ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒªã‚¹ãƒˆã‚’ä½œæˆï¼ˆè¢«ãƒªãƒ³ã‚¯æ•°ã§ã‚½ãƒ¼ãƒˆï¼‰
                unique_pages = []
                for url, info in self.pages.items():
                    try:
                        unique_pages.append({
                            'url': url,
                            'title': str(info.get('title', url))[:200],  # é•·ã™ãã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’åˆ¶é™
                            'inbound_links': int(info.get('inbound_links', 0))
                        })
                    except Exception as e:
                        print(f"ãƒšãƒ¼ã‚¸å‡¦ç†ã‚¨ãƒ©ãƒ¼: {url} - {e}")
                        continue
                
                # è¢«ãƒªãƒ³ã‚¯æ•°ã§ã‚½ãƒ¼ãƒˆï¼ˆå¤šã„é †ï¼‰
                unique_pages.sort(key=lambda x: x['inbound_links'], reverse=True)
                print(f"ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒšãƒ¼ã‚¸æ•°: {len(unique_pages)}")
                
                # 2. ãƒšãƒ¼ã‚¸ç•ªå·ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
                page_number_map = {}
                for i, page in enumerate(unique_pages, 1):
                    page_number_map[page['url']] = i
                
                # 3. è¢«ãƒªãƒ³ã‚¯ã‚ã‚Šãƒšãƒ¼ã‚¸ã®å‡ºåŠ›ï¼ˆåŒã˜ãƒšãƒ¼ã‚¸ã«ã¯åŒã˜ç•ªå·ï¼‰
                written_rows = 0
                for link in self.detailed_links:
                    try:
                        target_url = link.get('target_url', '')
                        if not target_url or target_url not in page_number_map:
                            continue
                            
                        target_info = self.pages.get(target_url, {})
                        target_title = str(target_info.get('title', target_url))[:200]
                        page_number = page_number_map[target_url]
                        
                        # å®‰å…¨ãªæ–‡å­—åˆ—å‡¦ç†
                        source_title = str(link.get('source_title', ''))[:200]
                        source_url = str(link.get('source_url', ''))[:500]
                        anchor_text = str(link.get('anchor_text', ''))[:200]
                        
                        writer.writerow([
                            page_number,                     # A_ç•ªå·ï¼ˆåŒä¸€ãƒšãƒ¼ã‚¸ã¯åŒä¸€ç•ªå·ï¼‰
                            target_title,                    # B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                            target_url,                      # C_URL
                            source_title,                    # D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                            source_url,                      # E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL
                            anchor_text                      # F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
                        ])
                        written_rows += 1
                        
                    except Exception as e:
                        print(f"ãƒªãƒ³ã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
                
                print(f"è¢«ãƒªãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿å®Œäº†: {written_rows}è¡Œ")
                
                # 4. å­¤ç«‹ãƒšãƒ¼ã‚¸ï¼ˆè¢«ãƒªãƒ³ã‚¯ãŒ0ã®ï¼‰ã®å‡ºåŠ›
                isolated_rows = 0
                for url, info in self.pages.items():
                    try:
                        if info.get('inbound_links', 0) == 0:
                            page_number = page_number_map.get(url, 0)
                            if page_number == 0:
                                continue
                                
                            title = str(info.get('title', url))[:200]
                            
                            writer.writerow([
                                page_number,        # A_ç•ªå·
                                title,              # B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                                url,               # C_URL
                                '',                # D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                                '',                # E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL
                                ''                 # F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
                            ])
                            isolated_rows += 1
                            
                    except Exception as e:
                        print(f"å­¤ç«‹ãƒšãƒ¼ã‚¸å‡¦ç†ã‚¨ãƒ©ãƒ¼: {url} - {e}")
                        continue
                
                print(f"å­¤ç«‹ãƒšãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿å®Œäº†: {isolated_rows}è¡Œ")
                
            print(f"CSVä¿å­˜å®Œäº†: ç·è¡Œæ•° {written_rows + isolated_rows + 1}è¡Œï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼å«ã‚€ï¼‰")
            return True
            
        except Exception as e:
            print(f"CSVä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return False

    def run(self):
        self.root.mainloop()

def main():
    app = LinkAnalyzerApp()
    app.run()

if __name__ == "__main__":
    main()

import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import re
import csv
import io
from datetime import datetime
import pandas as pd

# --- Analyzer Class (æ”¹å–„ç‰ˆ) ---
class ArigatayaAnalyzer:
    def __init__(self, base_url):
        self.base_url = base_url
        self.pages = {}
        self.links = []
        self.detailed_links = []
        self.domain = urlparse(base_url).netloc.replace('www.', '')

    def normalize_url(self, url):
        """URLã‚’æ­£è¦åŒ–ã™ã‚‹ã€‚ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã‚’é™¤å»ã—ã€æœ«å°¾ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’çµ±ä¸€ã™ã‚‹ã€‚"""
        try:
            parsed = urlparse(url)
            # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã‚’é™¤å»
            path = parsed.path.split('?')[0].split('#')[0]
            # www. ã‚’é™¤å»ã—ã€æœ«å°¾ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤
            netloc = parsed.netloc.replace('www.', '')
            path = path.rstrip('/')
            
            # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã®å ´åˆã¯ç©ºãƒ‘ã‚¹ã«ã™ã‚‹
            if not path:
                return f"https://{netloc}"
            
            return f"https://{netloc}{path}"
        except Exception:
            return url

    def is_internal(self, url):
        """URLãŒå†…éƒ¨ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹ã€‚"""
        return urlparse(url).netloc.replace('www.', '') == self.domain

    def extract_from_sitemap(self, url, visited_sitemaps):
        """ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’å†å¸°çš„ã«æ¢ç´¢ã—ã¦URLã‚’æŠ½å‡ºã™ã‚‹ã€‚"""
        if url in visited_sitemaps:
            return set()
        visited_sitemaps.add(url)
        
        urls = set()
        try:
            res = requests.get(url, timeout=10)
            res.raise_for_status()
            # XMLã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹
            soup = BeautifulSoup(res.content, 'xml')
            
            # sitemapindexã®å ´åˆã€å†å¸°çš„ã«æ¢ç´¢
            if soup.find('sitemapindex'):
                for loc in soup.find_all('loc'):
                    urls.update(self.extract_from_sitemap(loc.text.strip(), visited_sitemaps))
            # é€šå¸¸ã®sitemapã®å ´åˆã€URLã‚’æŠ½å‡º
            else:
                for loc in soup.find_all('loc'):
                    urls.add(self.normalize_url(loc.text.strip()))
        except Exception as e:
            print(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
        return urls

    def generate_seed_urls(self):
        """åˆ†æã®èµ·ç‚¹ã¨ãªã‚‹URLãƒªã‚¹ãƒˆã‚’ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‹ã‚‰ç”Ÿæˆã™ã‚‹ã€‚"""
        sitemap_root = urljoin(self.base_url, '/sitemap.xml')
        sitemap_urls = self.extract_from_sitemap(sitemap_root, set())
        print(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‹ã‚‰ {len(sitemap_urls)} å€‹ã®URLã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
        # åŸºæœ¬URLã‚‚è¿½åŠ ã—ã¦ãŠã
        seed_urls = set([self.normalize_url(self.base_url)]) | sitemap_urls
        return list(seed_urls)

    def is_content(self, url):
        """URLãŒã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒšãƒ¼ã‚¸ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹ï¼ˆæ”¹å–„ç‰ˆï¼‰ã€‚"""
        try:
            path = urlparse(url).path.lower()

            # é™¤å¤–ã™ã¹ãæ˜ç¢ºãªãƒ‘ã‚¿ãƒ¼ãƒ³
            exclude_patterns = [
                r'\.(jpg|jpeg|png|gif|webp|svg|ico|pdf|zip)$', # ç”»åƒã‚„ãƒ•ã‚¡ã‚¤ãƒ«
                r'/wp-admin', r'/wp-json', r'/wp-includes', r'/wp-content/plugins', # WordPressã‚·ã‚¹ãƒ†ãƒ ç³»
                r'/feed', r'/comments/feed', # ãƒ•ã‚£ãƒ¼ãƒ‰
                r'/trackback',
                r'sitemap\.xml', # ã‚µã‚¤ãƒˆãƒãƒƒãƒ—è‡ªä½“
                r'\/go\/', # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
                r'\/g\/', # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
                r'\/tag\/', # ã‚¿ã‚°ãƒšãƒ¼ã‚¸
                r'\/author\/', # è‘—è€…ãƒšãƒ¼ã‚¸
                r'\/privacy', # ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒãƒªã‚·ãƒ¼
            ]
            for pattern in exclude_patterns:
                if re.search(pattern, path):
                    # print(f"[é™¤å¤–] ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´: {url}")
                    return False

            # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆ/page/æ•°å­—ï¼‰ã¯ã€ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸ä»¥å¤–ã§ã¯é™¤å¤–ã™ã‚‹å‚¾å‘ãŒå¼·ã„ãŒã€ä»Šå›ã¯ä¸€æ—¦å«ã‚ã‚‹
            # if re.search(r'/page/\d+', path) and not path.startswith('/category/'):
            #     return False

            # ãƒ‘ã‚¹ãŒã€Œ/ã€ã§çµ‚ã‚ã‚Šã€ã‹ã¤ãƒ‘ã‚¹éšå±¤ãŒæ·±ã™ããªã„ã‚‚ã®ã‚’è¨˜äº‹ã¨ã¿ãªã™ï¼ˆä¾‹: /g-card/ï¼‰
            # arigataya.co.jp ã¯è¨˜äº‹URLãŒ /slug/ ã®å½¢å¼ãªã®ã§ã€ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã§çµ‚ã‚ã‚‹å˜ä¸€éšå±¤ã‚’å„ªå…ˆ
            # æ­£è¦åŒ–ã§æœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã¯é™¤å»æ¸ˆã¿ãªã®ã§ã€ãƒ‘ã‚¹ã®æ§‹é€ ã§åˆ¤æ–­
            clean_path = path.strip('/')
            if '/' not in clean_path and clean_path: # /slug ã®ã‚ˆã†ãªå½¢å¼
                 return True
            
            # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸
            if path == '/' or not path:
                return True
            
            # ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸
            if path.startswith('/category/'):
                return True

            # ä¸Šè¨˜ã®æ¡ä»¶ã«åˆè‡´ã—ãªã„ã‚‚ã®ã¯ä¸€æ—¦é™¤å¤–
            # print(f"[é™¤å¤–] å¯¾è±¡å¤–ã®æ§‹é€ : {url}")
            return False
        except Exception:
            return False

    def is_noindex_page(self, soup):
        """ãƒšãƒ¼ã‚¸ãŒnoindexã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹ã€‚"""
        meta_robots = soup.find('meta', attrs={'name': 'robots'})
        if meta_robots and 'noindex' in meta_robots.get('content', '').lower():
            return True
        return False

    def extract_links(self, soup, base_url):
        """ãƒšãƒ¼ã‚¸å†…ã‹ã‚‰å†…éƒ¨ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã™ã‚‹ï¼ˆonclickå¯¾å¿œï¼‰ã€‚"""
        links = []
        content_area = soup.select_one('.post_content, .entry-content, main, article')
        if not content_area:
            content_area = soup # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å…¨ä½“ã‚’å¯¾è±¡

        # é€šå¸¸ã®aã‚¿ã‚°
        for a in content_area.find_all('a', href=True):
            href = a.get('href')
            if href and not href.startswith(('#', 'javascript:', 'mailto:')):
                full_url = urljoin(base_url, href)
                if self.is_internal(full_url):
                    links.append({
                        'url': self.normalize_url(full_url),
                        'anchor_text': a.get_text(strip=True)[:100]
                    })
        
        # onclickå±æ€§
        for tag in content_area.find_all(onclick=True):
            onclick_val = tag.get('onclick')
            match = re.search(r"location\.href='([^']+)'", onclick_val)
            if match:
                href = match.group(1)
                full_url = urljoin(base_url, href)
                if self.is_internal(full_url):
                    links.append({
                        'url': self.normalize_url(full_url),
                        'anchor_text': tag.get_text(strip=True)[:100]
                    })
        return links

    def analyze(self, progress_callback):
        """ã‚µã‚¤ãƒˆåˆ†æã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã€‚"""
        self.pages, self.links, self.detailed_links = {}, [], []
        
        progress_callback("ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‹ã‚‰URLã‚’åé›†ä¸­...", 0.0)
        to_visit = self.generate_seed_urls()
        
        visited = set()
        processed_links = set() # (source, target) ã®ã‚¿ãƒ—ãƒ«ã‚’ä¿å­˜
        
        session = requests.Session()
        session.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'})

        max_pages = 500 # ã‚¯ãƒ­ãƒ¼ãƒ«ä¸Šé™
        queue = list(dict.fromkeys(to_visit)) # é‡è¤‡é™¤å»ã—ã¤ã¤é †åºç¶­æŒ
        
        while queue and len(self.pages) < max_pages:
            url = queue.pop(0)
            
            if url in visited:
                continue
            
            # ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹äº‹å‰ã«ãƒã‚§ãƒƒã‚¯
            if not self.is_content(url):
                visited.add(url)
                continue

            try:
                progress = len(self.pages) / max_pages
                progress_callback(f"åˆ†æä¸­ ({len(self.pages)}/{max_pages}): {url}", progress)
                
                response = session.get(url, timeout=10)
                visited.add(url) # è¨ªå•æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯
                if response.status_code != 200:
                    continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                if self.is_noindex_page(soup):
                    print(f"[ã‚¹ã‚­ãƒƒãƒ—] NOINDEXãƒšãƒ¼ã‚¸: {url}")
                    continue

                title = soup.title.string.strip() if soup.title and soup.title.string else url
                title = re.sub(r'\s*[|\-]\s*.*(arigataya|ã‚ã‚ŠãŒãŸã‚„).*$', '', title, flags=re.IGNORECASE).strip()
                
                # ã“ã®æ™‚ç‚¹ã§ãƒšãƒ¼ã‚¸ã‚’ç™»éŒ²
                if url not in self.pages:
                    self.pages[url] = {'title': title, 'outbound_links': [], 'inbound_links': 0}

                # ãƒšãƒ¼ã‚¸å†…ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
                extracted_links = self.extract_links(soup, url)
                for link_data in extracted_links:
                    target_url = link_data['url']
                    
                    # ãƒªãƒ³ã‚¯ãŒã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ã‹ãƒã‚§ãƒƒã‚¯
                    if self.is_content(target_url):
                        # ç™ºãƒªãƒ³ã‚¯ã¨ã—ã¦è¨˜éŒ²
                        link_key = (url, target_url)
                        if link_key not in processed_links:
                            self.pages[url]['outbound_links'].append(target_url)
                            self.detailed_links.append({
                                'source_url': url, 'source_title': title,
                                'target_url': target_url, 'anchor_text': link_data['anchor_text']
                            })
                            processed_links.add(link_key)
                        
                        # æœªè¨ªå•ãªã‚‰ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
                        if target_url not in visited and target_url not in queue:
                            queue.append(target_url)

                time.sleep(0.1)

            except requests.exceptions.RequestException as e:
                print(f"é€šä¿¡ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
                continue
            except Exception as e:
                print(f"å‡¦ç†ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
                continue

        progress_callback("è¢«ãƒªãƒ³ã‚¯æ•°ã‚’è¨ˆç®—ä¸­...", 0.95)
        # è¢«ãƒªãƒ³ã‚¯æ•°ã‚’å†è¨ˆç®—
        for page_url in self.pages:
            self.pages[page_url]['inbound_links'] = 0
        for link in self.detailed_links:
            target = link['target_url']
            if target in self.pages:
                self.pages[target]['inbound_links'] += 1
        
        progress_callback("åˆ†æå®Œäº†ï¼", 1.0)
        return self.pages, self.detailed_links

# --- Streamlit App (UIéƒ¨åˆ†ã¯å¤‰æ›´ãªã—) ---
def create_detailed_csv(pages, detailed_links):
    output = io.StringIO()
    writer = csv.writer(output)
    
    writer.writerow([
        'A_ç•ªå·', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL',
        'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL', 'F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'
    ])
    
    # è¢«ãƒªãƒ³ã‚¯æ•°ã§ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒšãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
    unique_pages = sorted(
        [{'url': u, 'title': i.get('title', u), 'inbound_links': i.get('inbound_links', 0)} for u, i in pages.items()],
        key=lambda x: x['inbound_links'],
        reverse=True
    )
    
    # URLã¨ãƒšãƒ¼ã‚¸ç•ªå·ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
    page_number_map = {page['url']: i for i, page in enumerate(unique_pages, 1)}

    # è¢«ãƒªãƒ³ã‚¯ã‚’æŒã¤ãƒšãƒ¼ã‚¸ã®æƒ…å ±ã‚’æ›¸ãå‡ºã—
    for link in detailed_links:
        target_url = link.get('target_url')
        if not target_url or target_url not in page_number_map:
            continue
        
        page_number = page_number_map[target_url]
        target_title = pages.get(target_url, {}).get('title', target_url)
        
        writer.writerow([
            page_number, target_title, target_url,
            link.get('source_title', ''), link.get('source_url', ''), link.get('anchor_text', '')
        ])

    # å­¤ç«‹ãƒšãƒ¼ã‚¸ï¼ˆè¢«ãƒªãƒ³ã‚¯0ï¼‰ã®æƒ…å ±ã‚’æ›¸ãå‡ºã—
    for url, info in pages.items():
        if info.get('inbound_links', 0) == 0:
            page_number = page_number_map.get(url)
            if page_number:
                writer.writerow([page_number, info.get('title', url), url, '', '', ''])
                
    return output.getvalue().encode('utf-8-sig')

def main():
    st.set_page_config(
        page_title="arigatayaå°‚ç”¨å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æ",
        page_icon="ğŸ”—",
        layout="wide"
    )
    
    st.title("ğŸ”— arigatayaå°‚ç”¨å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æãƒ„ãƒ¼ãƒ«")
    st.markdown("**onclickå¯¾å¿œãƒ»å…¨è‡ªå‹•ç‰ˆ (Streamlit) - æ”¹å–„ç‰ˆ**")

    if 'analysis_done' not in st.session_state:
        st.session_state.analysis_done = False
        st.session_state.results = None

    if not st.session_state.analysis_done:
        start_button = st.button("ğŸš€ åˆ†æé–‹å§‹", type="primary", use_container_width=True)
        
        if start_button and 'analysis_started' not in st.session_state:
            st.session_state.analysis_started = True

        if 'analysis_started' in st.session_state and st.session_state.analysis_started:
            status_placeholder = st.empty()
            progress_bar = st.progress(0)
            
            def progress_callback(message, progress):
                status_placeholder.info(message)
                progress_bar.progress(progress)

            try:
                analyzer = ArigatayaAnalyzer("https://arigataya.co.jp")
                pages, detailed_links = analyzer.analyze(progress_callback)
                
                st.session_state.results = {
                    "pages": pages,
                    "detailed_links": detailed_links
                }
                st.session_state.analysis_done = True
                del st.session_state.analysis_started # å®Œäº†ã—ãŸã‚‰å‰Šé™¤
                st.rerun()

            except Exception as e:
                st.error(f"åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                del st.session_state.analysis_started

    if st.session_state.analysis_done and st.session_state.results:
        results = st.session_state.results
        pages = results["pages"]
        detailed_links = results["detailed_links"]
        
        st.success("âœ… åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")

        total_pages = len(pages)
        total_links_count = len(detailed_links)
        isolated_pages = sum(1 for p in pages.values() if p.get('inbound_links', 0) == 0)
        popular_pages = sum(1 for p in pages.values() if p.get('inbound_links', 0) >= 5)

        col1, col2, col3, col4 = st.columns(4)
        col1.metric("ç·ãƒšãƒ¼ã‚¸æ•°", total_pages)
        col2.metric("ç·ãƒªãƒ³ã‚¯æ•°", total_links_count)
        col3.metric("å­¤ç«‹ãƒšãƒ¼ã‚¸", isolated_pages)
        col4.metric("äººæ°—ãƒšãƒ¼ã‚¸", popular_pages)

        csv_data = create_detailed_csv(pages, detailed_links)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"arigataya_{timestamp}.csv"
        
        st.download_button(
            label="ğŸ“¥ è©³ç´°CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv_data,
            file_name=filename,
            mime="text/csv",
            type="primary",
            use_container_width=True
        )

        st.subheader("ğŸ“Š åˆ†æçµæœä¸€è¦§")
        results_data = []
        for url, info in pages.items():
            results_data.append({
                'ã‚¿ã‚¤ãƒˆãƒ«': info.get('title', url),
                'URL': url,
                'è¢«ãƒªãƒ³ã‚¯æ•°': info.get('inbound_links', 0),
                'ç™ºãƒªãƒ³ã‚¯æ•°': len(info.get('outbound_links', []))
            })
        
        df = pd.DataFrame(results_data)
        df_sorted = df.sort_values('è¢«ãƒªãƒ³ã‚¯æ•°', ascending=False).reset_index(drop=True)
        
        st.dataframe(df_sorted, use_container_width=True)

        if st.button("ğŸ”„ å†åˆ†æã™ã‚‹"):
            st.session_state.analysis_done = False
            st.session_state.results = None
            if 'analysis_started' in st.session_state:
                del st.session_state.analysis_started
            st.rerun()

if __name__ == "__main__":
    main()

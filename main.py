import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import time
import re
import csv
import io
from datetime import datetime
import pandas as pd

# Answerç¾é‡‘åŒ–åˆ†æã‚¯ãƒ©ã‚¹
class AnswerAnalyzer:
    def __init__(self):
        self.pages = {}
        self.links = []
        self.detailed_links = []

    def normalize_url(self, url):
        if not url: return ""
        try:
            parsed = urlparse(url)
            return f"https://{parsed.netloc.replace('www.', '')}{parsed.path.rstrip('/')}"
        except:
            return url

    def is_article_page(self, url):
        path = urlparse(self.normalize_url(url)).path.lower()
        exclude_words = ['/site/', '/blog/', '/page/', '/feed', '/wp-', '.']
        if any(word in path for word in exclude_words):
            return False
        
        if path.startswith('/') and len(path) > 1:
            clean_path = path[1:].rstrip('/')
            if clean_path and '/' not in clean_path:
                return True
        return False

    def is_crawlable(self, url):
        path = urlparse(self.normalize_url(url)).path.lower()
        exclude_words = ['/site/', '/wp-admin', '/feed', '.jpg', '.png', '.css', '.js']
        if any(word in path for word in exclude_words):
            return False
        return (path.startswith('/blog') or self.is_article_page(url))

    def extract_links(self, soup, current_url):
        links = []
        for a in soup.find_all('a', href=True):
            href = a.get('href', '').strip()
            if href and not href.startswith('#'):
                absolute = urljoin(current_url, href)
                if 'answer-genkinka.jp' in absolute and self.is_crawlable(absolute):
                    links.append(self.normalize_url(absolute))
        return list(set(links))

    def extract_content_links(self, soup, current_url):
        content = soup.select_one('.entry-content, .post-content, main, article')
        if not content:
            return []
        
        links = []
        for a in content.find_all('a', href=True):
            href = a.get('href', '').strip()
            text = a.get_text(strip=True) or ''
            if href and not href.startswith('#') and text:
                absolute = urljoin(current_url, href)
                if 'answer-genkinka.jp' in absolute and self.is_article_page(absolute):
                    links.append({'url': self.normalize_url(absolute), 'anchor_text': text[:100]})
        return links

    def analyze(self, start_url, progress_callback=None):
        self.pages, self.links, self.detailed_links = {}, [], []
        visited, to_visit = set(), [self.normalize_url(start_url)]
        to_visit.append('https://answer-genkinka.jp/blog/page/2/')

        session = requests.Session()
        session.headers.update({'User-Agent': 'Mozilla/5.0'})
        
        max_pages = 50 # åé›†ãƒšãƒ¼ã‚¸æ•°ã®ä¸Šé™ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰

        # ãƒ•ã‚§ãƒ¼ã‚º1: ãƒšãƒ¼ã‚¸åé›†
        while to_visit and len(self.pages) < max_pages:
            url = to_visit.pop(0)
            if url in visited: continue
            
            try:
                if progress_callback:
                    progress_callback(f"åé›†ä¸­ ({len(self.pages)}/{max_pages}): {url}", len(self.pages) / max_pages)
                    
                response = session.get(url, timeout=10)
                visited.add(url)
                if response.status_code != 200: continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                robots = soup.find('meta', attrs={'name': 'robots'})
                if robots and 'noindex' in robots.get('content', '').lower():
                    continue
                
                if '/blog' in url:
                    page_links = self.extract_links(soup, url)
                    new_links = [l for l in page_links if l not in visited and l not in to_visit]
                    to_visit.extend(new_links)
                    continue
                
                if self.is_article_page(url):
                    title = soup.find('h1')
                    title = title.get_text(strip=True) if title else url
                    title = re.sub(r'\s*[|\-]\s*.*(answer-genkinka|ã‚¢ãƒ³ã‚µãƒ¼).*$', '', title, flags=re.IGNORECASE)
                    
                    self.pages[url] = {'title': title, 'outbound_links': []}
                    page_links = self.extract_links(soup, url)
                    new_links = [l for l in page_links if l not in visited and l not in to_visit]
                    to_visit.extend(new_links)
                
                time.sleep(0.1)
                
            except Exception:
                continue

        # ãƒ•ã‚§ãƒ¼ã‚º2: ãƒªãƒ³ã‚¯é–¢ä¿‚æ§‹ç¯‰
        if progress_callback:
            progress_callback("ãƒªãƒ³ã‚¯é–¢ä¿‚æ§‹ç¯‰ä¸­...", 0.9)
            
        processed = set()
        for i, url in enumerate(list(self.pages.keys())):
            try:
                response = session.get(url, timeout=10)
                if response.status_code != 200: continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                content_links = self.extract_content_links(soup, url)
                
                for link_data in content_links:
                    target = link_data['url']
                    if target in self.pages and target != url:
                        link_key = (url, target)
                        if link_key not in processed:
                            processed.add(link_key)
                            self.links.append((url, target))
                            self.pages[url]['outbound_links'].append(target)
                            self.detailed_links.append({
                                'source_url': url, 'source_title': self.pages[url]['title'],
                                'target_url': target, 'anchor_text': link_data['anchor_text']
                            })
            except Exception:
                continue

        # è¢«ãƒªãƒ³ã‚¯æ•°è¨ˆç®—
        for url in self.pages:
            self.pages[url]['inbound_links'] = sum(1 for s, t in self.links if t == url)

        return self.pages, self.links, self.detailed_links

### â–¼â–¼â–¼ å¤‰æ›´ç‚¹1: ArigatayaAnalyzerã‚¯ãƒ©ã‚¹ã®è¿½åŠ  â–¼â–¼â–¼
class ArigatayaAnalyzer:
    def __init__(self):
        self.pages = {}
        self.links = []
        self.detailed_links = []
        self.base_url = None
        self.domain = None

    def normalize_url(self, url):
        try:
            parsed = urlparse(url)
            path = parsed.path.split('?')[0].split('#')[0]
            netloc = parsed.netloc.replace('www.', '')
            path = path.rstrip('/')
            if not path:
                return f"https://{netloc}"
            return f"https://{netloc}{path}"
        except Exception:
            return url

    def is_internal(self, url):
        return urlparse(url).netloc.replace('www.', '') == self.domain

    def extract_from_sitemap(self, url, visited_sitemaps):
        if url in visited_sitemaps:
            return set()
        visited_sitemaps.add(url)
        urls = set()
        try:
            res = requests.get(url, timeout=10)
            res.raise_for_status()
            soup = BeautifulSoup(res.content, 'xml')
            if soup.find('sitemapindex'):
                for loc in soup.find_all('loc'):
                    urls.update(self.extract_from_sitemap(loc.text.strip(), visited_sitemaps))
            else:
                for loc in soup.find_all('loc'):
                    urls.add(self.normalize_url(loc.text.strip()))
        except Exception as e:
            print(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
        return urls

    def generate_seed_urls(self):
        sitemap_root = urljoin(self.base_url, '/sitemap.xml')
        sitemap_urls = self.extract_from_sitemap(sitemap_root, set())
        print(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‹ã‚‰ {len(sitemap_urls)} å€‹ã®URLã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
        seed_urls = set([self.normalize_url(self.base_url)]) | sitemap_urls
        return list(seed_urls)

    def is_content(self, url):
        try:
            path = urlparse(url).path.lower()
            exclude_patterns = [
                r'\.(jpg|jpeg|png|gif|webp|svg|ico|pdf|zip)$', r'/wp-admin', r'/wp-json', r'/wp-includes',
                r'/wp-content/plugins', r'/feed', r'/comments/feed', r'/trackback', r'sitemap\.xml',
                r'\/go\/', r'\/g\/', r'\/tag\/', r'\/author\/', r'\/privacy',
            ]
            if any(re.search(pattern, path) for pattern in exclude_patterns):
                return False
            clean_path = path.strip('/')
            if ('/' not in clean_path and clean_path) or path == '/' or not path or path.startswith('/category/'):
                return True
            return False
        except Exception:
            return False

    def is_noindex_page(self, soup):
        meta_robots = soup.find('meta', attrs={'name': 'robots'})
        if meta_robots and 'noindex' in meta_robots.get('content', '').lower():
            return True
        return False

    def extract_links(self, soup, base_url):
        links = []
        content_area = soup.select_one('.post_content, .entry-content, main, article') or soup
        for a in content_area.find_all('a', href=True):
            href = a.get('href')
            if href and not href.startswith(('#', 'javascript:', 'mailto:')):
                full_url = urljoin(base_url, href)
                if self.is_internal(full_url):
                    links.append({'url': self.normalize_url(full_url), 'anchor_text': a.get_text(strip=True)[:100]})
        for tag in content_area.find_all(onclick=True):
            onclick_val = tag.get('onclick')
            match = re.search(r"location\.href='([^']+)'", onclick_val)
            if match:
                href = match.group(1)
                full_url = urljoin(base_url, href)
                if self.is_internal(full_url):
                    links.append({'url': self.normalize_url(full_url), 'anchor_text': tag.get_text(strip=True)[:100]})
        return links

    def analyze(self, start_url, progress_callback=None):
        self.base_url = start_url
        self.domain = urlparse(start_url).netloc.replace('www.', '')
        self.pages, self.links, self.detailed_links = {}, [], []
        
        if progress_callback:
            progress_callback("ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‹ã‚‰URLã‚’åé›†ä¸­...", 0.0)
        
        to_visit = self.generate_seed_urls()
        visited, processed_links = set(), set()
        
        session = requests.Session()
        session.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'})
        
        max_pages = 500
        queue = list(dict.fromkeys(to_visit))
        
        while queue and len(self.pages) < max_pages:
            url = queue.pop(0)
            if url in visited or not self.is_content(url):
                visited.add(url)
                continue

            try:
                if progress_callback:
                    progress = len(self.pages) / max_pages
                    progress_callback(f"åˆ†æä¸­ ({len(self.pages)}/{max_pages}): {url}", progress)
                
                response = session.get(url, timeout=10)
                visited.add(url)
                if response.status_code != 200: continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                if self.is_noindex_page(soup): continue

                title = (soup.title.string.strip() if soup.title and soup.title.string else url)
                title = re.sub(r'\s*[|\-]\s*.*(arigataya|ã‚ã‚ŠãŒãŸã‚„).*$', '', title, flags=re.IGNORECASE).strip()
                
                if url not in self.pages:
                    self.pages[url] = {'title': title, 'outbound_links': [], 'inbound_links': 0}

                extracted_links = self.extract_links(soup, url)
                for link_data in extracted_links:
                    target_url = link_data['url']
                    if self.is_content(target_url):
                        link_key = (url, target_url)
                        if link_key not in processed_links:
                            self.pages[url]['outbound_links'].append(target_url)
                            self.detailed_links.append({'source_url': url, 'source_title': title, 'target_url': target_url, 'anchor_text': link_data['anchor_text']})
                            self.links.append(link_key)
                            processed_links.add(link_key)
                        if target_url not in visited and target_url not in queue:
                            queue.append(target_url)
                time.sleep(0.1)

            except Exception as e:
                print(f"ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
                continue

        if progress_callback: progress_callback("è¢«ãƒªãƒ³ã‚¯æ•°ã‚’è¨ˆç®—ä¸­...", 0.95)
        for page_url in self.pages: self.pages[page_url]['inbound_links'] = 0
        for _, target in self.links:
            if target in self.pages: self.pages[target]['inbound_links'] += 1
        
        return self.pages, self.links, self.detailed_links

# ãã®ä»–ã‚µã‚¤ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼åˆ†æã‚¯ãƒ©ã‚¹
class GenericAnalyzer:
    def __init__(self):
        pass

    def analyze(self, start_url, progress_callback=None):
        if progress_callback:
            progress_callback(f"ã“ã®ã‚µã‚¤ãƒˆã®åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã¯æº–å‚™ä¸­ã§ã™...", 0)
        return {}, [], []

# ã‚µã‚¤ãƒˆè¨­å®š
ANALYZER_CONFIGS = {
    "answer-genkinka.jp": {
        "name": "Answerç¾é‡‘åŒ–",
        "url": "https://answer-genkinka.jp/blog/",
        "status": "active",
        "description": "ç¾é‡‘åŒ–ã‚µãƒ¼ãƒ“ã‚¹å°‚é–€ã‚µã‚¤ãƒˆ",
        "features": ["ãƒ–ãƒ­ã‚°è¨˜äº‹åˆ†æ", "å…¨è‡ªå‹•CSVå‡ºåŠ›"],
        "color": "#FF6B6B",
        "analyzer_class": AnswerAnalyzer
    },
    ### â–¼â–¼â–¼ å¤‰æ›´ç‚¹2: ANALYZER_CONFIGSã®æ›´æ–° â–¼â–¼â–¼
    "arigataya.co.jp": {
        "name": "ã‚ã‚ŠãŒãŸã‚„", 
        "url": "https://arigataya.co.jp",
        "status": "active", # "planned" ã‹ã‚‰ "active" ã«å¤‰æ›´
        "description": "onclickå¯¾å¿œãƒ»å…¨è‡ªå‹•ç‰ˆ",
        "features": ["onclickå¯¾å¿œ", "è‡ªå‹•ãƒªãƒ³ã‚¯æ¤œå‡º"],
        "color": "#4ECDC4",
        "analyzer_class": ArigatayaAnalyzer # GenericAnalyzer ã‹ã‚‰ ArigatayaAnalyzer ã«å¤‰æ›´
    },
    "kau-ru.co.jp": {
        "name": "ã‚«ã‚¦ãƒ¼ãƒ«",
        "url": "https://kau-ru.co.jp", 
        "status": "planned",
        "description": "è¤‡æ•°ã‚µã‚¤ãƒˆå¯¾å¿œç‰ˆ",
        "features": ["WordPress API", "æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«é™¤å¤–"],
        "color": "#45B7D1",
        "analyzer_class": GenericAnalyzer
    },
    "crecaeru.co.jp": {
        "name": "ã‚¯ãƒ¬ã‹ãˆã‚‹",
        "url": "https://crecaeru.co.jp",
        "status": "planned",
        "description": "gnavé™¤å¤–å¯¾å¿œãƒ»onclickå¯¾å¿œç‰ˆ",
        "features": ["gnavé™¤å¤–", "onclickå¯¾å¿œ"],
        "color": "#96CEB4",
        "analyzer_class": GenericAnalyzer
    },
    "friendpay.jp": {
        "name": "ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒšã‚¤",
        "url": "https://friendpay.jp",
        "status": "planned",
        "description": "ã‚µã‚¤ãƒˆåˆ¥é™¤å¤–ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼å¯¾å¿œ",
        "features": ["ã‚µã‚¤ãƒˆåˆ¥é™¤å¤–", "æœ€é©åŒ–åˆ†æ"],
        "color": "#FFEAA7",
        "analyzer_class": GenericAnalyzer
    },
    "kaitori-life.co.jp": {
        "name": "è²·å–LIFE",
        "url": "https://kaitori-life.co.jp",
        "status": "planned",
        "description": "JINãƒ†ãƒ¼ãƒå°‚ç”¨æœ€é©åŒ–",
        "features": ["JINãƒ†ãƒ¼ãƒå¯¾å¿œ", "å°‚ç”¨ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼"],
        "color": "#FD79A8",
        "analyzer_class": GenericAnalyzer
    },
    "wallet-sos.jp": {
        "name": "ã‚¦ã‚©ãƒ¬ãƒƒãƒˆSOS",
        "url": "https://wallet-sos.jp",
        "status": "planned",
        "description": "Seleniumç‰ˆï¼ˆCloudflareå¯¾ç­–ï¼‰",
        "features": ["Seleniumå¯¾å¿œ", "Cloudflareå¯¾ç­–"],
        "color": "#A29BFE",
        "analyzer_class": GenericAnalyzer
    },
    "wonderwall-invest.co.jp": {
        "name": "ãƒ¯ãƒ³ãƒ€ãƒ¼ã‚¦ã‚©ãƒ¼ãƒ«",
        "url": "https://wonderwall-invest.co.jp",
        "status": "planned",
        "description": "secure-technologyå°‚ç”¨",
        "features": ["å°‚ç”¨æœ€é©åŒ–", "é«˜ç²¾åº¦åˆ†æ"],
        "color": "#6C5CE7",
        "analyzer_class": GenericAnalyzer
    },
    "fuyohin-kaishu.co.jp": {
        "name": "ä¸ç”¨å“å›å",
        "url": "https://fuyohin-kaishu.co.jp",
        "status": "planned",
        "description": "ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æãƒ»ä¿®æ­£ç‰ˆ",
        "features": ["ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ", "åŒ…æ‹¬çš„åé›†"],
        "color": "#00B894",
        "analyzer_class": GenericAnalyzer
    },
    "bic-gift.co.jp": {
        "name": "ãƒ“ãƒƒã‚¯ã‚®ãƒ•ãƒˆ",
        "url": "https://bic-gift.co.jp",
        "status": "planned",
        "description": "SANGOãƒ†ãƒ¼ãƒå°‚ç”¨ãƒ»å…¨è‡ªå‹•ç‰ˆ",
        "features": ["SANGOãƒ†ãƒ¼ãƒå¯¾å¿œ", "å°‚ç”¨æŠ½å‡º"],
        "color": "#E17055",
        "analyzer_class": GenericAnalyzer
    },
    "flashpay.jp/famipay": {
        "name": "ãƒ•ã‚¡ãƒŸãƒšã‚¤",
        "url": "https://flashpay.jp/famipay/",
        "status": "planned",
        "description": "/famipay/é…ä¸‹å°‚ç”¨",
        "features": ["é…ä¸‹é™å®šåˆ†æ", "é«˜ç²¾åº¦æŠ½å‡º"],
        "color": "#00CEC9",
        "analyzer_class": GenericAnalyzer
    },
    "flashpay.jp/media": {
        "name": "ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ãƒšã‚¤",
        "url": "https://flashpay.jp/media/",
        "status": "planned",
        "description": "/media/é…ä¸‹å°‚ç”¨",
        "features": ["ãƒ¡ãƒ‡ã‚£ã‚¢ç‰¹åŒ–", "åŠ¹ç‡åˆ†æ"],
        "color": "#74B9FF",
        "analyzer_class": GenericAnalyzer
    },
    "more-pay.jp": {
        "name": "ãƒ¢ã‚¢ãƒšã‚¤",
        "url": "https://more-pay.jp",
        "status": "planned",
        "description": "æ”¹å–„ç‰ˆãƒ»åŒ…æ‹¬çš„åˆ†æ",
        "features": ["åŒ…æ‹¬åˆ†æ", "æ”¹å–„ç‰ˆã‚¨ãƒ³ã‚¸ãƒ³"],
        "color": "#FD79A8",
        "analyzer_class": GenericAnalyzer
    },
    "pay-ful.jp": {
        "name": "ãƒšã‚¤ãƒ•ãƒ«",
        "url": "https://pay-ful.jp/media/",
        "status": "planned",
        "description": "å€‹åˆ¥è¨˜äº‹ãƒšãƒ¼ã‚¸é‡è¦–",
        "features": ["è¨˜äº‹é‡è¦–", "ç²¾å¯†åˆ†æ"],
        "color": "#FDCB6E",
        "analyzer_class": GenericAnalyzer
    },
    "smart-pay.website": {
        "name": "ã‚¹ãƒãƒ¼ãƒˆãƒšã‚¤",
        "url": "https://smart-pay.website/media/",
        "status": "planned",
        "description": "å¤§è¦æ¨¡ã‚µã‚¤ãƒˆå¯¾å¿œ",
        "features": ["å¤§è¦æ¨¡å¯¾å¿œ", "åŠ¹ç‡åŒ–ã‚¨ãƒ³ã‚¸ãƒ³"],
        "color": "#E84393",
        "analyzer_class": GenericAnalyzer
    },
    "xgift.jp": {
        "name": "ã‚¨ãƒƒã‚¯ã‚¹ã‚®ãƒ•ãƒˆ",
        "url": "https://xgift.jp/blog/",
        "status": "planned",
        "description": "AFFINGERå¯¾å¿œ",
        "features": ["AFFINGERå¯¾å¿œ", "ãƒ†ãƒ¼ãƒæœ€é©åŒ–"],
        "color": "#00B894",
        "analyzer_class": GenericAnalyzer
    }
}

def create_detailed_csv(pages, detailed_links):
    """è©³ç´°CSVãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
    csv_data = []
    csv_data.append(['ç•ªå·', 'ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'URL', 'è¢«ãƒªãƒ³ã‚¯å…ƒã‚¿ã‚¤ãƒˆãƒ«', 'è¢«ãƒªãƒ³ã‚¯å…ƒURL', 'ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'])
    
    targets = {}
    for link in detailed_links:
        target = link['target_url']
        targets.setdefault(target, []).append(link)
    
    sorted_targets = sorted(targets.items(), key=lambda x: len(x[1]), reverse=True)
    
    row = 1
    for target, links_list in sorted_targets:
        title = pages.get(target, {}).get('title', target)
        for link in links_list:
            csv_data.append([row, title, target, link['source_title'], link['source_url'], link['anchor_text']])
            row += 1
    
    for url, info in pages.items():
        if info.get('inbound_links', 0) == 0:
            csv_data.append([row, info['title'], url, '', '', ''])
            row += 1
    
    output = io.StringIO()
    writer = csv.writer(output)
    writer.writerows(csv_data)
    return output.getvalue()

def run_analysis(site_key, config):
    """åˆ†æå®Ÿè¡Œ"""
    st.info(f"{config['name']} ã®åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    
    status_placeholder = st.empty()
    progress_bar = st.progress(0)
    
    def progress_callback(message, progress):
        status_placeholder.text(message)
        progress_bar.progress(progress)
    
    # åˆ†æå®Ÿè¡Œ
    analyzer_class = config.get("analyzer_class", GenericAnalyzer)
    analyzer = analyzer_class()
    
    pages, links, detailed_links = analyzer.analyze(config['url'], progress_callback)
    
    progress_bar.progress(1.0)
    status_placeholder.text("åˆ†æå®Œäº†!")
    
    if pages:
        # çµ±è¨ˆè¡¨ç¤º
        total = len(pages)
        isolated = sum(1 for p in pages.values() if p.get('inbound_links', 0) == 0)
        popular = sum(1 for p in pages.values() if p.get('inbound_links', 0) >= 5)
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ç·ãƒšãƒ¼ã‚¸æ•°", total)
        with col2:
            st.metric("ç·ãƒªãƒ³ã‚¯æ•°", len(links))
        with col3:
            st.metric("å­¤ç«‹ãƒšãƒ¼ã‚¸", isolated)
        with col4:
            st.metric("äººæ°—ãƒšãƒ¼ã‚¸", popular)
        
        # çµæœè¡¨ç¤º
        st.subheader(f"ğŸ“Š {config['name']} åˆ†æçµæœ")
        
        # DataFrameã«å¤‰æ›
        results_data = []
        for url, info in pages.items():
            results_data.append({
                'ã‚¿ã‚¤ãƒˆãƒ«': info['title'],
                'URL': url,
                'è¢«ãƒªãƒ³ã‚¯æ•°': info.get('inbound_links', 0),
                'ç™ºãƒªãƒ³ã‚¯æ•°': len(info.get('outbound_links', []))
            })
        
        df = pd.DataFrame(results_data)
        df_sorted = df.sort_values('è¢«ãƒªãƒ³ã‚¯æ•°', ascending=False)
        
        # ã‚°ãƒ©ãƒ•è¡¨ç¤º
        if len(df_sorted) > 0:
            st.subheader("è¢«ãƒªãƒ³ã‚¯æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆä¸Šä½10ä»¶ï¼‰")
            top_10 = df_sorted.head(10)
            if not top_10.empty:
                chart_data = top_10.set_index('ã‚¿ã‚¤ãƒˆãƒ«')['è¢«ãƒªãƒ³ã‚¯æ•°']
                st.bar_chart(chart_data)
        
        # è©³ç´°ãƒ†ãƒ¼ãƒ–ãƒ«
        st.subheader("è©³ç´°ãƒ‡ãƒ¼ã‚¿")
        st.dataframe(df_sorted, use_container_width=True)
        
        # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        if detailed_links:
            csv_content = create_detailed_csv(pages, detailed_links)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{config['name']}-{timestamp}.csv"
            
            st.download_button(
                "ğŸ“¥ è©³ç´°CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                csv_content,
                filename,
                "text/csv",
                help="è¢«ãƒªãƒ³ã‚¯æ•°é †ã§ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸè©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ"
            )
        
        st.success("âœ… åˆ†æå®Œäº†!")
    
    else:
        if config['status'] == 'planned':
            st.warning(f"{config['name']} ã®åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã¯æº–å‚™ä¸­ã§ã™")
        else:
            st.error("âŒ ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

def main():
    st.set_page_config(
        page_title="å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æ çµ±æ‹¬ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ",
        page_icon="ğŸ›ï¸",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("ğŸ›ï¸ å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æ çµ±æ‹¬ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("**16ã‚µã‚¤ãƒˆå¯¾å¿œ - ä¸€å…ƒç®¡ç†ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**")
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
    if 'active_analysis' not in st.session_state:
        st.session_state.active_analysis = None
    if 'analysis_results' not in st.session_state:
        st.session_state.analysis_results = {}
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.header("ğŸ“‹ ãƒ¡ãƒ‹ãƒ¥ãƒ¼")
        menu_options = ["ğŸ  ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰", "ğŸ”— å€‹åˆ¥åˆ†æ", "ğŸ“Š çµ±è¨ˆãƒ»æ¯”è¼ƒ", "âš™ï¸ è¨­å®šç®¡ç†"]
        menu = st.radio("æ©Ÿèƒ½ã‚’é¸æŠ", menu_options, index=0)
        
        st.divider()
        st.markdown("**ğŸ“ˆ ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³**")
        active_count = sum(1 for config in ANALYZER_CONFIGS.values() if config['status'] == 'active')
        st.metric("ç¨¼åƒä¸­", f"{active_count}/16ã‚µã‚¤ãƒˆ")
        
        st.divider()
        st.markdown("**ğŸ• æœ€çµ‚æ›´æ–°**")
        st.text(datetime.now().strftime("%Y-%m-%d %H:%M"))
    
    # ãƒ¡ã‚¤ãƒ³ç”»é¢
    if menu == "ğŸ  ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰":
        show_dashboard()
    elif menu == "ğŸ”— å€‹åˆ¥åˆ†æ":
        show_individual_analysis()
    elif menu == "ğŸ“Š çµ±è¨ˆãƒ»æ¯”è¼ƒ":
        show_statistics()
    elif menu == "âš™ï¸ è¨­å®šç®¡ç†":
        show_settings()

def show_dashboard():
    st.header("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“æ¦‚è¦")
    
    active_sites = [k for k, v in ANALYZER_CONFIGS.items() if v['status'] == 'active']
    planned_sites = [k for k, v in ANALYZER_CONFIGS.items() if v['status'] == 'planned']
    
    col1, col2, col3, col4 = st.columns(4)
    with col1: st.metric("ç·ã‚µã‚¤ãƒˆæ•°", len(ANALYZER_CONFIGS))
    with col2: st.metric("ç¨¼åƒä¸­", len(active_sites))
    with col3: st.metric("æº–å‚™ä¸­", len(planned_sites))
    with col4: st.metric("æœ¬æ—¥ã®åˆ†æ", 0)
    
    st.divider()
    st.subheader("ğŸ”— åˆ†æå¯¾è±¡ã‚µã‚¤ãƒˆä¸€è¦§")
    
    cols = st.columns(3)
    sorted_sites = sorted(ANALYZER_CONFIGS.items(), key=lambda x: (x[1]['status'] != 'active', x[0]))

    for i, (site_key, config) in enumerate(sorted_sites):
        with cols[i % 3]:
            status_color = "ğŸŸ¢" if config['status'] == 'active' else "ğŸŸ¡"
            status_text = "ç¨¼åƒä¸­" if config['status'] == 'active' else "æº–å‚™ä¸­"
            
            with st.container(border=True):
                st.markdown(f"""
                <h4 style="color: {config['color']}; margin: 0 0 10px 0;">
                    {status_color} {config['name']}
                </h4>
                <p style="margin: 5px 0; font-size: 0.9em;">
                    <strong>URL:</strong> <a href="{config['url']}" target="_blank">{config['url'][:30]}...</a>
                </p>
                <p style="margin: 5px 0; font-size: 0.9em;">
                    <strong>ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:</strong> {status_text}
                </p>
                <p style="margin: 5px 0; font-size: 0.8em; color: #666;">
                    {config['description']}
                </p>
                """, unsafe_allow_html=True)
                
                if st.button(f"ğŸš€ åˆ†æå®Ÿè¡Œ", key=f"dash_analyze_{site_key}", use_container_width=True):
                    st.session_state.active_analysis = site_key
                    st.rerun()

    # åˆ†æãŒãƒˆãƒªã‚¬ãƒ¼ã•ã‚ŒãŸã‚‰å®Ÿè¡Œ
    if st.session_state.active_analysis:
        site_key = st.session_state.active_analysis
        config = ANALYZER_CONFIGS[site_key]
        st.session_state.active_analysis = None # ãƒˆãƒªã‚¬ãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ
        
        with st.spinner(f"{config['name']}ã®åˆ†æã‚’å®Ÿè¡Œä¸­..."):
            run_analysis(site_key, config)

def show_individual_analysis():
    st.header("ğŸ”— å€‹åˆ¥ã‚µã‚¤ãƒˆåˆ†æ")
    
    selected_site = st.selectbox(
        "åˆ†æå¯¾è±¡ã‚µã‚¤ãƒˆã‚’é¸æŠ",
        options=list(ANALYZER_CONFIGS.keys()),
        format_func=lambda x: f"{ANALYZER_CONFIGS[x]['name']} ({x})"
    )
    
    config = ANALYZER_CONFIGS[selected_site]
    
    with st.container(border=True):
        st.subheader(f"ğŸ¯ {config['name']} ã®è¨­å®š")
        st.write(f"**URL:** {config['url']}")
        st.write(f"**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:** {config['status']}")
        st.write(f"**èª¬æ˜:** {config['description']}")
        
        st.write("**å°‚ç”¨æ©Ÿèƒ½:**")
        feature_cols = st.columns(len(config['features']))
        for i, feature in enumerate(config['features']):
            with feature_cols[i]:
                st.info(feature)

    st.divider()
    
    if st.button(f"ğŸ” {config['name']} åˆ†æé–‹å§‹", type="primary", use_container_width=True):
        with st.spinner(f"{config['name']}ã®åˆ†æã‚’å®Ÿè¡Œä¸­..."):
            run_analysis(selected_site, config)

def show_statistics():
    st.header("ğŸ“Š çµ±è¨ˆãƒ»æ¯”è¼ƒåˆ†æ")
    st.info("ã“ã®æ©Ÿèƒ½ã¯ç¾åœ¨æº–å‚™ä¸­ã§ã™ã€‚")

def show_settings():
    st.header("âš™ï¸ è¨­å®šç®¡ç†")
    st.info("ã“ã®æ©Ÿèƒ½ã¯ç¾åœ¨æº–å‚™ä¸­ã§ã™ã€‚")

if __name__ == "__main__":
    main()

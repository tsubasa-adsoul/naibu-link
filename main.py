# main.py ï¼ˆçœŸã®æœ€çµ‚å®Œæˆç‰ˆãƒ»å…¨æ©Ÿèƒ½çµ±åˆï¼‰

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
import os
import json
import math
import tempfile
from pathlib import Path
from collections import Counter
from datetime import datetime
from urllib.parse import urlparse, urlunparse
import base64
from io import BytesIO, StringIO
import zipfile
import importlib.util
import time

# PyVisï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    from pyvis.network import Network
    HAS_PYVIS = True
except ImportError:
    HAS_PYVIS = False

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ğŸ”— å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ«",
    page_icon="ğŸ”—",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .main-header { font-size: 2.5rem; font-weight: bold; text-align: center; margin-bottom: 2rem; color: #1f77b4; }
    .success-box { background-color: #d4edda; border: 1px solid #c3e6cb; border-radius: 0.25rem; padding: 0.75rem; margin: 1rem 0; }
    .warning-box { background-color: #fff3cd; border: 1px solid #ffeaa7; border-radius: 0.25rem; padding: 0.75rem; margin: 1rem 0; }
</style>
""", unsafe_allow_html=True)

# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ï¼ˆå¤‰æ›´ãªã—ï¼‰ ---
@st.cache_data
def normalize_url(u, default_scheme="https", base_domain=None):
    if not isinstance(u, str) or not u.strip(): return ""
    u = u.strip()
    try:
        p = urlparse(u)
        if not p.netloc and base_domain:
            path = u if u.startswith("/") else f"/{u}"
            u = f"{default_scheme}://{base_domain}{path}"
            p = urlparse(u)
        elif not p.netloc: return u
        scheme = p.scheme or default_scheme
        netloc = p.netloc.lower().replace("www.", "")
        path = p.path or "/"
        return urlunparse((scheme, netloc, path, p.params, p.query, ""))
    except Exception: return u

def detect_site_info(filename, df):
    filename = filename.lower()
    site_name_map = {
        'auto_answer': "Answerç¾é‡‘åŒ–", 'auto_arigataya': "ã‚ã‚ŠãŒãŸã‚„", 'auto_bicgift': "ãƒ“ãƒƒã‚¯ã‚®ãƒ•ãƒˆ",
        'auto_crecaeru': "ã‚¯ãƒ¬ã‹ãˆã‚‹", 'auto_flashpay_famipay': "ãƒ•ã‚¡ãƒŸãƒšã‚¤", 
        'auto_flashpay_media': "ãƒ¡ãƒ‡ã‚£ã‚¢", 'auto_friendpay': "ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒšã‚¤",
        'auto_fuyouhin': "ä¸ç”¨å“å›åéšŠ", 'auto_kaitori_life': "è²·å–LIFE", 'auto_kau_ru': "ã‚«ã‚¦ãƒ¼ãƒ«",
        'auto_morepay': "MorePay", 'auto_payful': "ãƒšã‚¤ãƒ•ãƒ«", 'auto_smart': "ã‚¹ãƒãƒ¼ãƒˆãƒšã‚¤", 'auto_xgift': "XGIFT"
    }
    site_name = "Unknown Site"
    for key, name in site_name_map.items():
        if key in filename:
            site_name = name
            break
    domains = [urlparse(u).netloc.replace("www.","") for u in df['C_URL'].dropna() if isinstance(u, str) and 'http' in u]
    site_domain = Counter(domains).most_common(1)[0][0] if domains else None
    return site_name, site_domain

def generate_html_table(title, columns, rows):
    def esc(x): return str(x).replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
    html = f"<!doctype html><meta charset='utf-8'><title>{esc(title)}</title>"
    html += "<style>body{font-family:sans-serif;padding:16px}table{border-collapse:collapse;width:100%}th,td{border:1px solid #ddd;padding:8px;font-size:14px}th{position:sticky;top:0;background:#f7f7f7}a{color:#1565c0;text-decoration:none}a:hover{text-decoration:underline}</style>"
    html += f"<h1>{esc(title)}</h1><table><thead><tr>{''.join(f'<th>{esc(c)}</th>' for c in columns)}</tr></thead><tbody>"
    for row in rows:
        html += "<tr>"
        for i, cell in enumerate(row):
            val = esc(cell)
            if "URL" in columns[i].upper() and val.startswith("http"):
                val = f"<a href='{val}' target='_blank'>{val}</a>"
            html += f"<td>{val}</td>"
        html += "</tr>"
    html += "</tbody></table>"
    return html

def run_analysis_loop():
    state = st.session_state.analysis_state
    site_name = state['site_name']
    
    st.info(f"ã€Œ{site_name}ã€ã®åˆ†æã‚’å®Ÿè¡Œä¸­... (ãƒ•ã‚§ãƒ¼ã‚º: {state.get('phase', 'unknown')})")
    log_placeholder = st.empty()
    progress_placeholder = st.empty()
    
    while state.get('running') and state.get('phase') != 'completed':
        try:
            spec = importlib.util.spec_from_file_location(site_name, f"{site_name}.py")
            site_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(site_module)
            
            state = site_module.analyze_step(state)
            
            log_placeholder.code('\n'.join(state.get('log', [])), language="log")
            if 'progress' in state:
                progress_placeholder.progress(state['progress'], text=state.get('progress_text', ''))
            
            st.session_state.analysis_state = state
            time.sleep(0.5)

        except Exception as e:
            st.error(f"åˆ†æä¸­ã«è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.exception(e)
            state['running'] = False
            st.session_state.analysis_state = state
            break

    if st.session_state.analysis_state.get('phase') == 'completed':
        st.session_state.analysis_state['running'] = False
        st.success("åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚çµæœã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
        
        spec = importlib.util.spec_from_file_location(site_name, f"{site_name}.py")
        site_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(site_module)
        csv_string = site_module.generate_csv(st.session_state.analysis_state)
        
        st.session_state['last_analyzed_csv_data'] = csv_string
        st.session_state['last_analyzed_filename'] = f"{site_name}_analysis.csv"
        st.rerun()

# ãƒ¡ã‚¤ãƒ³é–¢æ•°
def main():
    st.markdown('<div class="main-header">ğŸ”— å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ«</div>', unsafe_allow_html=True)
    
    if 'analysis_state' not in st.session_state:
        st.session_state.analysis_state = {}

    with st.sidebar:
        st.header("ğŸ“ ãƒ‡ãƒ¼ã‚¿è¨­å®š")
        
        try:
            site_files = sorted([f for f in os.listdir('.') if f.startswith('auto_') and f.endswith('.py')])
            site_names = [os.path.splitext(f)[0] for f in site_files]
        except Exception:
            site_files, site_names = [], []

        source_options = ["CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"]
        if site_names:
            source_options.insert(0, "ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§æ–°è¦åˆ†æã‚’å®Ÿè¡Œ")
        
        analysis_source = st.radio("ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’é¸æŠ", source_options, key="analysis_source", on_change=lambda: st.session_state.pop('analysis_state', None))
        
        uploaded_file = None
        
        if analysis_source == "ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§æ–°è¦åˆ†æã‚’å®Ÿè¡Œ" and site_names:
            st.subheader("åˆ†æå¯¾è±¡ã‚µã‚¤ãƒˆ")
            selected_site_name = st.selectbox("ã‚µã‚¤ãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„", options=site_names)
            
            col1, col2 = st.columns(2)
            with col1:
                if st.button("ğŸš€ åˆ†æé–‹å§‹/å†é–‹", key="run_analysis"):
                    if st.session_state.analysis_state.get('site_name') != selected_site_name:
                        st.session_state.analysis_state = {
                            'site_name': selected_site_name, 'phase': 'initializing', 'log': [f"ã€Œ{selected_site_name}ã€ã®åˆ†ææº–å‚™ã‚’é–‹å§‹ã—ã¾ã™..."],
                            'to_visit': [], 'visited': set(), 'pages': {}, 'processed_links': set(), 'links': [], 'detailed_links': []
                        }
                    st.session_state.analysis_state['running'] = True
                    st.rerun()
            with col2:
                if st.button("â¹ï¸ ä¸­æ–­/ãƒªã‚»ãƒƒãƒˆ"):
                    st.session_state.analysis_state = {}
                    st.rerun()
        else:
            uploaded_file = st.file_uploader("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['csv'])
        
        st.header("ğŸ› ï¸ åˆ†æè¨­å®š")
        network_top_n = st.slider("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ï¼šä¸Šä½Nä»¶", 10, 100, 40, 5, help="ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã«è¡¨ç¤ºã™ã‚‹ä¸Šä½ãƒšãƒ¼ã‚¸æ•°")

    if st.session_state.analysis_state.get('running'):
        run_analysis_loop()
        return

    data_source = None
    filename_for_detect = "analysis"
    if uploaded_file:
        data_source = uploaded_file
        filename_for_detect = uploaded_file.name
    elif 'last_analyzed_csv_data' in st.session_state:
        data_source = StringIO(st.session_state['last_analyzed_csv_data'])
        filename_for_detect = st.session_state['last_analyzed_filename']
    
    if data_source is None:
        st.info("ğŸ‘† ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’é¸æŠã—ã€åˆ†æã‚’å®Ÿè¡Œã™ã‚‹ã‹ã€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return

    try:
        # â˜…â˜…â˜… ã“ã“ãŒæœ€é‡è¦ä¿®æ­£ç‚¹ â˜…â˜…â˜…
        # ä¸»ã®å„ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãŒå‡ºåŠ›ã™ã‚‹åˆ—å
        csv_columns = ['ç•ªå·', 'ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'URL', 'è¢«ãƒªãƒ³ã‚¯å…ƒã‚¿ã‚¤ãƒˆãƒ«', 'è¢«ãƒªãƒ³ã‚¯å…ƒURL', 'ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ']
        df = pd.read_csv(data_source, encoding="utf-8-sig", header=0, names=csv_columns, on_bad_lines='skip').fillna("")
        
        # å†…éƒ¨å‡¦ç†ã§ä½¿ã†çµ±ä¸€çš„ãªåˆ—åã«å¤‰æ›
        df.rename(columns={
            'ç•ªå·': 'A_ç•ªå·', 'ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«': 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'URL': 'C_URL',
            'è¢«ãƒªãƒ³ã‚¯å…ƒã‚¿ã‚¤ãƒˆãƒ«': 'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'è¢«ãƒªãƒ³ã‚¯å…ƒURL': 'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL',
            'ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ': 'F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'
        }, inplace=True)
        
        # A_ç•ªå·ã®å‡¦ç†ã‚’ã‚ˆã‚Šå®‰å…¨ã«
        df['A_ç•ªå·'] = pd.to_numeric(df['A_ç•ªå·'], errors='coerce')
        df['A_ç•ªå·'] = df.groupby('C_URL').ngroup() + 1
        
        # --- â˜…â˜…â˜… ã“ã“ã‹ã‚‰ä¸‹ã¯ã€ä¸»ã®ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ç´ æ™´ã‚‰ã—ã„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ©Ÿèƒ½ã§ã™ â˜…â˜…â˜…
        
        site_name, site_domain = detect_site_info(filename_for_detect, df)
        
        df['C_URL'] = df['C_URL'].apply(lambda u: normalize_url(u, base_domain=site_domain))
        df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'] = df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].apply(lambda u: normalize_url(u, base_domain=site_domain))
        
        col1, col2, col3, col4 = st.columns(4)
        unique_pages_df = df[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL']].drop_duplicates()
        has_link = (df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].astype(str) != "")
        unique_anchors = df[df['F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'].astype(str) != '']['F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'].nunique()
        col1.metric("ğŸ“Š ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", len(df))
        col2.metric("ğŸ“„ ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒšãƒ¼ã‚¸æ•°", len(unique_pages_df))
        col3.metric("ğŸ”— å†…éƒ¨ãƒªãƒ³ã‚¯æ•°", int(has_link.sum()))
        col4.metric("ğŸ·ï¸ ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¢ãƒ³ã‚«ãƒ¼æ•°", unique_anchors)
        
        st.markdown(f"""<div class="success-box"><strong>ğŸ“ èª­ã¿è¾¼ã¿å®Œäº†:</strong> {filename_for_detect}<br><strong>ğŸŒ ã‚µã‚¤ãƒˆ:</strong> {site_name}<br><strong>ğŸ”— ãƒ‰ãƒ¡ã‚¤ãƒ³:</strong> {site_domain or 'ä¸æ˜'}</div>""", unsafe_allow_html=True)
        
        tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ›ï¸ ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸", "ğŸ§© ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ", "ğŸ§­ å­¤ç«‹è¨˜äº‹", "ğŸ“ˆ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³", "ğŸ“Š ç·åˆãƒ¬ãƒãƒ¼ãƒˆ"])
        
        pages_df = df[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL']].drop_duplicates().copy()
        inbound_df = df[(df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].astype(str) != "") & (df['C_URL'].astype(str) != "")]
        inbound_counts = inbound_df.groupby('C_URL').size()
        pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'] = pages_df['C_URL'].map(inbound_counts).fillna(0).astype(int)
        pages_df = pages_df.sort_values(['è¢«ãƒªãƒ³ã‚¯æ•°', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«'], ascending=[False, True]).reset_index(drop=True)

        with tab1:
            st.header("ğŸ›ï¸ ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸åˆ†æ")
            top_pages = pages_df.head(20)
            fig = px.bar(top_pages.head(15).sort_values('è¢«ãƒªãƒ³ã‚¯æ•°'), x='è¢«ãƒªãƒ³ã‚¯æ•°', y='B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', orientation='h', title="è¢«ãƒªãƒ³ã‚¯æ•° TOP15")
            st.plotly_chart(fig, use_container_width=True)
            st.subheader("ğŸ“‹ ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ä¸€è¦§")
            st.dataframe(top_pages[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL', 'è¢«ãƒªãƒ³ã‚¯æ•°']], use_container_width=True)

        with tab2:
            st.header("ğŸ§© ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æï¼ˆã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆï¼‰")
            anchor_df = df[df['F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'].astype(str) != '']
            anchor_counts = Counter(anchor_df['F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'])
            if anchor_counts:
                total_anchors = sum(anchor_counts.values())
                diversity_index = 1 - sum((c/total_anchors)**2 for c in anchor_counts.values())
                c1,c2,c3 = st.columns(3)
                c1.metric("ğŸ”¢ ç·ã‚¢ãƒ³ã‚«ãƒ¼æ•°", total_anchors)
                c2.metric("ğŸ·ï¸ ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¢ãƒ³ã‚«ãƒ¼æ•°", len(anchor_counts))
                c3.metric("ğŸ“Š å¤šæ§˜æ€§æŒ‡æ•°", f"{diversity_index:.3f}", help="1ã«è¿‘ã„ã»ã©å¤šæ§˜")
                top_anchors = pd.DataFrame(anchor_counts.most_common(15), columns=['ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ', 'é »åº¦']).sort_values('é »åº¦')
                fig = px.bar(top_anchors, x='é »åº¦', y='ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ', orientation='h', title="ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆé »åº¦ TOP15")
                st.plotly_chart(fig, use_container_width=True)
                st.subheader("ğŸ“‹ ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆä¸€è¦§")
                st.dataframe(pd.DataFrame(anchor_counts.most_common(), columns=['ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ', 'é »åº¦']), use_container_width=True)
            else:
                st.warning("âš ï¸ ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

        with tab3:
            st.header("ğŸ§­ å­¤ç«‹è¨˜äº‹åˆ†æ")
            isolated_pages = pages_df[pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'] == 0].copy()
            if not isolated_pages.empty:
                st.metric("ğŸï¸ å­¤ç«‹è¨˜äº‹æ•°", len(isolated_pages))
                st.dataframe(isolated_pages[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL']], use_container_width=True)
            else:
                st.success("ğŸ‰ å­¤ç«‹è¨˜äº‹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼")

        with tab4:
            st.header("ğŸ“ˆ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³")
            if HAS_PYVIS:
                st.info("ğŸ”„ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã‚’ç”Ÿæˆä¸­...")
                try:
                    edges_df = df[(df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'] != "") & (df['C_URL'] != "")].copy()
                    top_n_urls = set(pages_df.head(network_top_n)['C_URL'])
                    source_urls_to_top_n = set(edges_df[edges_df['C_URL'].isin(top_n_urls)]['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'])
                    relevant_urls = top_n_urls.union(source_urls_to_top_n)
                    sub_edges = edges_df[edges_df['C_URL'].isin(relevant_urls) & edges_df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].isin(relevant_urls)]
                    
                    if not sub_edges.empty:
                        agg = sub_edges.groupby(['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL', 'C_URL']).size().reset_index(name='weight')
                        url_map = pd.concat([df[['C_URL', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«']].rename(columns={'C_URL':'url', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«':'title'}), df[['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL', 'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«']].rename(columns={'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL':'url', 'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«':'title'})]).drop_duplicates('url').set_index('url')['title'].to_dict()
                        net = Network(height="800px", width="100%", directed=True, notebook=True, cdn_resources='in_line')
                        net.set_options('{"physics":{"barnesHut":{"gravitationalConstant":-8000,"springLength":150,"avoidOverlap":0.1}}}')
                        nodes = set(agg['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL']).union(set(agg['C_URL']))
                        for u in nodes:
                            net.add_node(u, label=str(url_map.get(u, u))[:20], title=url_map.get(u, u), size=10 + math.log2(inbound_counts.get(u, 0) + 1) * 4)
                        for _, r in agg.iterrows():
                            net.add_edge(r['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'], r['C_URL'], value=r['weight'])
                        with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False) as tmp:
                            net.save_graph(tmp.name)
                            with open(tmp.name, 'r', encoding='utf-8') as f:
                                st.components.v1.html(f.read(), height=820, scrolling=True)
                        os.unlink(tmp.name)
                    else: st.warning("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æç”»ã™ã‚‹ãŸã‚ã®ãƒªãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
                except Exception as e:
                    st.error(f"âŒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            else: st.error("âŒ pyvisãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™ã€‚`pip install pyvis`ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
        
        with tab5:
            st.header("ğŸ“Š ç·åˆãƒ¬ãƒãƒ¼ãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
            # (ç·åˆãƒ¬ãƒãƒ¼ãƒˆã®ãƒ­ã‚¸ãƒƒã‚¯)
            pass

    except Exception as e:
        st.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.exception(e)

if __name__ == "__main__":
    main()

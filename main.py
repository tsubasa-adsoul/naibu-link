#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ« - Streamlitç‰ˆ
- ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ / ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ï¼ˆã‚¢ãƒ³ã‚«ãƒ¼ï¼‰ / å­¤ç«‹è¨˜äº‹ã‚’å…¨ä»¶å‡ºåŠ›
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ï¼ˆé™çš„ï¼šplotlyã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ï¼špyvisï¼‰
- CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
- HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
import os
import json
import math
import tempfile
from pathlib import Path
from collections import Counter
from datetime import datetime
from urllib.parse import urlparse
import base64
from io import BytesIO
import zipfile

# PyVisï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    from pyvis.network import Network
    HAS_PYVIS = True
except ImportError:
    HAS_PYVIS = False

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ğŸ”— å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ«",
    page_icon="ğŸ”—",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        margin-bottom: 2rem;
        color: #1f77b4;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
    }
    .success-box {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        border-radius: 0.25rem;
        padding: 0.75rem;
        margin: 1rem 0;
    }
    .warning-box {
        background-color: #fff3cd;
        border: 1px solid #ffeaa7;
        border-radius: 0.25rem;
        padding: 0.75rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
@st.cache_data
def safe_str(s):
    return s if isinstance(s, str) else ""

@st.cache_data
def normalize_url(u, default_scheme="https", base_domain=None):
    """URLæ­£è¦åŒ–"""
    if not isinstance(u, str) or not u.strip():
        return ""
    u = u.strip()
    
    try:
        from urllib.parse import urlparse, urlunparse
        p = urlparse(u)

        if not p.netloc and base_domain:
            path = u if u.startswith("/") else f"/{u}"
            u = f"{default_scheme}://{base_domain}{path}"
            p = urlparse(u)
        elif not p.netloc:
            return u

        scheme = p.scheme or default_scheme
        netloc = p.netloc.lower()
        if netloc.startswith("www."):
            netloc = netloc[4:]
        path = p.path or "/"
        return urlunparse((scheme, netloc, path, p.params, p.query, ""))
    except Exception:
        return u

def detect_site_info(filename, df):
    """ãƒ•ã‚¡ã‚¤ãƒ«åã¨URLã‹ã‚‰ã‚µã‚¤ãƒˆæƒ…å ±ã‚’æ¨æ¸¬"""
    filename = filename.lower()
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚µã‚¤ãƒˆåæ¨æ¸¬
    if 'kau-ru' in filename:
        site_name = "ã‚«ã‚¦ãƒ¼ãƒ«"
    elif 'kaitori-life' in filename:
        site_name = "è²·å–LIFE"
    elif 'friendpay' in filename:
        site_name = "ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒšã‚¤"
    elif 'kurekaeru' in filename or 'crecaeru' in filename:
        site_name = "ã‚¯ãƒ¬ã‹ãˆã‚‹"
    else:
        site_name = "Unknown Site"
    
    # URLã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³æ¨æ¸¬
    domains = []
    for u in df['C_URL'].tolist():
        if isinstance(u, str) and u:
            try:
                d = urlparse(u).netloc.lower()
                if d.startswith("www."):
                    d = d[4:]
                if d:
                    domains.append(d)
            except Exception:
                continue
    
    site_domain = Counter(domains).most_common(1)[0][0] if domains else None
    
    return site_name, site_domain

def create_download_link(content, filename, link_text="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"):
    """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ã‚’ä½œæˆ"""
    if isinstance(content, str):
        content = content.encode('utf-8')
    
    b64 = base64.b64encode(content).decode()
    href = f'<a href="data:text/html;base64,{b64}" download="{filename}" style="text-decoration: none; background-color: #1f77b4; color: white; padding: 0.5rem 1rem; border-radius: 0.25rem; display: inline-block;">{link_text}</a>'
    return href

def generate_html_table(title, columns, rows):
    """HTMLè¡¨ã‚’ç”Ÿæˆ"""
    def esc(x):
        return str(x).replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
    
    html_parts = [
        "<!doctype html>",
        "<meta charset='utf-8'>",
        f"<title>{esc(title)}</title>",
        """<style>
            body { font-family: Arial, 'Yu Gothic', Meiryo, sans-serif; padding: 16px; }
            table { border-collapse: collapse; width: 100%; }
            th, td { border: 1px solid #ddd; padding: 8px; font-size: 14px; }
            th { position: sticky; top: 0; background: #f7f7f7; font-weight: bold; }
            tr:nth-child(even) { background: #fafafa; }
            a { color: #1565c0; text-decoration: none; }
            a:hover { text-decoration: underline; }
            .header { text-align: center; margin-bottom: 2rem; }
        </style>""",
        f"<div class='header'><h1>{esc(title)}</h1></div>",
        "<table><thead><tr>"
    ]
    
    for col in columns:
        html_parts.append(f"<th>{esc(col)}</th>")
    
    html_parts.append("</tr></thead><tbody>")
    
    for row in rows:
        html_parts.append("<tr>")
        for i, cell in enumerate(row):
            val = esc(cell)
            header = columns[i]
            if "URL" in header.upper() or header.endswith("URL"):
                if val and (val.startswith("http://") or val.startswith("https://")):
                    val = f"<a href='{val}' target='_blank' rel='noopener'>{val}</a>"
            html_parts.append(f"<td>{val}</td>")
        html_parts.append("</tr>")
    
    html_parts.append("</tbody></table>")
    return "".join(html_parts)

# ãƒ¡ã‚¤ãƒ³é–¢æ•°
def main():
    # ãƒ˜ãƒƒãƒ€ãƒ¼
    st.markdown('<div class="main-header">ğŸ”— å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ«</div>', unsafe_allow_html=True)
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.header("ğŸ“ ãƒ‡ãƒ¼ã‚¿è¨­å®š")
        
        # CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        uploaded_file = st.file_uploader(
            "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
            type=['csv'],
            help="å†…éƒ¨ãƒªãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„"
        )
        
        if uploaded_file is not None:
            st.success("âœ… ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸ")
        
        st.header("ğŸ› ï¸ åˆ†æè¨­å®š")
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³è¨­å®š
        network_top_n = st.slider(
            "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ï¼šä¸Šä½Nä»¶",
            min_value=10,
            max_value=100,
            value=40,
            step=5,
            help="ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã«è¡¨ç¤ºã™ã‚‹ä¸Šä½ãƒšãƒ¼ã‚¸æ•°"
        )
        
        show_isolated = st.checkbox(
            "å­¤ç«‹ãƒšãƒ¼ã‚¸ã‚‚è¡¨ç¤º",
            value=False,
            help="ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã«å­¤ç«‹ãƒšãƒ¼ã‚¸ã‚‚å«ã‚ã‚‹"
        )
        
        # ãƒ¬ãƒãƒ¼ãƒˆè¨­å®š
        st.header("ğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆè¨­å®š")
        auto_download = st.checkbox(
            "HTMLãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆ",
            value=True,
            help="åˆ†æå®Ÿè¡Œæ™‚ã«HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’è‡ªå‹•ç”Ÿæˆ"
        )

    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
    if uploaded_file is None:
        st.info("ğŸ‘† ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
        
        # æœŸå¾…ã•ã‚Œã‚‹CSVãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’è¡¨ç¤º
        st.subheader("ğŸ“‹ æœŸå¾…ã•ã‚Œã‚‹CSVãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ")
        expected_columns = [
            'A_ç•ªå·',
            'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 
            'C_URL',
            'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«',
            'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL',
            'F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'
        ]
        
        sample_data = pd.DataFrame({
            'ã‚«ãƒ©ãƒ å': expected_columns,
            'èª¬æ˜': [
                'é€£ç•ª',
                'ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒšãƒ¼ã‚¸ã®ã‚¿ã‚¤ãƒˆãƒ«',
                'ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒšãƒ¼ã‚¸ã®URL',
                'ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã®ã‚¿ã‚¤ãƒˆãƒ«',
                'ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã®URL',
                'ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'
            ]
        })
        st.dataframe(sample_data, use_container_width=True)
        return
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    try:
        df = pd.read_csv(uploaded_file, encoding="utf-8-sig").fillna("")
        
        # å¿…è¦ãªåˆ—ã®å­˜åœ¨ç¢ºèª
        expected_columns = [
            'A_ç•ªå·', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL',
            'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL', 'F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'
        ]
        
        missing_columns = [col for col in expected_columns if col not in df.columns]
        if missing_columns:
            st.error(f"âŒ å¿…è¦ãªåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing_columns}")
            st.write("å®Ÿéš›ã®åˆ—:", list(df.columns))
            return
        
        # ã‚µã‚¤ãƒˆæƒ…å ±æ¤œå‡º
        site_name, site_domain = detect_site_info(uploaded_file.name, df)
        
        # URLæ­£è¦åŒ–
        def norm_url(u):
            return normalize_url(u, base_domain=site_domain)
        
        df['C_URL'] = df['C_URL'].apply(norm_url)
        df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'] = df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].apply(norm_url)
        
        # ãƒ‡ãƒ¼ã‚¿æ¦‚è¦è¡¨ç¤º
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("ğŸ“Š ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", len(df))
        
        with col2:
            unique_pages = df[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL']].drop_duplicates()
            st.metric("ğŸ“„ ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒšãƒ¼ã‚¸æ•°", len(unique_pages))
        
        with col3:
            has_link = (df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].astype(str) != "") & (df['C_URL'].astype(str) != "")
            st.metric("ğŸ”— å†…éƒ¨ãƒªãƒ³ã‚¯æ•°", has_link.sum())
        
        with col4:
            unique_anchors = df[df['F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'].astype(str) != '']['F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'].nunique()
            st.metric("ğŸ·ï¸ ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¢ãƒ³ã‚«ãƒ¼æ•°", unique_anchors)
        
        st.markdown(f"""
        <div class="success-box">
            <strong>ğŸ“ èª­ã¿è¾¼ã¿å®Œäº†:</strong> {uploaded_file.name}<br>
            <strong>ğŸŒ ã‚µã‚¤ãƒˆ:</strong> {site_name}<br>
            <strong>ğŸ”— ãƒ‰ãƒ¡ã‚¤ãƒ³:</strong> {site_domain or 'ä¸æ˜'}
        </div>
        """, unsafe_allow_html=True)
        
        # ã‚¿ãƒ–ã§æ©Ÿèƒ½ã‚’åˆ†å‰²
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "ğŸ›ï¸ ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸", "ğŸ§© ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ", "ğŸ§­ å­¤ç«‹è¨˜äº‹", 
            "ğŸ“ˆ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³", "ğŸ“Š ç·åˆãƒ¬ãƒãƒ¼ãƒˆ"
        ])
        
        # å…±é€šãƒ‡ãƒ¼ã‚¿æº–å‚™
        pages_df = df[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL']].drop_duplicates().copy()
        
        # è¢«ãƒªãƒ³ã‚¯æ•°è¨ˆç®—
        has_source = (df['D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«'].astype(str) != "") & (df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].astype(str) != "")
        inbound_df = df[has_source & (df['C_URL'].astype(str) != "")]
        inbound_counts = inbound_df.groupby('C_URL').size()
        
        pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'] = pages_df['C_URL'].map(inbound_counts).fillna(0).astype(int)
        pages_df = pages_df.sort_values(['è¢«ãƒªãƒ³ã‚¯æ•°', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«'], ascending=[False, True])
        
        # Tab 1: ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸åˆ†æ
        with tab1:
            st.header("ğŸ›ï¸ ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸åˆ†æ")
            st.write("è¢«ãƒªãƒ³ã‚¯æ•°ã®å¤šã„ãƒšãƒ¼ã‚¸ã‚’ç‰¹å®šã—ã¾ã™ã€‚")
            
            # ä¸Šä½è¡¨ç¤ºä»¶æ•°è¨­å®š
            top_n_pillar = st.slider("è¡¨ç¤ºä»¶æ•°", 5, 50, 20, key="pillar_top_n")
            
            # ä¸Šä½ãƒšãƒ¼ã‚¸è¡¨ç¤º
            top_pages = pages_df.head(top_n_pillar)
            
            # ã‚°ãƒ©ãƒ•è¡¨ç¤º
            fig = px.bar(
                top_pages.head(15), 
                x='è¢«ãƒªãƒ³ã‚¯æ•°', 
                y='B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«',
                orientation='h',
                title="è¢«ãƒªãƒ³ã‚¯æ•° TOP15",
                labels={'è¢«ãƒªãƒ³ã‚¯æ•°': 'è¢«ãƒªãƒ³ã‚¯æ•°', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«': 'ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«'}
            )
            fig.update_layout(yaxis={'categoryorder':'total ascending'}, height=600)
            st.plotly_chart(fig, use_container_width=True)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
            st.subheader("ğŸ“‹ ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ä¸€è¦§")
            display_df = top_pages[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL', 'è¢«ãƒªãƒ³ã‚¯æ•°']].copy()
            display_df.index = range(1, len(display_df) + 1)
            st.dataframe(display_df, use_container_width=True)
            
            # HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            if auto_download and st.button("ğŸ“¥ ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", key="download_pillar"):
                rows = [[i, safe_str(row['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«']), safe_str(row['C_URL']), int(row['è¢«ãƒªãƒ³ã‚¯æ•°'])]
                        for i, (_, row) in enumerate(pages_df.iterrows(), 1)]
                
                html_content = generate_html_table(
                    f"{site_name} ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸åˆ†æãƒ¬ãƒãƒ¼ãƒˆ",
                    ["#", "ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«", "URL", "è¢«ãƒªãƒ³ã‚¯æ•°"],
                    rows
                )
                
                filename = f"pillar_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
                download_link = create_download_link(html_content, filename)
                st.markdown(download_link, unsafe_allow_html=True)
        
        # Tab 2: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ
        with tab2:
            st.header("ğŸ§© ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æï¼ˆã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆï¼‰")
            st.write("ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã®é »åº¦ã¨å¤šæ§˜æ€§ã‚’åˆ†æã—ã¾ã™ã€‚")
            
            # ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
            anchor_df = df[df['F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'].astype(str) != '']
            anchor_counts = Counter(anchor_df['F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'])
            
            if anchor_counts:
                total_anchors = sum(anchor_counts.values())
                unique_anchors = len(anchor_counts)
                
                # HHIï¼ˆãƒãƒ¼ãƒ•ã‚£ãƒ³ãƒ€ãƒ¼ãƒ«ãƒ»ãƒãƒ¼ã‚·ãƒ¥ãƒãƒ³æŒ‡æ•°ï¼‰è¨ˆç®—
                hhi = sum((count/total_anchors)**2 for count in anchor_counts.values())
                diversity_index = 1 - hhi
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ğŸ”¢ ç·ã‚¢ãƒ³ã‚«ãƒ¼æ•°", total_anchors)
                with col2:
                    st.metric("ğŸ·ï¸ ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¢ãƒ³ã‚«ãƒ¼æ•°", unique_anchors)
                with col3:
                    st.metric("ğŸ“Š å¤šæ§˜æ€§æŒ‡æ•°", f"{diversity_index:.3f}", help="1ã«è¿‘ã„ã»ã©å¤šæ§˜")
                
                # ä¸Šä½ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã®ã‚°ãƒ©ãƒ•
                top_anchors = pd.DataFrame(anchor_counts.most_common(20), columns=['ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ', 'é »åº¦'])
                
                fig = px.bar(
                    top_anchors.head(15),
                    x='é »åº¦',
                    y='ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ',
                    orientation='h',
                    title="ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆé »åº¦ TOP15"
                )
                fig.update_layout(yaxis={'categoryorder':'total ascending'}, height=600)
                st.plotly_chart(fig, use_container_width=True)
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
                st.subheader("ğŸ“‹ ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆä¸€è¦§")
                display_anchors = pd.DataFrame(anchor_counts.most_common(), columns=['ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ', 'é »åº¦'])
                display_anchors.index = range(1, len(display_anchors) + 1)
                st.dataframe(display_anchors, use_container_width=True)
                
                # HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
                if auto_download and st.button("ğŸ“¥ ã‚¢ãƒ³ã‚«ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", key="download_anchor"):
                    rows = [[i, anchor, count] for i, (anchor, count) in enumerate(anchor_counts.most_common(), 1)]
                    
                    html_content = generate_html_table(
                        f"{site_name} ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ",
                        ["#", "ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ", "é »åº¦"],
                        rows
                    )
                    
                    filename = f"anchor_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
                    download_link = create_download_link(html_content, filename)
                    st.markdown(download_link, unsafe_allow_html=True)
            else:
                st.warning("âš ï¸ ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        
        # Tab 3: å­¤ç«‹è¨˜äº‹
        with tab3:
            st.header("ğŸ§­ å­¤ç«‹è¨˜äº‹åˆ†æ")
            st.write("å†…éƒ¨ãƒªãƒ³ã‚¯ã‚’å—ã‘ã¦ã„ãªã„ãƒšãƒ¼ã‚¸ã‚’ç‰¹å®šã—ã¾ã™ã€‚")
            
            # å­¤ç«‹è¨˜äº‹æŠ½å‡º
            isolated_pages = pages_df[pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'] == 0].copy()
            
            if not isolated_pages.empty:
                st.metric("ğŸï¸ å­¤ç«‹è¨˜äº‹æ•°", len(isolated_pages))
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
                st.subheader("ğŸ“‹ å­¤ç«‹è¨˜äº‹ä¸€è¦§")
                display_isolated = isolated_pages[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL']].copy()
                display_isolated.index = range(1, len(display_isolated) + 1)
                st.dataframe(display_isolated, use_container_width=True)
                
                # HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
                if auto_download and st.button("ğŸ“¥ å­¤ç«‹è¨˜äº‹ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", key="download_isolated"):
                    rows = [[i, safe_str(row['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«']), safe_str(row['C_URL'])]
                            for i, (_, row) in enumerate(isolated_pages.iterrows(), 1)]
                    
                    html_content = generate_html_table(
                        f"{site_name} å­¤ç«‹è¨˜äº‹åˆ†æãƒ¬ãƒãƒ¼ãƒˆ",
                        ["#", "ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«", "URL"],
                        rows
                    )
                    
                    filename = f"isolated_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
                    download_link = create_download_link(html_content, filename)
                    st.markdown(download_link, unsafe_allow_html=True)
            else:
                st.success("ğŸ‰ å­¤ç«‹è¨˜äº‹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼")
        
        # Tab 4: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³
        with tab4:
            st.header("ğŸ“ˆ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³")
            st.write("å†…éƒ¨ãƒªãƒ³ã‚¯ã®é–¢ä¿‚æ€§ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚")
            
            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¿ã‚¤ãƒ—é¸æŠ
            network_type = st.radio(
                "è¡¨ç¤ºã‚¿ã‚¤ãƒ—ã‚’é¸æŠ",
                ["æ•£å¸ƒå›³ï¼ˆè»½é‡ï¼‰", "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ï¼ˆä¸­é‡é‡ï¼‰", "ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ï¼ˆé‡ã„ï¼‰"],
                key="network_type"
            )
            
            if network_type == "æ•£å¸ƒå›³ï¼ˆè»½é‡ï¼‰":
                # ç°¡æ˜“æ•£å¸ƒå›³
                top_pages_network = pages_df.head(network_top_n)
                
                fig = px.scatter(
                    top_pages_network,
                    x=range(len(top_pages_network)),
                    y='è¢«ãƒªãƒ³ã‚¯æ•°',
                    size='è¢«ãƒªãƒ³ã‚¯æ•°',
                    hover_data=['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL'],
                    title=f"è¢«ãƒªãƒ³ã‚¯æ•°åˆ†å¸ƒï¼ˆä¸Šä½{network_top_n}ä»¶ï¼‰"
                )
                fig.update_layout(
                    xaxis_title="ãƒšãƒ¼ã‚¸é †ä½",
                    yaxis_title="è¢«ãƒªãƒ³ã‚¯æ•°",
                    height=600
                )
                st.plotly_chart(fig, use_container_width=True)
            
            elif network_type == "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ï¼ˆä¸­é‡é‡ï¼‰":
                # Plotlyãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³
                st.info("ğŸ”„ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã‚’ç”Ÿæˆä¸­...")
                
                # ã‚¨ãƒƒã‚¸ãƒ‡ãƒ¼ã‚¿æº–å‚™
                edges_df = df[
                    (df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].astype(str) != "") &
                    (df['C_URL'].astype(str) != "")
                ][['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL', 'C_URL']].copy()
                
                # ä¸Šä½ãƒšãƒ¼ã‚¸ã®ã¿
                top_urls = set(pages_df.head(network_top_n)['C_URL'])
                edges_filtered = edges_df[
                    edges_df['C_URL'].isin(top_urls)
                ]
                
                if not edges_filtered.empty:
                    # ãƒãƒ¼ãƒ‰æº–å‚™
                    nodes = set(edges_filtered['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL']).union(set(edges_filtered['C_URL']))
                    node_list = list(nodes)
                    node_indices = {node: i for i, node in enumerate(node_list)}
                    
                    # ã‚¨ãƒƒã‚¸æº–å‚™
                    edge_trace = []
                    for _, row in edges_filtered.iterrows():
                        x0, y0 = divmod(node_indices[row['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL']], 10)
                        x1, y1 = divmod(node_indices[row['C_URL']], 10)
                        edge_trace.extend([x0, x1, None])
                        edge_trace.extend([y0, y1, None])
                    
                    # ãƒãƒ¼ãƒ‰ä½ç½®ã¨ã‚µã‚¤ã‚º
                    node_x = [i // 10 for i in range(len(node_list))]
                    node_y = [i % 10 for i in range(len(node_list))]
                    node_sizes = [max(10, inbound_counts.get(node, 0) * 2) for node in node_list]
                    
                    # ã‚°ãƒ©ãƒ•ä½œæˆ
                    fig = go.Figure()
                    
                    # ã‚¨ãƒƒã‚¸æç”»
                    fig.add_trace(go.Scatter(
                        x=edge_trace[::3],
                        y=edge_trace[1::3],
                        mode='lines',
                        line=dict(width=0.5, color='#888'),
                        hoverinfo='none',
                        showlegend=False
                    ))
                    
                    # ãƒãƒ¼ãƒ‰æç”»
                    fig.add_trace(go.Scatter(
                        x=node_x,
                        y=node_y,
                        mode='markers',
                        marker=dict(
                            size=node_sizes,
                            color='lightblue',
                            line=dict(width=1, color='darkblue')
                        ),
                        text=[f"è¢«ãƒªãƒ³ã‚¯: {inbound_counts.get(node, 0)}" for node in node_list],
                        hoverinfo='text',
                        showlegend=False
                    ))
                    
                    fig.update_layout(
                        title="å†…éƒ¨ãƒªãƒ³ã‚¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
                        showlegend=False,
                        hovermode='closest',
                        margin=dict(b=20,l=5,r=5,t=40),
                        annotations=[
                            dict(
                                text="ãƒãƒ¼ãƒ‰ã‚µã‚¤ã‚º = è¢«ãƒªãƒ³ã‚¯æ•°",
                                showarrow=False,
                                xref="paper", yref="paper",
                                x=0.005, y=-0.002,
                                xanchor='left', yanchor='bottom',
                                font=dict(size=12)
                            )
                        ],
                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                        height=600
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                else:
                    st.warning("âš ï¸ è¡¨ç¤ºã§ãã‚‹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            
            elif network_type == "ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ï¼ˆé‡ã„ï¼‰":
                if not HAS_PYVIS:
                    st.error("âŒ pyvisãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™ã€‚`pip install pyvis`ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
                else:
                    st.info("ğŸ”„ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã‚’ç”Ÿæˆä¸­...")
                    
                    # å…ƒã®ãƒ­ãƒ¼ã‚«ãƒ«ç‰ˆã®ã‚³ãƒ¼ãƒ‰ã‚’ãã®ã¾ã¾æ¡ç”¨
                    try:
                        edges_df = df[
                            (df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].astype(str) != "") &
                            (df['C_URL'].astype(str) != "")
                        ][['D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL',
                           'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL']].copy()

                        def in_site(u: str) -> bool:
                            try:
                                d = urlparse(u).netloc.lower()
                                if d.startswith("www."): d = d[4:]
                                return (not site_domain) or (d == site_domain) or d.endswith("." + site_domain)
                            except Exception:
                                return True
                        
                        edges_df = edges_df[edges_df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].apply(in_site) & edges_df['C_URL'].apply(in_site)]
                        
                        if edges_df.empty:
                            st.warning("âš ï¸ æç”»å¯¾è±¡ã‚¨ãƒƒã‚¸ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                            return

                        in_counts = edges_df.groupby('C_URL').size().sort_values(ascending=False)

                        TOP_N = network_top_n  # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¨­å®šå€¤ã‚’ä½¿ç”¨
                        top_targets = set(in_counts.head(TOP_N).index)

                        sub = edges_df[edges_df['C_URL'].isin(top_targets)].copy()
                        agg = sub.groupby(['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL', 'C_URL']).size().reset_index(name='weight')

                        url2title = {}
                        for _, r in df[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«','C_URL']].drop_duplicates().iterrows():
                            if r['C_URL']:
                                url2title[r['C_URL']] = r['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«']
                        for _, r in df[['D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«','E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL']].drop_duplicates().iterrows():
                            if r['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL']:
                                url2title.setdefault(r['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'], r['D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«'])

                        def short_label(u: str, n=24) -> str:
                            t = str(url2title.get(u, u))
                            return (t[:n] + "â€¦") if len(t) > n else t

                        net = Network(height="800px", width="100%", directed=True, bgcolor="#ffffff")
                        options = {
                            "physics": {
                                "enabled": True,
                                "solver": "barnesHut",
                                "barnesHut": {
                                    "gravitationalConstant": -8000,
                                    "centralGravity": 0.3,
                                    "springLength": 160,
                                    "springConstant": 0.03,
                                    "damping": 0.12,
                                    "avoidOverlap": 0.1
                                },
                                "stabilization": {"enabled": True, "iterations": 120, "updateInterval": 25},
                                "minVelocity": 1,
                                "timestep": 0.6
                            },
                            "interaction": {
                                "hover": True, "zoomView": True, "dragView": True,
                                "dragNodes": True, "navigationButtons": True, "keyboard": True
                            },
                            "nodes": {"shape": "dot", "font": {"size": 12}},
                            "edges": {"smooth": {"enabled": True, "type": "dynamic"},
                                      "scaling": {"min": 1, "max": 8}}
                        }
                        net.set_options(json.dumps(options))

                        nodes = set(agg['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL']).union(set(agg['C_URL']))
                        
                        def node_size(u: str) -> int:
                            s = int(in_counts.get(u, 0))
                            return max(12, min(48, int(12 + math.log2(s + 1) * 8)))

                        for u in nodes:
                            net.add_node(
                                u,
                                label=short_label(u),
                                title=f"{url2title.get(u, u)}<br>{u}",
                                size=node_size(u)
                            )

                        for _, r in agg.iterrows():
                            src = r['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL']
                            dst = r['C_URL']
                            w   = int(r['weight'])
                            net.add_edge(src, dst, value=w, arrows="to")

                        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦ã‹ã‚‰Streamlitã§è¡¨ç¤º
                        with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False, encoding='utf-8') as tmp_file:
                            net.write_html(tmp_file.name, open_browser=False)
                            
                            with open(tmp_file.name, 'r', encoding='utf-8') as f:
                                html_content = f.read()
                            
                            # Streamlitç”¨ã«HTMLã‚’èª¿æ•´
                            html_content = html_content.replace(
                                '<div id="mynetworkid"',
                                '<div id="mynetworkid" style="border: 1px solid #ddd;"'
                            )
                            
                            # Streamlitã§è¡¨ç¤º
                            st.components.v1.html(html_content, height=850)
                            
                            # çµ±è¨ˆè¡¨ç¤º
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("ãƒãƒ¼ãƒ‰æ•°", len(nodes))
                            with col2:
                                st.metric("ã‚¨ãƒƒã‚¸æ•°", len(agg))
                            with col3:
                                st.metric("ä¸Šä½ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ•°", len(top_targets))
                            
                            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯
                            filename = f"interactive_network_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
                            download_link = create_download_link(html_content, filename, "ğŸ“¥ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
                            st.markdown(download_link, unsafe_allow_html=True)
                            
                            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                            os.unlink(tmp_file.name)

                    except Exception as e:
                        st.error(f"âŒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                        st.write("ã‚¨ãƒ©ãƒ¼è©³ç´°:", str(e))
        
        # Tab 5: ç·åˆãƒ¬ãƒãƒ¼ãƒˆ
        with tab5:
            st.header("ğŸ“Š ç·åˆãƒ¬ãƒãƒ¼ãƒˆ")
            st.write("å…¨ã¦ã®åˆ†æçµæœã‚’ã¾ã¨ã‚ãŸãƒ¬ãƒãƒ¼ãƒˆã§ã™ã€‚")
            
            # ã‚µãƒãƒªãƒ¼çµ±è¨ˆ
            st.subheader("ğŸ“ˆ ã‚µã‚¤ãƒˆçµ±è¨ˆã‚µãƒãƒªãƒ¼")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("### ğŸ¯ åŸºæœ¬çµ±è¨ˆ")
                basic_stats = {
                    "ç·ãƒšãƒ¼ã‚¸æ•°": len(pages_df),
                    "ç·å†…éƒ¨ãƒªãƒ³ã‚¯æ•°": has_source.sum(),
                    "å­¤ç«‹ãƒšãƒ¼ã‚¸æ•°": len(pages_df[pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'] == 0]),
                    "å¹³å‡è¢«ãƒªãƒ³ã‚¯æ•°": f"{pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'].mean():.2f}",
                    "æœ€å¤§è¢«ãƒªãƒ³ã‚¯æ•°": int(pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'].max()),
                    "ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¢ãƒ³ã‚«ãƒ¼æ•°": unique_anchors
                }
                
                for key, value in basic_stats.items():
                    st.metric(key, value)
            
            with col2:
                st.markdown("### ğŸ“Š è¢«ãƒªãƒ³ã‚¯æ•°åˆ†å¸ƒ")
                # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
                fig = px.histogram(
                    pages_df,
                    x='è¢«ãƒªãƒ³ã‚¯æ•°',
                    nbins=20,
                    title="è¢«ãƒªãƒ³ã‚¯æ•°ã®åˆ†å¸ƒ"
                )
                fig.update_layout(height=400)
                st.plotly_chart(fig, use_container_width=True)
            
            # ä¸Šä½/ä¸‹ä½ãƒšãƒ¼ã‚¸
            st.subheader("ğŸ† ãƒˆãƒƒãƒ—ï¼†ãƒœãƒˆãƒ ")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("#### ğŸ¥‡ è¢«ãƒªãƒ³ã‚¯æ•° TOP10")
                top10 = pages_df.head(10)[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'è¢«ãƒªãƒ³ã‚¯æ•°']]
                top10.index = range(1, 11)
                st.dataframe(top10, use_container_width=True)
            
            with col2:
                st.markdown("#### ğŸ¥‰ è¢«ãƒªãƒ³ã‚¯æ•° BOTTOM10")
                bottom10 = pages_df[pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'] > 0].tail(10)[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'è¢«ãƒªãƒ³ã‚¯æ•°']]
                bottom10.index = range(1, len(bottom10) + 1)
                st.dataframe(bottom10, use_container_width=True)
            
            # ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
            if anchor_counts:
                st.subheader("ğŸ·ï¸ ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("#### ğŸ” é »å‡ºã‚¢ãƒ³ã‚«ãƒ¼ TOP10")
                    top_anchors = pd.DataFrame(
                        anchor_counts.most_common(10),
                        columns=['ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ', 'é »åº¦']
                    )
                    top_anchors.index = range(1, len(top_anchors) + 1)
                    st.dataframe(top_anchors, use_container_width=True)
                
                with col2:
                    st.markdown("#### ğŸ“Š ã‚¢ãƒ³ã‚«ãƒ¼å¤šæ§˜æ€§")
                    total_anchors = sum(anchor_counts.values())
                    hhi = sum((count/total_anchors)**2 for count in anchor_counts.values())
                    diversity_index = 1 - hhi
                    
                    st.metric("å¤šæ§˜æ€§æŒ‡æ•°", f"{diversity_index:.3f}")
                    st.metric("é›†ä¸­åº¦ï¼ˆHHIï¼‰", f"{hhi:.3f}")
                    
                    # è§£é‡ˆ
                    if diversity_index > 0.8:
                        interpretation = "ğŸŸ¢ éå¸¸ã«å¤šæ§˜"
                    elif diversity_index > 0.6:
                        interpretation = "ğŸŸ¡ ã‚„ã‚„å¤šæ§˜"
                    else:
                        interpretation = "ğŸ”´ é›†ä¸­ã—ã¦ã„ã‚‹"
                    
                    st.write(f"**è§£é‡ˆ:** {interpretation}")
            
            # å•é¡Œã®æ¤œå‡º
            st.subheader("âš ï¸ å•é¡Œæ¤œå‡º")
            
            issues = []
            
            # å­¤ç«‹ãƒšãƒ¼ã‚¸ãŒå¤šã„
            isolated_count = len(pages_df[pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'] == 0])
            isolated_ratio = isolated_count / len(pages_df)
            if isolated_ratio > 0.3:
                issues.append(f"ğŸï¸ å­¤ç«‹ãƒšãƒ¼ã‚¸ãŒå¤šã™ãã¾ã™ï¼ˆ{isolated_count}ä»¶, {isolated_ratio:.1%}ï¼‰")
            
            # è¢«ãƒªãƒ³ã‚¯ãŒæ¥µç«¯ã«åã£ã¦ã„ã‚‹
            top1_ratio = pages_df.iloc[0]['è¢«ãƒªãƒ³ã‚¯æ•°'] / pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'].sum() if pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'].sum() > 0 else 0
            if top1_ratio > 0.5:
                issues.append(f"ğŸ¯ è¢«ãƒªãƒ³ã‚¯ãŒ1ãƒšãƒ¼ã‚¸ã«é›†ä¸­ã—ã™ãã¦ã„ã¾ã™ï¼ˆ{top1_ratio:.1%}ï¼‰")
            
            # ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã®å¤šæ§˜æ€§ãŒä½ã„
            if anchor_counts and diversity_index < 0.3:
                issues.append(f"ğŸ·ï¸ ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã®å¤šæ§˜æ€§ãŒä½ã„ã§ã™ï¼ˆ{diversity_index:.3f}ï¼‰")
            
            if issues:
                for issue in issues:
                    st.warning(issue)
            else:
                st.success("ğŸ‰ ç‰¹ã«å¤§ããªå•é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼")
            
            # æ¨å¥¨äº‹é …
            st.subheader("ğŸ’¡ æ¨å¥¨äº‹é …")
            
            recommendations = []
            
            if isolated_count > 0:
                recommendations.append("ğŸ”— å­¤ç«‹ãƒšãƒ¼ã‚¸ã«å†…éƒ¨ãƒªãƒ³ã‚¯ã‚’è¿½åŠ ã—ã¦ãã ã•ã„")
            
            if top1_ratio > 0.3:
                recommendations.append("âš–ï¸ å†…éƒ¨ãƒªãƒ³ã‚¯ã‚’ã‚ˆã‚Šå‡ç­‰ã«åˆ†æ•£ã•ã›ã¦ãã ã•ã„")
            
            if anchor_counts and diversity_index < 0.5:
                recommendations.append("ğŸ·ï¸ ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å¢—ã‚„ã—ã¦ãã ã•ã„")
            
            if len(pages_df) > 100 and pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'].mean() < 2:
                recommendations.append("ğŸ“ˆ å…¨ä½“çš„ãªå†…éƒ¨ãƒªãƒ³ã‚¯å¯†åº¦ã‚’é«˜ã‚ã¦ãã ã•ã„")
            
            if recommendations:
                for rec in recommendations:
                    st.info(rec)
            else:
                st.success("âœ… ç¾åœ¨ã®å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ ã¯è‰¯å¥½ã§ã™ï¼")
            
            # å…¨ä½“ãƒ¬ãƒãƒ¼ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            if st.button("ğŸ“¥ ç·åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", key="download_summary"):
                # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã§å…¨ãƒ¬ãƒãƒ¼ãƒˆã‚’ã¾ã¨ã‚ã‚‹
                zip_buffer = BytesIO()
                
                with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                    # ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ãƒ¬ãƒãƒ¼ãƒˆ
                    pillar_rows = [[i, safe_str(row['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«']), safe_str(row['C_URL']), int(row['è¢«ãƒªãƒ³ã‚¯æ•°'])]
                                  for i, (_, row) in enumerate(pages_df.iterrows(), 1)]
                    pillar_html = generate_html_table(
                        f"{site_name} ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸åˆ†æãƒ¬ãƒãƒ¼ãƒˆ",
                        ["#", "ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«", "URL", "è¢«ãƒªãƒ³ã‚¯æ•°"],
                        pillar_rows
                    )
                    zip_file.writestr("pillar_report.html", pillar_html.encode('utf-8'))
                    
                    # ã‚¢ãƒ³ã‚«ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
                    if anchor_counts:
                        anchor_rows = [[i, anchor, count] for i, (anchor, count) in enumerate(anchor_counts.most_common(), 1)]
                        anchor_html = generate_html_table(
                            f"{site_name} ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ",
                            ["#", "ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ", "é »åº¦"],
                            anchor_rows
                        )
                        zip_file.writestr("anchor_report.html", anchor_html.encode('utf-8'))
                    
                    # å­¤ç«‹è¨˜äº‹ãƒ¬ãƒãƒ¼ãƒˆ
                    if isolated_count > 0:
                        isolated_pages = pages_df[pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'] == 0]
                        isolated_rows = [[i, safe_str(row['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«']), safe_str(row['C_URL'])]
                                        for i, (_, row) in enumerate(isolated_pages.iterrows(), 1)]
                        isolated_html = generate_html_table(
                            f"{site_name} å­¤ç«‹è¨˜äº‹åˆ†æãƒ¬ãƒãƒ¼ãƒˆ",
                            ["#", "ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«", "URL"],
                            isolated_rows
                        )
                        zip_file.writestr("isolated_report.html", isolated_html.encode('utf-8'))
                    
                    # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
                    summary_content = f"""
                    <!doctype html>
                    <meta charset='utf-8'>
                    <title>{site_name} ç·åˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ</title>
                    <style>
                        body {{ font-family: Arial, 'Yu Gothic', Meiryo, sans-serif; padding: 20px; }}
                        .header {{ text-align: center; margin-bottom: 2rem; }}
                        .section {{ margin: 2rem 0; padding: 1rem; border-left: 4px solid #1f77b4; background: #f8f9fa; }}
                        .metric {{ display: inline-block; margin: 0.5rem 1rem; padding: 0.5rem; background: white; border-radius: 4px; }}
                        .issue {{ color: #dc3545; margin: 0.5rem 0; }}
                        .recommendation {{ color: #28a745; margin: 0.5rem 0; }}
                    </style>
                    <div class='header'>
                        <h1>{site_name} å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æ ç·åˆãƒ¬ãƒãƒ¼ãƒˆ</h1>
                        <p>ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}</p>
                    </div>
                    
                    <div class='section'>
                        <h2>ğŸ“Š åŸºæœ¬çµ±è¨ˆ</h2>
                        <div class='metric'>ç·ãƒšãƒ¼ã‚¸æ•°: {len(pages_df)}</div>
                        <div class='metric'>ç·å†…éƒ¨ãƒªãƒ³ã‚¯æ•°: {has_source.sum()}</div>
                        <div class='metric'>å­¤ç«‹ãƒšãƒ¼ã‚¸æ•°: {isolated_count}</div>
                        <div class='metric'>å¹³å‡è¢«ãƒªãƒ³ã‚¯æ•°: {pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'].mean():.2f}</div>
                        <div class='metric'>æœ€å¤§è¢«ãƒªãƒ³ã‚¯æ•°: {int(pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'].max())}</div>
                        <div class='metric'>ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¢ãƒ³ã‚«ãƒ¼æ•°: {unique_anchors}</div>
                    </div>
                    
                    <div class='section'>
                        <h2>âš ï¸ æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ</h2>
                        {('<br>'.join([f'<div class="issue">{issue}</div>' for issue in issues]) if issues else '<p>å•é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚</p>')}
                    </div>
                    
                    <div class='section'>
                        <h2>ğŸ’¡ æ¨å¥¨äº‹é …</h2>
                        {('<br>'.join([f'<div class="recommendation">{rec}</div>' for rec in recommendations]) if recommendations else '<p>ç¾åœ¨ã®æ§‹é€ ã¯è‰¯å¥½ã§ã™ã€‚</p>')}
                    </div>
                    """
                    zip_file.writestr("summary_report.html", summary_content.encode('utf-8'))
                
                zip_buffer.seek(0)
                
                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ä½œæˆ
                b64 = base64.b64encode(zip_buffer.getvalue()).decode()
                filename = f"link_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
                href = f'<a href="data:application/zip;base64,{b64}" download="{filename}" style="text-decoration: none; background-color: #1f77b4; color: white; padding: 0.75rem 1.5rem; border-radius: 0.25rem; display: inline-block; font-weight: bold;">ğŸ“¦ ç·åˆãƒ¬ãƒãƒ¼ãƒˆï¼ˆZIPï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰</a>'
                st.markdown(href, unsafe_allow_html=True)
    
    except Exception as e:
        st.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.write("ã‚¨ãƒ©ãƒ¼ã®è©³ç´°:")
        st.exception(e)

if __name__ == "__main__":
    main()

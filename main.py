import streamlit as st
import pandas as pd
import plotly.express as px
import importlib.util
import time
from io import StringIO
from urllib.parse import urlparse, urlunparse
from collections import Counter
import numpy as np
import os

st.set_page_config(page_title="ğŸ”— å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ«", layout="wide")
st.markdown("""<style>.main-header { font-size: 2.5rem; font-weight: bold; text-align: center; margin-bottom: 2rem; color: #1f77b4; }</style>""", unsafe_allow_html=True)

@st.cache_data
def normalize_url_for_display(u, base_domain=None):
    if not isinstance(u, str) or not u.strip(): return ""
    try:
        p = urlparse(u)
        if not p.netloc and base_domain:
            u = f"https://{base_domain}{'/' if not u.startswith('/') else ''}{u}"
            p = urlparse(u)
        scheme = p.scheme or 'https'
        netloc = p.netloc.lower().replace("www.", "")
        path = p.path or "/"
        return urlunparse((scheme, netloc, path, p.params, p.query, ""))
    except Exception: return u

def detect_site_info(filename, df):
    filename = filename.lower()
    site_name_map = {
        'auto_answer': "Answerç¾é‡‘åŒ–", 'auto_arigataya': "ã‚ã‚ŠãŒãŸã‚„", 'auto_bicgift': "ãƒ“ãƒƒã‚¯ã‚®ãƒ•ãƒˆ",
        'auto_crecaeru': "ã‚¯ãƒ¬ã‹ãˆã‚‹", 'auto_flashpay_famipay': "ãƒ•ã‚¡ãƒŸãƒšã‚¤", 
        'auto_flashpay_media': "ãƒ¡ãƒ‡ã‚£ã‚¢", 'auto_friendpay': "ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒšã‚¤",
        'auto_fuyouhin': "ä¸ç”¨å“å›åéšŠ", 'auto_kaitori_life': "è²·å–LIFE", 'auto_kau_ru': "ã‚«ã‚¦ãƒ¼ãƒ«",
        'auto_morepay': "MorePay", 'auto_payful': "ãƒšã‚¤ãƒ•ãƒ«", 'auto_smart': "ã‚¹ãƒãƒ¼ãƒˆãƒšã‚¤", 'auto_xgift': "XGIFT"
    }
    site_name = "Unknown Site"
    for key in site_name_map:
        if key in filename:
            site_name = site_name_map[key]
            break
    domains = [urlparse(u).netloc.replace("www.","") for u in df['C_URL'].dropna() if isinstance(u, str) and 'http' in u]
    site_domain = Counter(domains).most_common(1)[0][0] if domains else None
    return site_name, site_domain

def run_analysis_loop():
    state = st.session_state.analysis_state
    site_name = state['site_name']
    
    st.info(f"ã€Œ{site_name}ã€ã®åˆ†æã‚’å®Ÿè¡Œä¸­... (ãƒ•ã‚§ãƒ¼ã‚º: {state.get('phase', 'unknown')})")
    log_placeholder = st.empty()
    progress_placeholder = st.empty()
    
    while state.get('running') and state.get('phase') != 'completed' and state.get('phase') != 'error':
        try:
            spec = importlib.util.spec_from_file_location(site_name, f"{site_name}.py")
            site_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(site_module)
            
            state = site_module.analyze_step(state)
            
            log_placeholder.code('\n'.join(state.get('log', [])), language="log")
            if 'progress' in state:
                progress_placeholder.progress(state['progress'], text=state.get('progress_text', ''))
            
            st.session_state.analysis_state = state
            time.sleep(1)
        except Exception as e:
            st.error(f"åˆ†æä¸­ã«è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.exception(e)
            state['running'] = False
            state['phase'] = 'error'
            st.session_state.analysis_state = state
            break

    if st.session_state.analysis_state.get('phase') == 'completed':
        st.session_state.analysis_state['running'] = False
        st.success("åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚çµæœã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
        
        spec = importlib.util.spec_from_file_location(site_name, f"{site_name}.py")
        site_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(site_module)
        csv_string = site_module.generate_csv(st.session_state.analysis_state)
        
        st.session_state['last_analyzed_csv_data'] = csv_string
        st.session_state['last_analyzed_filename'] = f"{site_name}_analysis.csv"
        st.rerun()

def main():
    st.markdown('<div class="main-header">ğŸ”— å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ«</div>', unsafe_allow_html=True)
    
    if 'analysis_state' not in st.session_state:
        st.session_state.analysis_state = {}

    with st.sidebar:
        st.header("ğŸ“ ãƒ‡ãƒ¼ã‚¿è¨­å®š")
        try:
            site_files = sorted([f for f in os.listdir('.') if f.startswith('auto_') and f.endswith('.py')])
            site_names = [os.path.splitext(f)[0] for f in site_files]
        except: site_files, site_names = [], []

        source_options = ["CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"]
        if site_names: source_options.insert(0, "ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§æ–°è¦åˆ†æã‚’å®Ÿè¡Œ")
        
        analysis_source = st.radio("ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’é¸æŠ", source_options, key="analysis_source", on_change=lambda: st.session_state.pop('analysis_state', None))
        
        uploaded_file = None
        if analysis_source == "ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§æ–°è¦åˆ†æã‚’å®Ÿè¡Œ" and site_names:
            selected_site_name = st.selectbox("ã‚µã‚¤ãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„", options=site_names)
            col1, col2 = st.columns(2)
            with col1:
                if st.button("ğŸš€ åˆ†æé–‹å§‹/å†é–‹"):
                    if st.session_state.analysis_state.get('site_name') != selected_site_name:
                        st.session_state.analysis_state = {'site_name': selected_site_name, 'phase': 'initializing'}
                    st.session_state.analysis_state['running'] = True
                    st.rerun()
            with col2:
                if st.button("â¹ï¸ ä¸­æ–­/ãƒªã‚»ãƒƒãƒˆ"):
                    st.session_state.analysis_state = {}
                    st.rerun()
        else:
            uploaded_file = st.file_uploader("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['csv'])

    if st.session_state.analysis_state.get('running'):
        run_analysis_loop()
        return

    data_source = None
    filename_for_detect = "analysis"
    if uploaded_file:
        data_source = uploaded_file
        filename_for_detect = uploaded_file.name
    elif 'last_analyzed_csv_data' in st.session_state:
        data_source = StringIO(st.session_state['last_analyzed_csv_data'])
        filename_for_detect = st.session_state['last_analyzed_filename']
    
    if data_source is None:
        st.info("ğŸ‘† ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’é¸æŠã™ã‚‹ã‹ã€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return

    try:
        # â˜…â˜…â˜… ã“ã“ãŒæœ€é‡è¦ä¿®æ­£ç‚¹ â˜…â˜…â˜…
        df = pd.read_csv(data_source, encoding="utf-8-sig", header=0).fillna("")
        if df.empty:
            st.warning("åˆ†æçµæœãŒ0ä»¶ã§ã—ãŸã€‚ã‚¯ãƒ­ãƒ¼ãƒ«ãŒæ­£å¸¸ã«è¡Œã‚ã‚Œãªã‹ã£ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            return

        df.columns = ['A_ç•ªå·', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL', 'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL', 'F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ']
        df['A_ç•ªå·'] = df.groupby('C_URL')['A_ç•ªå·'].transform('first')
        df['A_ç•ªå·'] = pd.to_numeric(df['A_ç•ªå·'], errors='coerce').fillna(0).astype(int)
        
        site_name, site_domain = detect_site_info(filename_for_detect, df)
        df['C_URL'] = df['C_URL'].apply(lambda u: normalize_url_for_display(u, base_domain=site_domain))
        df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'] = df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].apply(lambda u: normalize_url_for_display(u, base_domain=site_domain))
        
        # --- ã“ã“ã‹ã‚‰ä¸»ã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ©Ÿèƒ½ ---
        st.markdown(f"### {site_name} ã®åˆ†æçµæœ")
        
        tab1, tab2, tab3 = st.tabs(["ğŸ“Š ãƒ‡ãƒ¼ã‚¿ä¸€è¦§", "ğŸ›ï¸ ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸", "ğŸ§© ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ"])
        
        with tab1:
            st.dataframe(df)

        pages_df = df[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL']].drop_duplicates().copy()
        inbound_counts = df[df['D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«'] != ''].groupby('C_URL').size()
        pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'] = pages_df['C_URL'].map(inbound_counts).fillna(0).astype(int)
        pages_df = pages_df.sort_values('è¢«ãƒªãƒ³ã‚¯æ•°', ascending=False).reset_index(drop=True)

        with tab2:
            st.header("ğŸ›ï¸ ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸åˆ†æ")
            st.dataframe(pages_df[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL', 'è¢«ãƒªãƒ³ã‚¯æ•°']], use_container_width=True)
            fig = px.bar(pages_df.head(20).sort_values('è¢«ãƒªãƒ³ã‚¯æ•°'), x='è¢«ãƒªãƒ³ã‚¯æ•°', y='B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', orientation='h', title="è¢«ãƒªãƒ³ã‚¯æ•° TOP20")
            st.plotly_chart(fig, use_container_width=True)

        with tab3:
            st.header("ğŸ§© ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æï¼ˆã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆï¼‰")
            anchor_counts = Counter(df['F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'].replace('', np.nan).dropna())
            if anchor_counts:
                st.dataframe(pd.DataFrame(anchor_counts.most_common(), columns=['ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ', 'é »åº¦']), use_container_width=True)
            else:
                st.warning("ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    except Exception as e:
        st.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.exception(e)

if __name__ == "__main__":
    main()

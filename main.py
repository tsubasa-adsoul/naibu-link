#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆStreamlitç‰ˆï¼‰
- CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
- ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸åˆ†æ
- ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ï¼ˆã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆï¼‰åˆ†æ
- å­¤ç«‹è¨˜äº‹åˆ†æ
- ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ï¼ˆPyVisï¼‰
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from collections import Counter
from urllib.parse import urlparse
import tempfile
import os
from pathlib import Path
import logging

# PyVisï¼ˆä»»æ„ï¼‰
try:
    from pyvis.network import Network
    HAS_PYVIS = True
except ImportError:
    HAS_PYVIS = False
    st.error("PyVisãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install pyvis`ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ«",
    page_icon="ğŸ”—",
    layout="wide"
)

EXPECTED_COLUMNS = [
    'A_ç•ªå·',
    'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 
    'C_URL',
    'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«',
    'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL',
    'F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'
]

def safe_str(s):
    return s if isinstance(s, str) else ""

def normalize_url(u, default_scheme="https", base_domain=None):
    if not isinstance(u, str) or not u.strip():
        return ""
    u = u.strip()
    from urllib.parse import urlparse, urlunparse
    p = urlparse(u)

    if not p.netloc and base_domain:
        path = u if u.startswith("/") else f"/{u}"
        u = f"{default_scheme}://{base_domain}{path}"
        p = urlparse(u)
    elif not p.netloc:
        return u

    scheme = p.scheme or default_scheme
    netloc = p.netloc.lower()
    if netloc.startswith("www."): 
        netloc = netloc[4:]
    path = p.path or "/"
    return urlunparse((scheme, netloc, path, p.params, p.query, ""))

def detect_site_info(filename, df):
    filename = filename.lower() if filename else ""
    
    if 'kau-ru' in filename or 'kauru' in filename:
        site_name = "ã‚«ã‚¦ãƒ¼ãƒ«"
    elif 'kaitori-life' in filename:
        site_name = "è²·å–LIFE"  
    elif 'friendpay' in filename:
        site_name = "ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒšã‚¤"
    elif 'kurekaeru' in filename or 'crecaeru' in filename:
        site_name = "ã‚¯ãƒ¬ã‹ãˆã‚‹"
    elif 'bicgift' in filename or 'bic-gift' in filename:
        site_name = "ãƒ“ãƒƒã‚¯ã‚®ãƒ•ãƒˆ"
    else:
        site_name = "Unknown Site"

    # ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ç‰¹å®š
    domains = []
    for u in df['C_URL'].tolist():
        if isinstance(u, str) and u:
            d = urlparse(u).netloc.lower()
            if d.startswith("www."): 
                d = d[4:]
            if d: 
                domains.append(d)
    
    site_domain = Counter(domains).most_common(1)[0][0] if domains else None
    return site_name, site_domain

def create_network_visualization(df, site_name, top_n=40):
    """PyVisã‚’ä½¿ç”¨ã—ã¦ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã‚’ä½œæˆ"""
    if not HAS_PYVIS:
        st.error("PyVisãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return None
    
    try:
        # ã‚¨ãƒƒã‚¸ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        edges_df = df[
            (df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].astype(str) != "") &
            (df['C_URL'].astype(str) != "")
        ][['D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL',
           'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL']].copy()

        if edges_df.empty:
            st.warning("æç”»å¯¾è±¡ã®ã‚¨ãƒƒã‚¸ãŒã‚ã‚Šã¾ã›ã‚“")
            return None

        # è¢«ãƒªãƒ³ã‚¯æ•°ã‚’è¨ˆç®—
        in_counts = edges_df.groupby('C_URL').size().sort_values(ascending=False)
        
        # ä¸Šä½Nä»¶ã‚’é¸æŠ
        top_targets = set(in_counts.head(top_n).index)
        sub = edges_df[edges_df['C_URL'].isin(top_targets)].copy()
        
        # ã‚¨ãƒƒã‚¸ã‚’é›†ç´„
        agg = sub.groupby(['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL', 'C_URL']).size().reset_index(name='weight')
        
        # URLã¨ã‚¿ã‚¤ãƒˆãƒ«ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
        url2title = {}
        for _, r in df[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«','C_URL']].drop_duplicates().iterrows():
            if r['C_URL']:
                url2title[r['C_URL']] = r['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«']
        for _, r in df[['D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«','E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL']].drop_duplicates().iterrows():
            if r['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL']:
                url2title.setdefault(r['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'], r['D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«'])

        def short_label(u: str, n=20) -> str:
            t = str(url2title.get(u, u))
            return (t[:n] + "...") if len(t) > n else t

        # PyVisãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½œæˆ
        net = Network(
            height="800px", 
            width="100%", 
            directed=True, 
            bgcolor="#ffffff"
        )
        
        # ç‰©ç†è¨­å®š
        net.barnes_hut(
            gravity=-8000,
            central_gravity=0.3,
            spring_length=200,
            spring_strength=0.05,
            damping=0.15
        )

        # ãƒãƒ¼ãƒ‰ã¨ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
        nodes = set(agg['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL']).union(set(agg['C_URL']))
        
        for u in nodes:
            size = max(15, min(50, int(15 + (in_counts.get(u, 0) * 2))))
            net.add_node(
                u,
                label=short_label(u),
                title=f"{url2title.get(u, u)}<br>{u}",
                size=size
            )

        for _, r in agg.iterrows():
            net.add_edge(
                r['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'], 
                r['C_URL'], 
                width=min(int(r['weight']), 10)
            )

        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False) as tmp:
            net.save_graph(tmp.name)
            with open(tmp.name, 'r', encoding='utf-8') as f:
                html_content = f.read()
            os.unlink(tmp.name)
            
        return html_content
        
    except Exception as e:
        st.error(f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

def main():
    st.title("ğŸ”— å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ«")
    st.markdown("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ ã‚’åˆ†æã—ã¾ã™ã€‚")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    with st.sidebar:
        st.header("ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š")
        uploaded_file = st.file_uploader("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['csv'])
        
        if uploaded_file:
            st.success(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {uploaded_file.name}")
            
        st.header("ğŸ› ï¸ è¡¨ç¤ºè¨­å®š")
        network_top_n = st.slider("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ï¼šä¸Šä½Nä»¶", 10, 100, 40, 5)

    if uploaded_file is None:
        st.info("ğŸ‘ˆ ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
        return

    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    try:
        df = pd.read_csv(uploaded_file, encoding="utf-8-sig").fillna("")
        
        # åˆ—ã®ç¢ºèª
        if not all(col in df.columns for col in EXPECTED_COLUMNS):
            st.error(f"CSVåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™\nå¿…è¦: {EXPECTED_COLUMNS}\nå®Ÿéš›: {list(df.columns)}")
            return
            
        # ã‚µã‚¤ãƒˆæƒ…å ±ã®æ¤œå‡º
        site_name, site_domain = detect_site_info(uploaded_file.name, df)
        
        # URLæ­£è¦åŒ–
        def _norm(u): 
            return normalize_url(u, base_domain=site_domain)
        df['C_URL'] = df['C_URL'].apply(_norm)
        df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'] = df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].apply(_norm)
        
        st.success(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {site_name} ({len(df)}è¡Œ)")
        
    except Exception as e:
        st.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return

    # ã‚¿ãƒ–ã§æ©Ÿèƒ½ã‚’åˆ†å‰²
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ“Š ãƒ‡ãƒ¼ã‚¿ä¸€è¦§", 
        "ğŸ›ï¸ ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸", 
        "ğŸ§© ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ", 
        "ğŸ§­ å­¤ç«‹è¨˜äº‹",
        "ğŸ“ˆ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³"
    ])

    # ãƒ‡ãƒ¼ã‚¿é›†è¨ˆ
    pages_df = df[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL']].drop_duplicates().copy()
    has_src = (df['D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«'].astype(str) != "") & (df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].astype(str) != "")
    inbound_df = df[has_src & (df['C_URL'].astype(str) != "")]
    inbound_counts = inbound_df.groupby('C_URL').size()
    pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'] = pages_df['C_URL'].map(inbound_counts).fillna(0).astype(int)
    pages_df = pages_df.sort_values('è¢«ãƒªãƒ³ã‚¯æ•°', ascending=False).reset_index(drop=True)

    with tab1:
        st.header("ğŸ“Š å…¨ãƒ‡ãƒ¼ã‚¿")
        st.dataframe(df, use_container_width=True)

    with tab2:
        st.header("ğŸ›ï¸ ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸åˆ†æ")
        st.dataframe(pages_df, use_container_width=True)
        
        # ä¸Šä½20ä»¶ã®æ£’ã‚°ãƒ©ãƒ•
        if len(pages_df) > 0:
            top20 = pages_df.head(20)
            fig = px.bar(
                top20.sort_values('è¢«ãƒªãƒ³ã‚¯æ•°'), 
                x='è¢«ãƒªãƒ³ã‚¯æ•°', 
                y='B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«',
                orientation='h',
                title="è¢«ãƒªãƒ³ã‚¯æ•° TOP20",
                height=600
            )
            fig.update_layout(yaxis={'categoryorder': 'total ascending'})
            st.plotly_chart(fig, use_container_width=True)

    with tab3:
        st.header("ğŸ§© ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æï¼ˆã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆï¼‰")
        
        anchors = df[df['F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'].astype(str) != '']
        if not anchors.empty:
            anchor_counts = Counter(anchors['F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'])
            anchor_df = pd.DataFrame(anchor_counts.most_common(), columns=['ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ', 'é »åº¦'])
            
            # å¤šæ§˜æ€§æŒ‡æ¨™ã®è¨ˆç®—
            total = sum(anchor_counts.values())
            hhi = sum((c/total)**2 for c in anchor_counts.values()) if total else 0.0
            diversity = 1 - hhi
            
            st.metric("ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆå¤šæ§˜æ€§", f"{diversity:.3f}", help="1ã«è¿‘ã„ã»ã©å¤šæ§˜")
            st.dataframe(anchor_df, use_container_width=True)
        else:
            st.warning("ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    with tab4:
        st.header("ğŸ§­ å­¤ç«‹è¨˜äº‹")
        
        isolated = pages_df[pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'] == 0].copy()
        if not isolated.empty:
            st.warning(f"å­¤ç«‹è¨˜äº‹ãŒ {len(isolated)} ä»¶è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
            st.dataframe(isolated[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL']], use_container_width=True)
        else:
            st.success("å­¤ç«‹è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“")

    with tab5:
        st.header("ğŸ“ˆ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³")
        
        if HAS_PYVIS:
            with st.spinner("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã‚’ç”Ÿæˆä¸­..."):
                html_content = create_network_visualization(df, site_name, network_top_n)
                
                if html_content:
                    st.components.v1.html(html_content, height=820, scrolling=False)
                    st.success(f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã‚’è¡¨ç¤ºã—ã¾ã—ãŸï¼ˆä¸Šä½{network_top_n}ä»¶ï¼‰")
                else:
                    st.error("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        else:
            st.error("PyVisãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")

if __name__ == "__main__":
    main()

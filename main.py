import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import time
import re
import csv
import io
from datetime import datetime
import pandas as pd

# Answerç¾é‡‘åŒ–åˆ†æã‚¯ãƒ©ã‚¹
class AnswerAnalyzer:
    def __init__(self):
        self.pages = {}
        self.links = []
        self.detailed_links = []

    def normalize_url(self, url):
        if not url: return ""
        try:
            parsed = urlparse(url)
            return f"https://{parsed.netloc.replace('www.', '')}{parsed.path.rstrip('/')}"
        except:
            return url

    def is_article_page(self, url):
        path = urlparse(self.normalize_url(url)).path.lower()
        exclude_words = ['/site/', '/blog/', '/page/', '/feed', '/wp-', '.']
        if any(word in path for word in exclude_words):
            return False
        
        if path.startswith('/') and len(path) > 1:
            clean_path = path[1:].rstrip('/')
            if clean_path and '/' not in clean_path:
                return True
        return False

    def is_crawlable(self, url):
        path = urlparse(self.normalize_url(url)).path.lower()
        exclude_words = ['/site/', '/wp-admin', '/feed', '.jpg', '.png', '.css', '.js']
        if any(word in path for word in exclude_words):
            return False
        return (path.startswith('/blog') or self.is_article_page(url))

    def extract_links(self, soup, current_url):
        links = []
        for a in soup.find_all('a', href=True):
            href = a.get('href', '').strip()
            if href and not href.startswith('#'):
                absolute = urljoin(current_url, href)
                if 'answer-genkinka.jp' in absolute and self.is_crawlable(absolute):
                    links.append(self.normalize_url(absolute))
        return list(set(links))

    def extract_content_links(self, soup, current_url):
        content = soup.select_one('.entry-content, .post-content, main, article')
        if not content:
            return []
        
        links = []
        for a in content.find_all('a', href=True):
            href = a.get('href', '').strip()
            text = a.get_text(strip=True) or ''
            if href and not href.startswith('#') and text:
                absolute = urljoin(current_url, href)
                if 'answer-genkinka.jp' in absolute and self.is_article_page(absolute):
                    links.append({'url': self.normalize_url(absolute), 'anchor_text': text[:100]})
        return links

    def analyze(self, start_url, progress_callback=None):
        self.pages, self.links, self.detailed_links = {}, [], []
        visited, to_visit = set(), [self.normalize_url(start_url)]
        to_visit.append('https://answer-genkinka.jp/blog/page/2/')

        session = requests.Session()
        session.headers.update({'User-Agent': 'Mozilla/5.0'})

        # ãƒ•ã‚§ãƒ¼ã‚º1: ãƒšãƒ¼ã‚¸åé›†
        while to_visit and len(self.pages) < 50:
            url = to_visit.pop(0)
            if url in visited: continue
            
            try:
                if progress_callback:
                    progress_callback(f"åé›†ä¸­: {url}")
                    
                response = session.get(url, timeout=10)
                if response.status_code != 200: continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                robots = soup.find('meta', attrs={'name': 'robots'})
                if robots and 'noindex' in robots.get('content', '').lower():
                    continue
                
                if '/blog' in url:
                    page_links = self.extract_links(soup, url)
                    new_links = [l for l in page_links if l not in visited and l not in to_visit]
                    to_visit.extend(new_links)
                    visited.add(url)
                    continue
                
                if self.is_article_page(url):
                    title = soup.find('h1')
                    title = title.get_text(strip=True) if title else url
                    title = re.sub(r'\s*[|\-]\s*.*(answer-genkinka|ã‚¢ãƒ³ã‚µãƒ¼).*$', '', title, flags=re.IGNORECASE)
                    
                    self.pages[url] = {'title': title, 'outbound_links': []}
                    page_links = self.extract_links(soup, url)
                    new_links = [l for l in page_links if l not in visited and l not in to_visit]
                    to_visit.extend(new_links)
                
                visited.add(url)
                time.sleep(0.1)
                
            except Exception:
                continue

        # ãƒ•ã‚§ãƒ¼ã‚º2: ãƒªãƒ³ã‚¯é–¢ä¿‚æ§‹ç¯‰
        if progress_callback:
            progress_callback("ãƒªãƒ³ã‚¯é–¢ä¿‚æ§‹ç¯‰ä¸­...")
            
        processed = set()
        for url in list(self.pages.keys()):
            try:
                response = session.get(url, timeout=10)
                if response.status_code != 200: continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                content_links = self.extract_content_links(soup, url)
                
                for link_data in content_links:
                    target = link_data['url']
                    if target in self.pages and target != url:
                        link_key = (url, target)
                        if link_key not in processed:
                            processed.add(link_key)
                            self.links.append((url, target))
                            self.pages[url]['outbound_links'].append(target)
                            self.detailed_links.append({
                                'source_url': url, 'source_title': self.pages[url]['title'],
                                'target_url': target, 'anchor_text': link_data['anchor_text']
                            })
            except Exception:
                continue

        # è¢«ãƒªãƒ³ã‚¯æ•°è¨ˆç®—
        for url in self.pages:
            self.pages[url]['inbound_links'] = sum(1 for s, t in self.links if t == url)

        return self.pages, self.links, self.detailed_links

# ãã®ä»–ã‚µã‚¤ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼åˆ†æã‚¯ãƒ©ã‚¹ï¼ˆå¾Œã§å®Ÿè£…ï¼‰
class GenericAnalyzer:
    def __init__(self, site_name, base_url):
        self.site_name = site_name
        self.base_url = base_url

    def analyze(self, start_url, progress_callback=None):
        if progress_callback:
            progress_callback(f"{self.site_name} ã®åˆ†æã¯æº–å‚™ä¸­ã§ã™...")
        return {}, [], []

# ã‚µã‚¤ãƒˆè¨­å®š
ANALYZER_CONFIGS = {
    "answer-genkinka.jp": {
        "name": "Answerç¾é‡‘åŒ–",
        "url": "https://answer-genkinka.jp/blog/",
        "status": "active",
        "description": "ç¾é‡‘åŒ–ã‚µãƒ¼ãƒ“ã‚¹å°‚é–€ã‚µã‚¤ãƒˆ",
        "features": ["ãƒ–ãƒ­ã‚°è¨˜äº‹åˆ†æ", "å…¨è‡ªå‹•CSVå‡ºåŠ›"],
        "color": "#FF6B6B",
        "analyzer_class": AnswerAnalyzer
    },
    "arigataya.co.jp": {
        "name": "ã‚ã‚ŠãŒãŸã‚„", 
        "url": "https://arigataya.co.jp",
        "status": "planned",
        "description": "onclickå¯¾å¿œãƒ»å…¨è‡ªå‹•ç‰ˆ",
        "features": ["onclickå¯¾å¿œ", "è‡ªå‹•ãƒªãƒ³ã‚¯æ¤œå‡º"],
        "color": "#4ECDC4",
        "analyzer_class": GenericAnalyzer
    },
    "kau-ru.co.jp": {
        "name": "ã‚«ã‚¦ãƒ¼ãƒ«",
        "url": "https://kau-ru.co.jp", 
        "status": "planned",
        "description": "è¤‡æ•°ã‚µã‚¤ãƒˆå¯¾å¿œç‰ˆ",
        "features": ["WordPress API", "æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«é™¤å¤–"],
        "color": "#45B7D1",
        "analyzer_class": GenericAnalyzer
    },
    "crecaeru.co.jp": {
        "name": "ã‚¯ãƒ¬ã‹ãˆã‚‹",
        "url": "https://crecaeru.co.jp",
        "status": "planned",
        "description": "gnavé™¤å¤–å¯¾å¿œãƒ»onclickå¯¾å¿œç‰ˆ",
        "features": ["gnavé™¤å¤–", "onclickå¯¾å¿œ"],
        "color": "#96CEB4",
        "analyzer_class": GenericAnalyzer
    },
    "friendpay.jp": {
        "name": "ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒšã‚¤",
        "url": "https://friendpay.jp",
        "status": "planned",
        "description": "ã‚µã‚¤ãƒˆåˆ¥é™¤å¤–ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼å¯¾å¿œ",
        "features": ["ã‚µã‚¤ãƒˆåˆ¥é™¤å¤–", "æœ€é©åŒ–åˆ†æ"],
        "color": "#FFEAA7",
        "analyzer_class": GenericAnalyzer
    },
    "kaitori-life.co.jp": {
        "name": "è²·å–LIFE",
        "url": "https://kaitori-life.co.jp",
        "status": "planned",
        "description": "JINãƒ†ãƒ¼ãƒå°‚ç”¨æœ€é©åŒ–",
        "features": ["JINãƒ†ãƒ¼ãƒå¯¾å¿œ", "å°‚ç”¨ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼"],
        "color": "#FD79A8",
        "analyzer_class": GenericAnalyzer
    },
    "wallet-sos.jp": {
        "name": "ã‚¦ã‚©ãƒ¬ãƒƒãƒˆSOS",
        "url": "https://wallet-sos.jp",
        "status": "planned",
        "description": "Seleniumç‰ˆï¼ˆCloudflareå¯¾ç­–ï¼‰",
        "features": ["Seleniumå¯¾å¿œ", "Cloudflareå¯¾ç­–"],
        "color": "#A29BFE",
        "analyzer_class": GenericAnalyzer
    },
    "wonderwall-invest.co.jp": {
        "name": "ãƒ¯ãƒ³ãƒ€ãƒ¼ã‚¦ã‚©ãƒ¼ãƒ«",
        "url": "https://wonderwall-invest.co.jp",
        "status": "planned",
        "description": "secure-technologyå°‚ç”¨",
        "features": ["å°‚ç”¨æœ€é©åŒ–", "é«˜ç²¾åº¦åˆ†æ"],
        "color": "#6C5CE7",
        "analyzer_class": GenericAnalyzer
    },
    "fuyohin-kaishu.co.jp": {
        "name": "ä¸ç”¨å“å›å",
        "url": "https://fuyohin-kaishu.co.jp",
        "status": "planned",
        "description": "ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æãƒ»ä¿®æ­£ç‰ˆ",
        "features": ["ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ", "åŒ…æ‹¬çš„åé›†"],
        "color": "#00B894",
        "analyzer_class": GenericAnalyzer
    },
    "bic-gift.co.jp": {
        "name": "ãƒ“ãƒƒã‚¯ã‚®ãƒ•ãƒˆ",
        "url": "https://bic-gift.co.jp",
        "status": "planned",
        "description": "SANGOãƒ†ãƒ¼ãƒå°‚ç”¨ãƒ»å…¨è‡ªå‹•ç‰ˆ",
        "features": ["SANGOãƒ†ãƒ¼ãƒå¯¾å¿œ", "å°‚ç”¨æŠ½å‡º"],
        "color": "#E17055",
        "analyzer_class": GenericAnalyzer
    },
    "flashpay.jp/famipay": {
        "name": "ãƒ•ã‚¡ãƒŸãƒšã‚¤",
        "url": "https://flashpay.jp/famipay/",
        "status": "planned",
        "description": "/famipay/é…ä¸‹å°‚ç”¨",
        "features": ["é…ä¸‹é™å®šåˆ†æ", "é«˜ç²¾åº¦æŠ½å‡º"],
        "color": "#00CEC9",
        "analyzer_class": GenericAnalyzer
    },
    "flashpay.jp/media": {
        "name": "ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ãƒšã‚¤",
        "url": "https://flashpay.jp/media/",
        "status": "planned",
        "description": "/media/é…ä¸‹å°‚ç”¨",
        "features": ["ãƒ¡ãƒ‡ã‚£ã‚¢ç‰¹åŒ–", "åŠ¹ç‡åˆ†æ"],
        "color": "#74B9FF",
        "analyzer_class": GenericAnalyzer
    },
    "more-pay.jp": {
        "name": "ãƒ¢ã‚¢ãƒšã‚¤",
        "url": "https://more-pay.jp",
        "status": "planned",
        "description": "æ”¹å–„ç‰ˆãƒ»åŒ…æ‹¬çš„åˆ†æ",
        "features": ["åŒ…æ‹¬åˆ†æ", "æ”¹å–„ç‰ˆã‚¨ãƒ³ã‚¸ãƒ³"],
        "color": "#FD79A8",
        "analyzer_class": GenericAnalyzer
    },
    "pay-ful.jp": {
        "name": "ãƒšã‚¤ãƒ•ãƒ«",
        "url": "https://pay-ful.jp/media/",
        "status": "planned",
        "description": "å€‹åˆ¥è¨˜äº‹ãƒšãƒ¼ã‚¸é‡è¦–",
        "features": ["è¨˜äº‹é‡è¦–", "ç²¾å¯†åˆ†æ"],
        "color": "#FDCB6E",
        "analyzer_class": GenericAnalyzer
    },
    "smart-pay.website": {
        "name": "ã‚¹ãƒãƒ¼ãƒˆãƒšã‚¤",
        "url": "https://smart-pay.website/media/",
        "status": "planned",
        "description": "å¤§è¦æ¨¡ã‚µã‚¤ãƒˆå¯¾å¿œ",
        "features": ["å¤§è¦æ¨¡å¯¾å¿œ", "åŠ¹ç‡åŒ–ã‚¨ãƒ³ã‚¸ãƒ³"],
        "color": "#E84393",
        "analyzer_class": GenericAnalyzer
    },
    "xgift.jp": {
        "name": "ã‚¨ãƒƒã‚¯ã‚¹ã‚®ãƒ•ãƒˆ",
        "url": "https://xgift.jp/blog/",
        "status": "planned",
        "description": "AFFINGERå¯¾å¿œ",
        "features": ["AFFINGERå¯¾å¿œ", "ãƒ†ãƒ¼ãƒæœ€é©åŒ–"],
        "color": "#00B894",
        "analyzer_class": GenericAnalyzer
    }
}

def create_detailed_csv(pages, detailed_links):
    """è©³ç´°CSVãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
    csv_data = []
    csv_data.append(['ç•ªå·', 'ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'URL', 'è¢«ãƒªãƒ³ã‚¯å…ƒã‚¿ã‚¤ãƒˆãƒ«', 'è¢«ãƒªãƒ³ã‚¯å…ƒURL', 'ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'])
    
    targets = {}
    for link in detailed_links:
        target = link['target_url']
        targets.setdefault(target, []).append(link)
    
    sorted_targets = sorted(targets.items(), key=lambda x: len(x[1]), reverse=True)
    
    row = 1
    for target, links_list in sorted_targets:
        title = pages.get(target, {}).get('title', target)
        for link in links_list:
            csv_data.append([row, title, target, link['source_title'], link['source_url'], link['anchor_text']])
            row += 1
    
    for url, info in pages.items():
        if info.get('inbound_links', 0) == 0:
            csv_data.append([row, info['title'], url, '', '', ''])
            row += 1
    
    output = io.StringIO()
    writer = csv.writer(output)
    writer.writerows(csv_data)
    return output.getvalue()

def run_analysis(site_key, config):
    """åˆ†æå®Ÿè¡Œ"""
    st.info(f"{config['name']} ã®åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    
    status_placeholder = st.empty()
    progress_bar = st.progress(0)
    
    def progress_callback(message):
        status_placeholder.text(message)
    
    # åˆ†æå®Ÿè¡Œ
    if config['analyzer_class'] == AnswerAnalyzer:
        analyzer = AnswerAnalyzer()
    else:
        analyzer = GenericAnalyzer(config['name'], config['url'])
    
    pages, links, detailed_links = analyzer.analyze(config['url'], progress_callback)
    
    progress_bar.progress(1.0)
    status_placeholder.text("åˆ†æå®Œäº†!")
    
    if pages:
        # çµ±è¨ˆè¡¨ç¤º
        total = len(pages)
        isolated = sum(1 for p in pages.values() if p.get('inbound_links', 0) == 0)
        popular = sum(1 for p in pages.values() if p.get('inbound_links', 0) >= 5)
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ç·ãƒšãƒ¼ã‚¸æ•°", total)
        with col2:
            st.metric("ç·ãƒªãƒ³ã‚¯æ•°", len(links))
        with col3:
            st.metric("å­¤ç«‹ãƒšãƒ¼ã‚¸", isolated)
        with col4:
            st.metric("äººæ°—ãƒšãƒ¼ã‚¸", popular)
        
        # çµæœè¡¨ç¤º
        st.subheader(f"ğŸ“Š {config['name']} åˆ†æçµæœ")
        
        # DataFrameã«å¤‰æ›
        results_data = []
        for url, info in pages.items():
            results_data.append({
                'ã‚¿ã‚¤ãƒˆãƒ«': info['title'],
                'URL': url,
                'è¢«ãƒªãƒ³ã‚¯æ•°': info.get('inbound_links', 0),
                'ç™ºãƒªãƒ³ã‚¯æ•°': len(info.get('outbound_links', []))
            })
        
        df = pd.DataFrame(results_data)
        df_sorted = df.sort_values('è¢«ãƒªãƒ³ã‚¯æ•°', ascending=False)
        
        # ã‚°ãƒ©ãƒ•è¡¨ç¤º
        if len(df_sorted) > 0:
            st.subheader("è¢«ãƒªãƒ³ã‚¯æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆä¸Šä½10ä»¶ï¼‰")
            top_10 = df_sorted.head(10)
            if not top_10.empty:
                chart_data = top_10.set_index('ã‚¿ã‚¤ãƒˆãƒ«')['è¢«ãƒªãƒ³ã‚¯æ•°']
                st.bar_chart(chart_data)
        
        # è©³ç´°ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆå¸¸ã«è¡¨ç¤ºï¼‰
        st.subheader("è©³ç´°ãƒ‡ãƒ¼ã‚¿")
        st.dataframe(df_sorted, use_container_width=True)
        
        # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆè©³ç´°ãƒ‡ãƒ¼ã‚¿ã¯æ¶ˆãˆãªã„ï¼‰
        if detailed_links:
            csv_content = create_detailed_csv(pages, detailed_links)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{config['name']}-{timestamp}.csv"
            
            st.download_button(
                "ğŸ“¥ è©³ç´°CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                csv_content,
                filename,
                "text/csv",
                help="è¢«ãƒªãƒ³ã‚¯æ•°é †ã§ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸè©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ"
            )
        
        st.success("âœ… åˆ†æå®Œäº†!")
    
    else:
        if config['status'] == 'planned':
            st.warning(f"{config['name']} ã®åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã¯æº–å‚™ä¸­ã§ã™")
        else:
            st.error("âŒ ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

def main():
    st.set_page_config(
        page_title="å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æ çµ±æ‹¬ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ",
        page_icon="ğŸ›ï¸",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("ğŸ›ï¸ å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æ çµ±æ‹¬ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("**16ã‚µã‚¤ãƒˆå¯¾å¿œ - ä¸€å…ƒç®¡ç†ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.header("ğŸ“‹ ãƒ¡ãƒ‹ãƒ¥ãƒ¼")
        menu = st.radio(
            "æ©Ÿèƒ½ã‚’é¸æŠ",
            ["ğŸ  ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰", "ğŸ”— å€‹åˆ¥åˆ†æ", "ğŸ“Š çµ±è¨ˆãƒ»æ¯”è¼ƒ", "âš™ï¸ è¨­å®šç®¡ç†"],
            index=0
        )
        
        st.divider()
        st.markdown("**ğŸ“ˆ ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³**")
        active_count = sum(1 for config in ANALYZER_CONFIGS.values() if config['status'] == 'active')
        st.metric("ç¨¼åƒä¸­", f"{active_count}/16ã‚µã‚¤ãƒˆ")
        
        st.divider()
        st.markdown("**ğŸ• æœ€çµ‚æ›´æ–°**")
        st.text(datetime.now().strftime("%Y-%m-%d %H:%M"))
    
    # ãƒ¡ã‚¤ãƒ³ç”»é¢
    if menu == "ğŸ  ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰":
        show_dashboard()
    elif menu == "ğŸ”— å€‹åˆ¥åˆ†æ":
        show_individual_analysis()
    elif menu == "ğŸ“Š çµ±è¨ˆãƒ»æ¯”è¼ƒ":
        show_statistics()
    elif menu == "âš™ï¸ è¨­å®šç®¡ç†":
        show_settings()

def show_dashboard():
    """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¡¨ç¤º"""
    st.header("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“æ¦‚è¦")
    
    # çµ±è¨ˆæƒ…å ±
    col1, col2, col3, col4 = st.columns(4)
    
    active_sites = [k for k, v in ANALYZER_CONFIGS.items() if v['status'] == 'active']
    planned_sites = [k for k, v in ANALYZER_CONFIGS.items() if v['status'] == 'planned']
    
    with col1:
        st.metric("ç·ã‚µã‚¤ãƒˆæ•°", len(ANALYZER_CONFIGS), delta="16ã‚µã‚¤ãƒˆ")
    with col2:
        st.metric("ç¨¼åƒä¸­", len(active_sites), delta=f"+{len(active_sites)}")
    with col3:
        st.metric("æº–å‚™ä¸­", len(planned_sites), delta=f"{len(planned_sites)}ã‚µã‚¤ãƒˆ")
    with col4:
        st.metric("æœ¬æ—¥ã®åˆ†æ", 0, delta="0å›")
    
    st.divider()
    
    # ã‚µã‚¤ãƒˆä¸€è¦§ã‚«ãƒ¼ãƒ‰è¡¨ç¤º
    st.subheader("ğŸ”— åˆ†æå¯¾è±¡ã‚µã‚¤ãƒˆä¸€è¦§")
    
    # 3åˆ—ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã§ã‚«ãƒ¼ãƒ‰è¡¨ç¤º
    cols = st.columns(3)
    
    for i, (site_key, config) in enumerate(ANALYZER_CONFIGS.items()):
        col_idx = i % 3
        
        with cols[col_idx]:
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã«å¿œã˜ãŸè‰²åˆ†ã‘
            if config['status'] == 'active':
                status_color = "ğŸŸ¢"
                status_text = "ç¨¼åƒä¸­"
            elif config['status'] == 'planned':
                status_color = "ğŸŸ¡" 
                status_text = "æº–å‚™ä¸­"
            else:
                status_color = "ğŸ”´"
                status_text = "åœæ­¢ä¸­"
            
            with st.container():
                st.markdown(f"""
                <div style="
                    border: 2px solid {config['color']};
                    border-radius: 10px;
                    padding: 15px;
                    margin: 10px 0;
                    background: linear-gradient(135deg, {config['color']}15, {config['color']}05);
                ">
                    <h4 style="color: {config['color']}; margin: 0 0 10px 0;">
                        {status_color} {config['name']}
                    </h4>
                    <p style="margin: 5px 0; font-size: 0.9em;">
                        <strong>URL:</strong> {config['url'][:30]}...
                    </p>
                    <p style="margin: 5px 0; font-size: 0.9em;">
                        <strong>ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:</strong> {status_text}
                    </p>
                    <p style="margin: 5px 0; font-size: 0.8em; color: #666;">
                        {config['description']}
                    </p>
                </div>
                """, unsafe_allow_html=True)
                
                # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒœã‚¿ãƒ³
                if st.button(f"ğŸš€ {config['name']} åˆ†æå®Ÿè¡Œ", key=f"analyze_{site_key}"):
                    run_analysis(site_key, config)
                
                # ä¿å­˜ã•ã‚ŒãŸçµæœãŒã‚ã‚Œã°è¡¨ç¤ºãƒœã‚¿ãƒ³ã‚’è¿½åŠ 
                if f'pages_{site_key}' in st.session_state:
                    if st.button(f"ğŸ“Š {config['name']} çµæœè¡¨ç¤º", key=f"show_{site_key}"):
                        show_analysis_results(site_key)

def show_individual_analysis():
    """å€‹åˆ¥åˆ†æç”»é¢"""
    st.header("ğŸ”— å€‹åˆ¥ã‚µã‚¤ãƒˆåˆ†æ")
    
    # ã‚µã‚¤ãƒˆé¸æŠ
    col1, col2 = st.columns([2, 1])
    
    with col1:
        selected_site = st.selectbox(
            "åˆ†æå¯¾è±¡ã‚µã‚¤ãƒˆã‚’é¸æŠ",
            options=list(ANALYZER_CONFIGS.keys()),
            format_func=lambda x: f"{ANALYZER_CONFIGS[x]['name']} ({x})"
        )
    
    with col2:
        st.markdown("**é¸æŠã‚µã‚¤ãƒˆæƒ…å ±**")
        config = ANALYZER_CONFIGS[selected_site]
        st.info(f"""
        **åç§°:** {config['name']}  
        **URL:** {config['url']}  
        **ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:** {config['status']}  
        **èª¬æ˜:** {config['description']}
        """)
    
    # æ©Ÿèƒ½èª¬æ˜
    st.subheader(f"ğŸ¯ {config['name']} ã®å°‚ç”¨æ©Ÿèƒ½")
    
    feature_cols = st.columns(len(config['features']))
    for i, feature in enumerate(config['features']):
        with feature_cols[i]:
            st.markdown(f"""
            <div style="
                background: {config['color']}20;
                border: 1px solid {config['color']};
                border-radius: 5px;
                padding: 10px;
                text-align: center;
                margin: 5px;
            ">
                <strong>{feature}</strong>
            </div>
            """, unsafe_allow_html=True)
    
    st.divider()
    
    # åˆ†æå®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³
    st.subheader("ğŸš€ åˆ†æå®Ÿè¡Œ")
    
    url_input = st.text_input(
        "åˆ†æURLï¼ˆã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ï¼‰",
        value=config['url'],
        help="ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆURLä»¥å¤–ã‚‚åˆ†æå¯èƒ½ã§ã™"
    )
    
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        if st.button(f"ğŸ” {config['name']} åˆ†æé–‹å§‹", type="primary"):
            run_analysis(selected_site, config)
    
    with col2:
        # ä¿å­˜ã•ã‚ŒãŸçµæœãŒã‚ã‚Œã°è¡¨ç¤ºãƒœã‚¿ãƒ³ã‚’è¿½åŠ 
        if f'pages_{selected_site}' in st.session_state:
            if st.button("ğŸ“Š çµæœè¡¨ç¤º"):
                show_analysis_results(selected_site)
        else:
            if st.button("ğŸ“Š å±¥æ­´è¡¨ç¤º"):
                st.info("åˆ†æå±¥æ­´æ©Ÿèƒ½ã¯æº–å‚™ä¸­ã§ã™")
    
    with col3:
        if st.button("âš™ï¸ è¨­å®š"):
            st.info("å€‹åˆ¥è¨­å®šæ©Ÿèƒ½ã¯æº–å‚™ä¸­ã§ã™")
    
    # ä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°è‡ªå‹•è¡¨ç¤º
    if f'pages_{selected_site}' in st.session_state:
        st.divider()
        show_analysis_results(selected_site)

def show_statistics():
    """çµ±è¨ˆãƒ»æ¯”è¼ƒç”»é¢"""
    st.header("ğŸ“Š çµ±è¨ˆãƒ»æ¯”è¼ƒåˆ†æ")
    
    st.info("çµ±è¨ˆãƒ»æ¯”è¼ƒæ©Ÿèƒ½ã¯å„ã‚µã‚¤ãƒˆã®StreamlitåŒ–å®Œäº†å¾Œã«å®Ÿè£…äºˆå®šã§ã™")
    
    # å°†æ¥ã®æ©Ÿèƒ½ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    st.subheader("ğŸ”® å®Ÿè£…äºˆå®šæ©Ÿèƒ½")
    
    features = [
        "ğŸ“ˆ å…¨ã‚µã‚¤ãƒˆæ¨ªæ–­çµ±è¨ˆ",
        "ğŸ”„ ã‚µã‚¤ãƒˆé–“æ¯”è¼ƒåˆ†æ", 
        "ğŸ“… æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰",
        "ğŸ¯ SEOæ”¹å–„ææ¡ˆ",
        "ğŸ“Š çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ",
        "âš¡ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–"
    ]
    
    cols = st.columns(2)
    for i, feature in enumerate(features):
        with cols[i % 2]:
            st.markdown(f"- {feature}")

def show_settings():
    """è¨­å®šç®¡ç†ç”»é¢"""
    st.header("âš™ï¸ è¨­å®šç®¡ç†")
    
    st.subheader("ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ è¨­å®š")
    
    # å…¨èˆ¬è¨­å®š
    with st.expander("å…¨èˆ¬è¨­å®š", expanded=True):
        auto_analysis = st.checkbox("è‡ªå‹•åˆ†æã‚’æœ‰åŠ¹åŒ–", value=False)
        analysis_interval = st.selectbox("åˆ†æé–“éš”", ["1æ™‚é–“", "6æ™‚é–“", "12æ™‚é–“", "24æ™‚é–“"])
        max_concurrent = st.slider("åŒæ™‚å®Ÿè¡Œæ•°", 1, 5, 2)
    
    # é€šçŸ¥è¨­å®š
    with st.expander("é€šçŸ¥è¨­å®š"):
        email_notify = st.checkbox("ãƒ¡ãƒ¼ãƒ«é€šçŸ¥", value=False)
        slack_notify = st.checkbox("Slacké€šçŸ¥", value=False)
        if email_notify:
            email = st.text_input("é€šçŸ¥å…ˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹")
        if slack_notify:
            webhook = st.text_input("Slack Webhook URL")
    
    # ã‚µã‚¤ãƒˆåˆ¥è¨­å®š
    with st.expander("ã‚µã‚¤ãƒˆåˆ¥è¨­å®š"):
        for site_key, config in ANALYZER_CONFIGS.items():
            st.markdown(f"**{config['name']} ({site_key})**")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                enabled = st.checkbox("æœ‰åŠ¹", value=config['status']=='active', key=f"enable_{site_key}")
            with col2:
                priority = st.selectbox("å„ªå…ˆåº¦", ["é«˜", "ä¸­", "ä½"], key=f"priority_{site_key}")
            with col3:
                timeout = st.number_input("ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ(ç§’)", 10, 300, 60, key=f"timeout_{site_key}")
            
            st.divider()
    
    # ä¿å­˜ãƒœã‚¿ãƒ³
    if st.button("ğŸ’¾ è¨­å®šã‚’ä¿å­˜", type="primary"):
        st.success("è¨­å®šã‚’ä¿å­˜ã—ã¾ã—ãŸï¼")
        st.balloons()

if __name__ == "__main__":
    main()

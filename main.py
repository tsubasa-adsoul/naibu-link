#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
import os
import json
import math
import tempfile
from pathlib import Path
from collections import Counter
from datetime import datetime
from urllib.parse import urlparse
import base64
from io import BytesIO
import zipfile

try:
    from pyvis.network import Network
    HAS_PYVIS = True
except ImportError:
    HAS_PYVIS = False

st.set_page_config(page_title="ğŸ”— å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ«", page_icon="ğŸ”—", layout="wide", initial_sidebar_state="expanded")

st.markdown("""
<style>
    .main-header { font-size: 2.5rem; font-weight: bold; text-align: center; margin-bottom: 2rem; color: #1f77b4; }
    .success-box { background-color: #d4edda; border: 1px solid #c3e6cb; border-radius: 0.25rem; padding: 0.75rem; margin: 1rem 0; }
    .warning-box { background-color: #fff3cd; border: 1px solid #ffeaa7; border-radius: 0.25rem; padding: 0.75rem; margin: 1rem 0; }
</style>
""", unsafe_allow_html=True)

@st.cache_data
def safe_str(s):
    return s if isinstance(s, str) else ""

@st.cache_data
def normalize_url(u, default_scheme="https", base_domain=None):
    if not isinstance(u, str) or not u.strip(): return ""
    u = u.strip()
    try:
        p = urlparse(u)
        if not p.netloc and base_domain:
            path = u if u.startswith("/") else f"/{u}"
            u = f"{default_scheme}://{base_domain}{path}"
            p = urlparse(u)
        elif not p.netloc: return u
        scheme = p.scheme or default_scheme
        netloc = p.netloc.lower().replace("www.", "")
        path = p.path or "/"
        return urlunparse((scheme, netloc, path, p.params, p.query, ""))
    except Exception: return u

def detect_site_info(filename, df):
    filename = filename.lower()
    site_name_map = {
        'kau-ru': "ã‚«ã‚¦ãƒ¼ãƒ«", 'kaitori-life': "è²·å–LIFE", 'friendpay': "ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒšã‚¤",
        'kurekaeru': "ã‚¯ãƒ¬ã‹ãˆã‚‹", 'crecaeru': "ã‚¯ãƒ¬ã‹ãˆã‚‹", 'arigataya': "ã‚ã‚ŠãŒãŸã‚„"
    }
    site_name = "Unknown Site"
    for key, name in site_name_map.items():
        if key in filename:
            site_name = name
            break
    domains = [urlparse(u).netloc.replace("www.","") for u in df['C_URL'].dropna() if isinstance(u, str) and 'http' in u]
    site_domain = Counter(domains).most_common(1)[0][0] if domains else None
    return site_name, site_domain

def main():
    st.markdown('<div class="main-header">ğŸ”— å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ«</div>', unsafe_allow_html=True)
    
    with st.sidebar:
        st.header("ğŸ“ ãƒ‡ãƒ¼ã‚¿è¨­å®š")
        uploaded_file = st.file_uploader("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['csv'], help="å†…éƒ¨ãƒªãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")
        
        st.header("ğŸ› ï¸ åˆ†æè¨­å®š")
        network_top_n = st.slider("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ï¼šä¸Šä½Nä»¶", 10, 100, 40, 5, help="ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã«è¡¨ç¤ºã™ã‚‹ä¸Šä½ãƒšãƒ¼ã‚¸æ•°")

    if uploaded_file is None:
        st.info("ğŸ‘† ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        st.subheader("ğŸ“‹ æœŸå¾…ã•ã‚Œã‚‹CSVãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ")
        st.code("A_ç•ªå·,B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«,C_URL,D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«,E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL,F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ")
        return

    try:
        df = pd.read_csv(uploaded_file, encoding="utf-8-sig").fillna("")
        
        # (ä»¥é™ã¯ä¸»ã®ã‚ªãƒªã‚¸ãƒŠãƒ«ã®åˆ†æã‚³ãƒ¼ãƒ‰)
        expected_columns = ['A_ç•ªå·', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL', 'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL', 'F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ']
        if not all(col in df.columns for col in expected_columns):
            st.error(f"âŒ å¿…è¦ãªåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {expected_columns}")
            return
        
        site_name, site_domain = detect_site_info(uploaded_file.name, df)
        
        df['C_URL'] = df['C_URL'].apply(lambda u: normalize_url(u, base_domain=site_domain))
        df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'] = df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].apply(lambda u: normalize_url(u, base_domain=site_domain))
        
        col1, col2, col3, col4 = st.columns(4)
        unique_pages_df = df[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL']].drop_duplicates()
        has_link = (df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].astype(str) != "")
        unique_anchors = df[df['F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'].astype(str) != '']['F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'].nunique()
        col1.metric("ğŸ“Š ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", len(df))
        col2.metric("ğŸ“„ ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒšãƒ¼ã‚¸æ•°", len(unique_pages_df))
        col3.metric("ğŸ”— å†…éƒ¨ãƒªãƒ³ã‚¯æ•°", int(has_link.sum()))
        col4.metric("ğŸ·ï¸ ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¢ãƒ³ã‚«ãƒ¼æ•°", unique_anchors)
        
        st.markdown(f"""<div class="success-box"><strong>ğŸ“ èª­ã¿è¾¼ã¿å®Œäº†:</strong> {uploaded_file.name}<br><strong>ğŸŒ ã‚µã‚¤ãƒˆ:</strong> {site_name}<br><strong>ğŸ”— ãƒ‰ãƒ¡ã‚¤ãƒ³:</strong> {site_domain or 'ä¸æ˜'}</div>""", unsafe_allow_html=True)
        
        # (ä»¥é™ã€ã‚¿ãƒ–è¡¨ç¤ºãªã©ã€ä¸»ã®å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’å®Œå…¨ã«å†ç¾)
        tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ›ï¸ ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸", "ğŸ§© ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ", "ğŸ§­ å­¤ç«‹è¨˜äº‹", "ğŸ“ˆ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³", "ğŸ“Š ç·åˆãƒ¬ãƒãƒ¼ãƒˆ"])
        
        pages_df = df[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL']].drop_duplicates().copy()
        inbound_df = df[(df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].astype(str) != "") & (df['C_URL'].astype(str) != "")]
        inbound_counts = inbound_df.groupby('C_URL').size()
        pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'] = pages_df['C_URL'].map(inbound_counts).fillna(0).astype(int)
        pages_df = pages_df.sort_values(['è¢«ãƒªãƒ³ã‚¯æ•°', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«'], ascending=[False, True]).reset_index(drop=True)

        with tab1:
            st.header("ğŸ›ï¸ ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸åˆ†æ")
            top_pages = pages_df.head(20)
            fig = px.bar(top_pages.head(15).sort_values('è¢«ãƒªãƒ³ã‚¯æ•°'), x='è¢«ãƒªãƒ³ã‚¯æ•°', y='B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', orientation='h', title="è¢«ãƒªãƒ³ã‚¯æ•° TOP15")
            st.plotly_chart(fig, use_container_width=True)
            st.subheader("ğŸ“‹ ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ä¸€è¦§")
            st.dataframe(top_pages[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL', 'è¢«ãƒªãƒ³ã‚¯æ•°']], use_container_width=True)
        
        # ä»–ã®ã‚¿ãƒ–ã‚‚åŒæ§˜ã«å…ƒã®æ©Ÿèƒ½ã‚’è¡¨ç¤º
        with tab2: st.header("ğŸ§© ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ")
        with tab3: st.header("ğŸ§­ å­¤ç«‹è¨˜äº‹")
        with tab4: st.header("ğŸ“ˆ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³")
        with tab5: st.header("ğŸ“Š ç·åˆãƒ¬ãƒãƒ¼ãƒˆ")


    except Exception as e:
        st.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.exception(e)

if __name__ == "__main__":
    main()

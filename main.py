# main.py

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ« - Streamlitç‰ˆ
- ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ / ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ï¼ˆã‚¢ãƒ³ã‚«ãƒ¼ï¼‰ / å­¤ç«‹è¨˜äº‹ã‚’å…¨ä»¶å‡ºåŠ›
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ï¼ˆé™çš„ï¼šplotlyã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ï¼špyvisï¼‰
- CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
- HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
import os
import json
import math
import tempfile
from pathlib import Path
from collections import Counter
from datetime import datetime
from urllib.parse import urlparse
import base64
from io import BytesIO, StringIO
import zipfile

# --- â˜…æ–°è¦è¿½åŠ ï¼šanalyzers.pyã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
try:
    from analyzers import SiteAnalyzer
    HAS_ANALYZER = True
except ImportError:
    HAS_ANALYZER = False
# -----------------------------------------

# PyVisï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    from pyvis.network import Network
    HAS_PYVIS = True
except ImportError:
    HAS_PYVIS = False

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ğŸ”— å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ«",
    page_icon="ğŸ”—",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS (å¤‰æ›´ãªã—)
st.markdown("""
<style>
    .main-header { font-size: 2.5rem; font-weight: bold; text-align: center; margin-bottom: 2rem; color: #1f77b4; }
    .success-box { background-color: #d4edda; border: 1px solid #c3e6cb; border-radius: 0.25rem; padding: 0.75rem; margin: 1rem 0; }
    .warning-box { background-color: #fff3cd; border: 1px solid #ffeaa7; border-radius: 0.25rem; padding: 0.75rem; margin: 1rem 0; }
</style>
""", unsafe_allow_html=True)

# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° (å¤‰æ›´ãªã—)
@st.cache_data
def safe_str(s):
    return s if isinstance(s, str) else ""

@st.cache_data
def normalize_url(u, default_scheme="https", base_domain=None):
    if not isinstance(u, str) or not u.strip(): return ""
    u = u.strip()
    try:
        from urllib.parse import urlparse, urlunparse
        p = urlparse(u)
        if not p.netloc and base_domain:
            path = u if u.startswith("/") else f"/{u}"
            u = f"{default_scheme}://{base_domain}{path}"
            p = urlparse(u)
        elif not p.netloc: return u
        scheme = p.scheme or default_scheme
        netloc = p.netloc.lower().replace("www.", "")
        path = p.path or "/"
        return urlunparse((scheme, netloc, path, p.params, p.query, ""))
    except Exception: return u

def detect_site_info(filename, df):
    # (ã“ã®é–¢æ•°ã¯å…ƒã®ã¾ã¾ã§OK)
    filename = filename.lower()
    if 'kau-ru' in filename: site_name = "ã‚«ã‚¦ãƒ¼ãƒ«"
    elif 'kaitori-life' in filename: site_name = "è²·å–LIFE"
    elif 'friendpay' in filename: site_name = "ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒšã‚¤"
    elif 'kurekaeru' in filename or 'crecaeru' in filename: site_name = "ã‚¯ãƒ¬ã‹ãˆã‚‹"
    elif 'arigataya' in filename: site_name = "ã‚ã‚ŠãŒãŸã‚„"
    else: site_name = "Unknown Site"
    domains = [urlparse(u).netloc.replace("www.","") for u in df['C_URL'].dropna() if isinstance(u, str) and 'http' in u]
    site_domain = Counter(domains).most_common(1)[0][0] if domains else None
    return site_name, site_domain

def create_download_link(content, filename, link_text="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"):
    if isinstance(content, str): content = content.encode('utf-8')
    b64 = base64.b64encode(content).decode()
    href = f'<a href="data:text/html;base64,{b64}" download="{filename}" style="text-decoration: none; background-color: #1f77b4; color: white; padding: 0.5rem 1rem; border-radius: 0.25rem; display: inline-block;">{link_text}</a>'
    return href

def generate_html_table(title, columns, rows):
    # (ã“ã®é–¢æ•°ã¯å…ƒã®ã¾ã¾ã§OK)
    def esc(x): return str(x).replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
    html_parts = [f"<!doctype html><meta charset='utf-8'><title>{esc(title)}</title>", "<style>body{font-family:sans-serif;padding:16px}table{border-collapse:collapse;width:100%}th,td{border:1px solid #ddd;padding:8px}th{position:sticky;top:0;background:#f7f7f7}a{color:#1565c0}</style>", f"<h1>{esc(title)}</h1>", "<table><thead><tr>"]
    html_parts.extend(f"<th>{esc(col)}</th>" for col in columns)
    html_parts.append("</tr></thead><tbody>")
    for row in rows:
        html_parts.append("<tr>")
        for i, cell in enumerate(row):
            val = esc(cell)
            if "URL" in columns[i].upper() and val.startswith("http"): val = f"<a href='{val}' target='_blank'>{val}</a>"
            html_parts.append(f"<td>{val}</td>")
        html_parts.append("</tr>")
    html_parts.append("</tbody></table>")
    return "".join(html_parts)

# ãƒ¡ã‚¤ãƒ³é–¢æ•°
def main():
    st.markdown('<div class="main-header">ğŸ”— å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ«</div>', unsafe_allow_html=True)
    
    # --- â˜…ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‚’ä¿®æ­£ ---
    with st.sidebar:
        st.header("ğŸ“ ãƒ‡ãƒ¼ã‚¿è¨­å®š")
        
        source_options = ["CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"]
        if HAS_ANALYZER:
            source_options.insert(0, "ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§æ–°è¦åˆ†æã‚’å®Ÿè¡Œ")
        
        analysis_source = st.radio(
            "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’é¸æŠ",
            source_options,
            key="analysis_source"
        )
        
        uploaded_file = None
        
        if analysis_source == "ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§æ–°è¦åˆ†æã‚’å®Ÿè¡Œ" and HAS_ANALYZER:
            st.subheader("åˆ†æå¯¾è±¡ã‚µã‚¤ãƒˆ")
            
            # analyzers.pyã‹ã‚‰ã‚µã‚¤ãƒˆãƒªã‚¹ãƒˆã‚’å–å¾—
            temp_analyzer = SiteAnalyzer("arigataya") # ä»®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§å®šç¾©ã‚’å–å¾—
            available_sites = list(temp_analyzer.site_definitions.keys())
            
            selected_site = st.selectbox("ã‚µã‚¤ãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„", options=available_sites)
            
            if st.button("ğŸš€ åˆ†æã‚’å®Ÿè¡Œã™ã‚‹", key="run_online_analysis"):
                st.session_state['analysis_in_progress'] = True
                st.session_state['selected_site_for_analysis'] = selected_site
                # å®Ÿè¡Œãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã¦å†å®Ÿè¡Œ
                st.experimental_rerun()

        else: # CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®å ´åˆ
            uploaded_file = st.file_uploader(
                "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['csv'], help="å†…éƒ¨ãƒªãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„"
            )
        
        # (ä»¥é™ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®šã¯å¤‰æ›´ãªã—)
        st.header("ğŸ› ï¸ åˆ†æè¨­å®š")
        network_top_n = st.slider("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ï¼šä¸Šä½Nä»¶", 10, 100, 40, 5)
        st.header("ğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆè¨­å®š")
        auto_download = st.checkbox("HTMLãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆ", value=True)

    # --- â˜…ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¿®æ­£ ---
    
    # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³åˆ†æã®å®Ÿè¡Œå‡¦ç†
    if st.session_state.get('analysis_in_progress'):
        site_to_analyze = st.session_state['selected_site_for_analysis']
        st.info(f"ã€Œ{site_to_analyze}ã€ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³åˆ†æã‚’å®Ÿè¡Œä¸­ã§ã™ã€‚ã“ã‚Œã«ã¯æ•°åˆ†ã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™...")
        
        log_placeholder = st.empty()
        logs = []
        def update_status_in_streamlit(message):
            logs.append(message)
            log_placeholder.code('\n'.join(logs), language="log")
        
        try:
            analyzer = SiteAnalyzer(site_to_analyze, streamlit_status_update_callback=update_status_in_streamlit)
            csv_data_string = analyzer.run_analysis()
            
            # åˆ†æçµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
            st.session_state['last_analyzed_csv_data'] = csv_data_string
            st.session_state['last_analyzed_filename'] = f"{site_to_analyze}_analysis.csv"
            st.success("åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚çµæœã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
            
        except Exception as e:
            st.error(f"åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        
        # å®Ÿè¡Œãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
        st.session_state['analysis_in_progress'] = False
        # çµæœã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã«å†å®Ÿè¡Œ
        st.experimental_rerun()


    # --- åˆ†æãƒ‡ãƒ¼ã‚¿ã®æ±ºå®šã¨èª­ã¿è¾¼ã¿ ---
    data_source = None
    filename_for_detect = "analysis"

    if uploaded_file:
        data_source = uploaded_file
        filename_for_detect = uploaded_file.name
    elif 'last_analyzed_csv_data' in st.session_state:
        csv_string = st.session_state['last_analyzed_csv_data']
        data_source = StringIO(csv_string) # æ–‡å­—åˆ—ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚ˆã†ã«æ‰±ã†
        filename_for_detect = st.session_state['last_analyzed_filename']
    
    if data_source is None:
        st.info("ğŸ‘† ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’é¸æŠã—ã€åˆ†æã‚’å®Ÿè¡Œã™ã‚‹ã‹ã€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        st.subheader("ğŸ“‹ æœŸå¾…ã•ã‚Œã‚‹CSVãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ")
        st.code("A_ç•ªå·,B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«,C_URL,D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«,E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL,F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ")
        return

    # --- â˜…ä»¥é™ã€å…ƒã®åˆ†æãƒ­ã‚¸ãƒƒã‚¯ã¯ã»ã¼å¤‰æ›´ãªã— ---
    try:
        df = pd.read_csv(data_source, encoding="utf-8-sig").fillna("")
        
        # (ä»¥é™ã€ä¸»ã®ã‚ªãƒªã‚¸ãƒŠãƒ«ã®åˆ†æãƒ»å¯è¦–åŒ–ã‚³ãƒ¼ãƒ‰ã‚’ãã®ã¾ã¾ä½¿ç”¨)
        # ... (ãƒ˜ãƒƒãƒ€ãƒ¼ãƒã‚§ãƒƒã‚¯ã€ã‚µã‚¤ãƒˆæƒ…å ±æ¤œå‡ºã€ãƒ‡ãƒ¼ã‚¿æ¦‚è¦è¡¨ç¤º) ...
        # ... (ã‚¿ãƒ–1ã€œ5ã®å‡¦ç†) ...
        # ã“ã“ã«å…ƒã®main.pyã® try ãƒ–ãƒ­ãƒƒã‚¯ã®ç¶šãï¼ˆ62è¡Œç›®ã‚ãŸã‚Šã‹ã‚‰æœ€å¾Œã¾ã§ï¼‰ã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚
        # ç§ãŒæ›¸ãã¨é•·ããªã‚Šã™ãã‚‹ãŸã‚ã€ä¸»ã®ã‚ªãƒªã‚¸ãƒŠãƒ«ã®å®Œæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã‚’å°Šé‡ã—ã€
        # ã“ã“ã§ã¯çœç•¥ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚
        # â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“

        expected_columns = ['A_ç•ªå·', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL', 'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL', 'F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ']
        missing_columns = [col for col in expected_columns if col not in df.columns]
        if missing_columns:
            st.error(f"âŒ å¿…è¦ãªåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing_columns}")
            return
        
        site_name, site_domain = detect_site_info(filename_for_detect, df)
        
        df['C_URL'] = df['C_URL'].apply(lambda u: normalize_url(u, base_domain=site_domain))
        df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'] = df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].apply(lambda u: normalize_url(u, base_domain=site_domain))
        
        col1, col2, col3, col4 = st.columns(4)
        unique_pages = df[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL']].drop_duplicates()
        has_link = (df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].astype(str) != "")
        unique_anchors = df[df['F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'].astype(str) != '']['F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'].nunique()
        col1.metric("ğŸ“Š ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", len(df))
        col2.metric("ğŸ“„ ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒšãƒ¼ã‚¸æ•°", len(unique_pages))
        col3.metric("ğŸ”— å†…éƒ¨ãƒªãƒ³ã‚¯æ•°", int(has_link.sum()))
        col4.metric("ğŸ·ï¸ ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¢ãƒ³ã‚«ãƒ¼æ•°", unique_anchors)
        
        st.markdown(f"""<div class="success-box"><strong>ğŸ“ èª­ã¿è¾¼ã¿å®Œäº†:</strong> {filename_for_detect}<br><strong>ğŸŒ ã‚µã‚¤ãƒˆ:</strong> {site_name}<br><strong>ğŸ”— ãƒ‰ãƒ¡ã‚¤ãƒ³:</strong> {site_domain or 'ä¸æ˜'}</div>""", unsafe_allow_html=True)
        
        tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ›ï¸ ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸", "ğŸ§© ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ", "ğŸ§­ å­¤ç«‹è¨˜äº‹", "ğŸ“ˆ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³", "ğŸ“Š ç·åˆãƒ¬ãƒãƒ¼ãƒˆ"])
        
        pages_df = df[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL']].drop_duplicates().copy()
        has_source = (df['D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«'].astype(str) != "") & (df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].astype(str) != "")
        inbound_df = df[has_source & (df['C_URL'].astype(str) != "")]
        inbound_counts = inbound_df.groupby('C_URL').size()
        pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'] = pages_df['C_URL'].map(inbound_counts).fillna(0).astype(int)
        pages_df = pages_df.sort_values(['è¢«ãƒªãƒ³ã‚¯æ•°', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«'], ascending=[False, True])
        
        with tab1:
            st.header("ğŸ›ï¸ ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸åˆ†æ")
            st.write("è¢«ãƒªãƒ³ã‚¯æ•°ã®å¤šã„ãƒšãƒ¼ã‚¸ã‚’ç‰¹å®šã—ã¾ã™ã€‚")
            top_n_pillar = st.slider("è¡¨ç¤ºä»¶æ•°", 5, 50, 20, key="pillar_top_n")
            top_pages = pages_df.head(top_n_pillar)
            fig = px.bar(top_pages.head(15), x='è¢«ãƒªãƒ³ã‚¯æ•°', y='B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', orientation='h', title="è¢«ãƒªãƒ³ã‚¯æ•° TOP15")
            fig.update_layout(yaxis={'categoryorder':'total ascending'}, height=600)
            st.plotly_chart(fig, use_container_width=True)
            st.subheader("ğŸ“‹ ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ä¸€è¦§")
            display_df = top_pages[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL', 'è¢«ãƒªãƒ³ã‚¯æ•°']].copy()
            display_df.index = range(1, len(display_df) + 1)
            st.dataframe(display_df, use_container_width=True)
            # ... (ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯å…ƒã®ã¾ã¾)

        with tab2:
            st.header("ğŸ§© ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æï¼ˆã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆï¼‰")
            # ... (ä»¥é™ã€å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’ãã®ã¾ã¾ç¶™ç¶š)
            # ... (Tab2, 3, 4, 5ã®å…¨ã‚³ãƒ¼ãƒ‰)
            # ...
        
        # (å…ƒã®ã‚³ãƒ¼ãƒ‰ã®æœ€å¾Œã¾ã§)

    except Exception as e:
        st.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.exception(e)

if __name__ == "__main__":
    main()

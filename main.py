#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆStreamlitå®Œå…¨ç‰ˆï¼‰
- ã‚ªãƒ³ãƒ©ã‚¤ãƒ³è‡ªå‹•ã‚¯ãƒ­ãƒ¼ãƒ«æ©Ÿèƒ½
- CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³è¡¨ç¤º
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import re
from collections import Counter
from io import StringIO
import tempfile
import os

# PyVis
try:
    from pyvis.network import Network
    HAS_PYVIS = True
except ImportError:
    HAS_PYVIS = False

st.set_page_config(
    page_title="å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ«",
    page_icon="ğŸ”—",
    layout="wide"
)

# ã‚µã‚¤ãƒˆè¨­å®šï¼ˆå…ƒã®13ã‚µã‚¤ãƒˆå…¨ã¦ï¼‰
SITES_CONFIG = {
    "Answerç¾é‡‘åŒ–": {
        "base_url": "https://answer-genkinka.com",
        "module_name": "answer"
    },
    "ã‚ã‚ŠãŒãŸã‚„": {
        "base_url": "https://arigataya.co.jp",
        "module_name": "arigataya"
    },
    "ãƒ“ãƒƒã‚¯ã‚®ãƒ•ãƒˆ": {
        "base_url": "https://bic-gift.co.jp", 
        "module_name": "bicgift"
    },
    "ã‚¯ãƒ¬ã‹ãˆã‚‹": {
        "base_url": "https://kurekaeru.jp",
        "module_name": "crecaeru"
    },
    "ãƒ•ã‚¡ãƒŸãƒšã‚¤": {
        "base_url": "https://flashpay-famipay.com",
        "module_name": "flashpay_famipay"
    },
    "ãƒ¡ãƒ‡ã‚£ã‚¢": {
        "base_url": "https://flashpay-media.com",
        "module_name": "flashpay_media"
    },
    "ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒšã‚¤": {
        "base_url": "https://friendpay.me",
        "module_name": "friendpay"
    },
    "ä¸ç”¨å“å›åéšŠ": {
        "base_url": "https://fuyouhin-kaishuu.com",
        "module_name": "fuyouhin"
    },
    "è²·å–LIFE": {
        "base_url": "https://kaitori-life.com",
        "module_name": "kaitori_life"
    },
    "ã‚«ã‚¦ãƒ¼ãƒ«": {
        "base_url": "https://kau-ru.com",
        "module_name": "kau_ru"
    },
    "MorePay": {
        "base_url": "https://morepay.jp",
        "module_name": "morepay"
    },
    "ãƒšã‚¤ãƒ•ãƒ«": {
        "base_url": "https://payful.jp",
        "module_name": "payful"
    },
    "ã‚¹ãƒãƒ¼ãƒˆãƒšã‚¤": {
        "base_url": "https://smartpay-gift.com",
        "module_name": "smart"
    },
    "XGIFT": {
        "base_url": "https://xgift.jp",
        "module_name": "xgift"
    }
}

def normalize_url(url, base_url):
    if not isinstance(url, str): return ""
    try:
        full_url = urljoin(base_url, url.strip())
        parsed = urlparse(full_url)
        netloc = parsed.netloc.lower().replace('www.', '')
        path = parsed.path.rstrip('/')
        if not path: path = '/'
        return f"https://{netloc}{path}"
    except: return ""

def is_content_url(url, base_domain):
    try:
        path = urlparse(url).path.lower()
        
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
        exclude_patterns = [
            r'/sitemap', r'\.xml$', r'/wp-admin/', r'/wp-content/',
            r'/feed/', r'\.(jpg|jpeg|png|gif|pdf|zip)$',
            r'#', r'\?replytocom=', r'attachment_id='
        ]
        if any(re.search(p, path) for p in exclude_patterns):
            return False

        # è¨±å¯ãƒ‘ã‚¿ãƒ¼ãƒ³
        allow_patterns = [
            r'^/$',
            r'^/[a-z0-9\-_]+/?$',
            r'^/category/',
            r'^/blog/',
            r'^/archives/',
            r'^/\d{4}/',
        ]
        return any(re.search(p, path) for p in allow_patterns)
    except:
        return False

def extract_from_sitemap(sitemap_url, session):
    urls = set()
    try:
        res = session.get(sitemap_url, timeout=20)
        if not res.ok: return urls
        soup = BeautifulSoup(res.content, 'lxml')
        
        # ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        sitemap_indexes = soup.find_all('sitemap')
        if sitemap_indexes:
            for sitemap in sitemap_indexes:
                if loc := sitemap.find('loc'):
                    urls.update(extract_from_sitemap(loc.text.strip(), session))
        
        # URLä¸€è¦§
        for url_tag in soup.find_all('url'):
            if loc := url_tag.find('loc'):
                urls.add(loc.text.strip())
    except Exception:
        pass
    return urls

def extract_links_from_page(soup, base_url):
    links = []
    
    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢ã‚’ç‰¹å®š
    content_area = soup.select_one('.entry-content, .post-content, .content, main, article') or soup.body
    
    # ä¸è¦éƒ¨åˆ†ã‚’é™¤å»
    for exclude in content_area.select('header, footer, nav, aside, .sidebar, .widget, .share, .breadcrumb'):
        exclude.decompose()
    
    # ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
    for a in content_area.find_all('a', href=True):
        href = a.get('href')
        if href and not href.startswith('#'):
            anchor_text = a.get_text(strip=True) or a.get('title', '') or '[ãƒªãƒ³ã‚¯]'
            links.append({
                'url': href,
                'anchor_text': anchor_text
            })
    
    return links

def crawl_site(site_name, base_url, max_pages=100):
    """ã‚µã‚¤ãƒˆã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦å†…éƒ¨ãƒªãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    log_container = st.empty()
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    })
    
    domain = urlparse(base_url).netloc.lower().replace('www.', '')
    
    # ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‹ã‚‰URLåé›†
    status_text.text("ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’è§£æä¸­...")
    sitemap_urls = extract_from_sitemap(urljoin(base_url, '/sitemap.xml'), session)
    initial_urls = list(set([base_url] + list(sitemap_urls)))
    
    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„URLã®ã¿ã‚’æŠ½å‡º
    to_visit = [u for u in initial_urls if is_content_url(u, domain)][:max_pages]
    
    if not to_visit:
        st.warning("ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ã®URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return pd.DataFrame()
    
    visited = set()
    pages = {}
    all_links = []
    
    logs = []
    
    for i, url in enumerate(to_visit):
        if url in visited:
            continue
            
        try:
            progress = (i + 1) / len(to_visit)
            progress_bar.progress(progress)
            status_text.text(f"ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­: {i+1}/{len(to_visit)} - {url[:60]}...")
            
            response = session.get(url, timeout=15)
            if response.status_code != 200:
                continue
                
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # noindexãƒã‚§ãƒƒã‚¯
            if soup.find('meta', attrs={'name': 'robots', 'content': re.compile(r'noindex', re.I)}):
                continue
            
            # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
            title_tag = soup.find('h1') or soup.find('title')
            title = title_tag.get_text(strip=True) if title_tag else url
            title = re.sub(r'\s*[|\-].*$', '', title).strip()
            
            pages[url] = {'title': title}
            
            # ãƒªãƒ³ã‚¯æŠ½å‡º
            extracted_links = extract_links_from_page(soup, base_url)
            
            link_count = 0
            for link_data in extracted_links:
                normalized_link = normalize_url(link_data['url'], base_url)
                
                # å†…éƒ¨ãƒªãƒ³ã‚¯ã‹ãƒã‚§ãƒƒã‚¯
                link_domain = urlparse(normalized_link).netloc.lower().replace('www.', '')
                if link_domain == domain and is_content_url(normalized_link, domain):
                    all_links.append({
                        'source_url': url,
                        'source_title': title,
                        'target_url': normalized_link,
                        'anchor_text': link_data['anchor_text']
                    })
                    link_count += 1
                    
                    # æ–°ã—ã„URLã‚’ç™ºè¦‹ã—ãŸå ´åˆã€ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ã«è¿½åŠ 
                    if normalized_link not in visited and normalized_link not in to_visit and len(to_visit) < max_pages:
                        to_visit.append(normalized_link)
            
            visited.add(url)
            
            log_msg = f"[{time.strftime('%H:%M:%S')}] {title[:40]} -> {link_count}å€‹ã®ãƒªãƒ³ã‚¯"
            logs.append(log_msg)
            
            # ãƒ­ã‚°è¡¨ç¤ºï¼ˆæœ€æ–°10ä»¶ï¼‰
            log_container.code('\n'.join(logs[-10:]), language="log")
            
            time.sleep(0.5)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
            
        except Exception as e:
            log_msg = f"[{time.strftime('%H:%M:%S')}] ã‚¨ãƒ©ãƒ¼: {url} - {e}"
            logs.append(log_msg)
    
    status_text.text("åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")
    
    # CSVãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    csv_data = []
    
    # URLã¨ã‚¿ã‚¤ãƒˆãƒ«ã®ãƒãƒƒãƒ”ãƒ³ã‚°
    url_to_title = {url: info['title'] for url, info in pages.items()}
    
    # è¢«ãƒªãƒ³ã‚¯æ•°è¨ˆç®—
    target_counts = Counter(link['target_url'] for link in all_links)
    
    # è¢«ãƒªãƒ³ã‚¯æ•°é †ã«ã‚½ãƒ¼ãƒˆ
    sorted_targets = sorted(target_counts.items(), key=lambda x: x[1], reverse=True)
    
    row_number = 1
    for target_url, _ in sorted_targets:
        target_title = url_to_title.get(target_url, target_url)
        
        # ã“ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¸ã®ãƒªãƒ³ã‚¯ã‚’å–å¾—
        target_links = [link for link in all_links if link['target_url'] == target_url]
        
        for link in target_links:
            csv_data.append({
                'A_ç•ªå·': row_number,
                'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«': target_title,
                'C_URL': target_url,
                'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«': link['source_title'],
                'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL': link['source_url'],
                'F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ': link['anchor_text']
            })
        
        row_number += 1
    
    # å­¤ç«‹ãƒšãƒ¼ã‚¸ã‚‚è¿½åŠ 
    for url, info in pages.items():
        if url not in target_counts:
            csv_data.append({
                'A_ç•ªå·': row_number,
                'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«': info['title'],
                'C_URL': url,
                'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«': '',
                'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL': '',
                'F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ': ''
            })
            row_number += 1
    
    progress_bar.progress(1.0)
    status_text.text(f"åˆ†æå®Œäº†: {len(pages)}ãƒšãƒ¼ã‚¸ã€{len(all_links)}ãƒªãƒ³ã‚¯ã‚’æ¤œå‡º")
    
    return pd.DataFrame(csv_data)

def create_network_visualization(df, site_name, top_n=40):
    """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã‚’ä½œæˆ"""
    if not HAS_PYVIS:
        return None
    
    try:
        edges_df = df[
            (df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].astype(str) != "") &
            (df['C_URL'].astype(str) != "")
        ].copy()

        if edges_df.empty:
            return None

        # è¢«ãƒªãƒ³ã‚¯æ•°è¨ˆç®—
        in_counts = edges_df.groupby('C_URL').size().sort_values(ascending=False)
        top_targets = set(in_counts.head(top_n).index)
        
        # ä¸Šä½ãƒšãƒ¼ã‚¸é–“ã®ãƒªãƒ³ã‚¯ã®ã¿
        sub_edges = edges_df[
            edges_df['C_URL'].isin(top_targets) & 
            edges_df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].isin(top_targets)
        ]
        
        if sub_edges.empty:
            return None
        
        # ã‚¨ãƒƒã‚¸é›†ç´„
        agg = sub_edges.groupby(['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL', 'C_URL']).size().reset_index(name='weight')
        
        # URLâ†’ã‚¿ã‚¤ãƒˆãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°
        url2title = {}
        for _, r in df[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«','C_URL']].drop_duplicates().iterrows():
            if r['C_URL']:
                url2title[r['C_URL']] = r['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«']
        for _, r in df[['D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«','E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL']].drop_duplicates().iterrows():
            if r['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL']:
                url2title.setdefault(r['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'], r['D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«'])

        # PyVisãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä½œæˆ
        net = Network(height="800px", width="100%", directed=True, bgcolor="#ffffff")
        
        net.barnes_hut(
            gravity=-8000,
            central_gravity=0.3,
            spring_length=200,
            spring_strength=0.05,
            damping=0.15
        )

        # ãƒãƒ¼ãƒ‰è¿½åŠ 
        nodes = set(agg['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL']).union(set(agg['C_URL']))
        for u in nodes:
            size = max(15, min(50, int(15 + (in_counts.get(u, 0) * 2))))
            label = str(url2title.get(u, u))[:20]
            if len(label) < len(str(url2title.get(u, u))):
                label += "..."
            
            net.add_node(
                u,
                label=label,
                title=f"{url2title.get(u, u)}<br>{u}",
                size=size
            )

        # ã‚¨ãƒƒã‚¸è¿½åŠ 
        for _, r in agg.iterrows():
            net.add_edge(
                r['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'], 
                r['C_URL'], 
                width=min(int(r['weight']), 10)
            )

        # HTMLç”Ÿæˆ
        with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False) as tmp:
            net.save_graph(tmp.name)
            with open(tmp.name, 'r', encoding='utf-8') as f:
                html_content = f.read()
            os.unlink(tmp.name)
            
        return html_content
        
    except Exception:
        return None

def main():
    st.title("ğŸ”— å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ«")
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–
    if 'analysis_data' not in st.session_state:
        st.session_state.analysis_data = None
    if 'site_name' not in st.session_state:
        st.session_state.site_name = None

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.header("ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é¸æŠ
        data_source = st.radio(
            "åˆ†ææ–¹æ³•ã‚’é¸æŠ",
            ["ã‚ªãƒ³ãƒ©ã‚¤ãƒ³è‡ªå‹•åˆ†æ", "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"]
        )
        
        if data_source == "ã‚ªãƒ³ãƒ©ã‚¤ãƒ³è‡ªå‹•åˆ†æ":
            st.subheader("ã‚µã‚¤ãƒˆé¸æŠ")
            selected_site = st.selectbox("åˆ†æå¯¾è±¡ã‚µã‚¤ãƒˆ", list(SITES_CONFIG.keys()))
            
            max_pages = st.slider("æœ€å¤§ãƒšãƒ¼ã‚¸æ•°", 50, 500, 200, 50)
            
            if st.button("ğŸš€ åˆ†æé–‹å§‹"):
                config = SITES_CONFIG[selected_site]
                with st.spinner(f"{selected_site}ã‚’åˆ†æä¸­..."):
                    df = crawl_site(selected_site, config["base_url"], max_pages)
                    if not df.empty:
                        st.session_state.analysis_data = df
                        st.session_state.site_name = selected_site
                        st.success("åˆ†æå®Œäº†ï¼")
                        st.rerun()
        
        else:
            uploaded_file = st.file_uploader("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['csv'])
            if uploaded_file:
                try:
                    df = pd.read_csv(uploaded_file, encoding="utf-8-sig").fillna("")
                    st.session_state.analysis_data = df
                    st.session_state.site_name = uploaded_file.name
                    st.success("CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼")
                except Exception as e:
                    st.error(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        # è¨­å®š
        st.header("ğŸ› ï¸ è¡¨ç¤ºè¨­å®š")
        network_top_n = st.slider("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ï¼šä¸Šä½Nä»¶", 10, 100, 40, 5)

    # ãƒ¡ã‚¤ãƒ³ç”»é¢
    if st.session_state.analysis_data is None:
        st.info("ğŸ‘ˆ ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return

    df = st.session_state.analysis_data
    site_name = st.session_state.site_name

    # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    csv_string = df.to_csv(index=False, encoding="utf-8-sig")
    st.download_button(
        label="ğŸ“¥ åˆ†æçµæœã‚’CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv_string,
        file_name=f"{site_name}_analysis.csv",
        mime="text/csv"
    )

    # ã‚¿ãƒ–è¡¨ç¤º
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ“Š ãƒ‡ãƒ¼ã‚¿ä¸€è¦§",
        "ğŸ›ï¸ ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸", 
        "ğŸ§© ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ",
        "ğŸ§­ å­¤ç«‹è¨˜äº‹",
        "ğŸ“ˆ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³"
    ])

    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
    pages_df = df[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL']].drop_duplicates().copy()
    has_src = (df['D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«'].astype(str) != "") & (df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].astype(str) != "")
    inbound_counts = df[has_src].groupby('C_URL').size()
    pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'] = pages_df['C_URL'].map(inbound_counts).fillna(0).astype(int)
    pages_df = pages_df.sort_values('è¢«ãƒªãƒ³ã‚¯æ•°', ascending=False).reset_index(drop=True)

    with tab1:
        st.header("ğŸ“Š å…¨ãƒ‡ãƒ¼ã‚¿")
        st.dataframe(df, use_container_width=True)

    with tab2:
        st.header("ğŸ›ï¸ ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸åˆ†æ")
        st.dataframe(pages_df, use_container_width=True)
        
        if len(pages_df) > 0:
            top20 = pages_df.head(20)
            fig = px.bar(
                top20.sort_values('è¢«ãƒªãƒ³ã‚¯æ•°'), 
                x='è¢«ãƒªãƒ³ã‚¯æ•°', 
                y='B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«',
                orientation='h',
                title="è¢«ãƒªãƒ³ã‚¯æ•° TOP20",
                height=600
            )
            fig.update_layout(yaxis={'categoryorder': 'total ascending'})
            st.plotly_chart(fig, use_container_width=True)

    with tab3:
        st.header("ğŸ§© ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æï¼ˆã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆï¼‰")
        
        anchors = df[df['F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'].astype(str) != '']
        if not anchors.empty:
            anchor_counts = Counter(anchors['F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'])
            anchor_df = pd.DataFrame(anchor_counts.most_common(), columns=['ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ', 'é »åº¦'])
            
            total = sum(anchor_counts.values())
            hhi = sum((c/total)**2 for c in anchor_counts.values()) if total else 0.0
            diversity = 1 - hhi
            
            st.metric("ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆå¤šæ§˜æ€§", f"{diversity:.3f}")
            st.dataframe(anchor_df, use_container_width=True)
        else:
            st.warning("ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    with tab4:
        st.header("ğŸ§­ å­¤ç«‹è¨˜äº‹")
        
        isolated = pages_df[pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'] == 0].copy()
        if not isolated.empty:
            st.warning(f"å­¤ç«‹è¨˜äº‹ãŒ {len(isolated)} ä»¶è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
            st.dataframe(isolated[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL']], use_container_width=True)
        else:
            st.success("å­¤ç«‹è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“")

    with tab5:
        st.header("ğŸ“ˆ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³")
        
        if HAS_PYVIS:
            with st.spinner("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã‚’ç”Ÿæˆä¸­..."):
                html_content = create_network_visualization(df, site_name, network_top_n)
                
                if html_content:
                    st.components.v1.html(html_content, height=820)
                    st.success(f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã‚’è¡¨ç¤ºã—ã¾ã—ãŸï¼ˆä¸Šä½{network_top_n}ä»¶ï¼‰")
                else:
                    st.error("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        else:
            st.error("PyVisãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")

if __name__ == "__main__":
    main()

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ« - Streamlitç‰ˆ (ãƒ­ãƒ¼ã‚«ãƒ«åˆ†ææ©Ÿèƒ½çµ±åˆ)
- ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ / ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ï¼ˆã‚¢ãƒ³ã‚«ãƒ¼ï¼‰ / å­¤ç«‹è¨˜äº‹ã‚’å…¨ä»¶å‡ºåŠ›
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ï¼ˆé™çš„ï¼šplotlyã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ï¼špyvisï¼‰
- ãƒ­ãƒ¼ã‚«ãƒ«åˆ†æå®Ÿè¡Œæ©Ÿèƒ½ or CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
- HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
import os
import sys
import json
import math
import tempfile
from pathlib import Path
from collections import Counter
from datetime import datetime
from urllib.parse import urlparse
import base64
from io import BytesIO
import zipfile
import subprocess

# PyVisï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    from pyvis.network import Network
    HAS_PYVIS = True
except ImportError:
    HAS_PYVIS = False

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ğŸ”— å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ«",
    page_icon="ğŸ”—",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        margin-bottom: 2rem;
        color: #1f77b4;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
    }
    .success-box {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        border-radius: 0.25rem;
        padding: 0.75rem;
        margin: 1rem 0;
    }
    .warning-box {
        background-color: #fff3cd;
        border: 1px solid #ffeaa7;
        border-radius: 0.25rem;
        padding: 0.75rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° (å¤‰æ›´ãªã—)
@st.cache_data
def safe_str(s):
    return s if isinstance(s, str) else ""

@st.cache_data
def normalize_url(u, default_scheme="https", base_domain=None):
    if not isinstance(u, str) or not u.strip():
        return ""
    u = u.strip()
    try:
        from urllib.parse import urlparse, urlunparse
        p = urlparse(u)
        if not p.netloc and base_domain:
            path = u if u.startswith("/") else f"/{u}"
            u = f"{default_scheme}://{base_domain}{path}"
            p = urlparse(u)
        elif not p.netloc:
            return u
        scheme = p.scheme or default_scheme
        netloc = p.netloc.lower().replace("www.", "")
        path = p.path or "/"
        return urlunparse((scheme, netloc, path, p.params, p.query, ""))
    except Exception:
        return u

def detect_site_info(filename, df):
    site_map = {
        'arigataya': 'ã‚ã‚ŠãŒãŸã‚„',
        'kau-ru': 'ã‚«ã‚¦ãƒ¼ãƒ«',
        'kaitori-life': 'è²·å–LIFE',
        'friendpay': 'ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒšã‚¤',
        'crecaeru': 'ã‚¯ãƒ¬ã‹ãˆã‚‹',
    }
    site_name = "Unknown Site"
    for key, name in site_map.items():
        if key in filename.lower():
            site_name = name
            break
    
    domains = [urlparse(u).netloc.replace("www.","") for u in df['C_URL'].dropna() if isinstance(u, str) and 'http' in u]
    site_domain = Counter(domains).most_common(1)[0][0] if domains else None
    
    return site_name, site_domain

def create_download_link(content, filename, link_text="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"):
    if isinstance(content, str):
        content = content.encode('utf-8')
    b64 = base64.b64encode(content).decode()
    href = f'<a href="data:application/octet-stream;base64,{b64}" download="{filename}" style="text-decoration: none; background-color: #1f77b4; color: white; padding: 0.5rem 1rem; border-radius: 0.25rem; display: inline-block;">{link_text}</a>'
    return href

def generate_html_table(title, columns, rows):
    def esc(x): return str(x).replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
    html = f"<!doctype html><meta charset='utf-8'><title>{esc(title)}</title>"
    html += "<style>body{font-family:sans-serif;padding:16px}table{border-collapse:collapse;width:100%}th,td{border:1px solid #ddd;padding:8px;font-size:14px}th{background:#f7f7f7;font-weight:bold}a{color:#1565c0;text-decoration:none}a:hover{text-decoration:underline}</style>"
    html += f"<h1>{esc(title)}</h1><table><thead><tr>{''.join(f'<th>{esc(c)}</th>' for c in columns)}</tr></thead><tbody>"
    for row in rows:
        html += "<tr>"
        for i, cell in enumerate(row):
            val = esc(cell)
            if "URL" in columns[i].upper() and val.startswith("http"):
                val = f"<a href='{val}' target='_blank'>{val}</a>"
            html += f"<td>{val}</td>"
        html += "</tr>"
    html += "</tbody></table>"
    return html

# ãƒ¡ã‚¤ãƒ³é–¢æ•°
def main():
    st.markdown('<div class="main-header">ğŸ”— å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ«</div>', unsafe_allow_html=True)
    
    # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
    with st.sidebar:
        st.header("ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®š")
        
        analysis_source = st.radio(
            "ã©ã¡ã‚‰ã®æ–¹æ³•ã§åˆ†æã—ã¾ã™ã‹ï¼Ÿ",
            ("ãƒ­ãƒ¼ã‚«ãƒ«ã§æ–°è¦åˆ†æã‚’å®Ÿè¡Œ", "æ—¢å­˜ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"),
            key="analysis_source"
        )
        
        uploaded_file_obj = None
        
        if analysis_source == "ãƒ­ãƒ¼ã‚«ãƒ«ã§æ–°è¦åˆ†æã‚’å®Ÿè¡Œ":
            st.subheader("åˆ†æå¯¾è±¡ã‚µã‚¤ãƒˆ")
            
            # ä¸»ã®ç®¡ç†ã‚µã‚¤ãƒˆãƒªã‚¹ãƒˆ
            available_sites = [
                "arigataya", "kau-ru", "kaitori-life", "friendpay", "crecaeru",
                # "site6", "site7", "site8", "site9", "site10" # å¿…è¦ã«å¿œã˜ã¦è¿½åŠ 
            ]
            
            selected_site = st.selectbox(
                "ã‚µã‚¤ãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„",
                options=available_sites
            )
            
            # ãƒ­ãƒ¼ã‚«ãƒ«åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ‘ã‚¹
            # ã“ã®Streamlitã‚¢ãƒ—ãƒªã¨åŒã˜éšå±¤ã«ã‚ã‚‹ã“ã¨ã‚’æƒ³å®š
            analyzer_script_path = "local_analyzer_cli.py"

            if st.button("ğŸš€ åˆ†æã‚’å®Ÿè¡Œã™ã‚‹", key="run_local_analysis"):
                if not os.path.exists(analyzer_script_path):
                    st.error(f"åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {analyzer_script_path}\nã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒã˜éšå±¤ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚")
                else:
                    with st.spinner(f"ã€Œ{selected_site}ã€ã®å†…éƒ¨ãƒªãƒ³ã‚¯ã‚’åˆ†æä¸­ã§ã™...ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚"):
                        try:
                            output_csv_path = os.path.join(tempfile.gettempdir(), f"{selected_site}_links_{datetime.now().strftime('%Y%m%d%H%M%S')}.csv")
                            
                            python_executable = sys.executable
                            command = [python_executable, analyzer_script_path, selected_site, output_csv_path]
                            
                            process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding='utf-8')
                            
                            log_placeholder = st.empty()
                            logs = ""
                            while True:
                                output = process.stdout.readline()
                                if output == '' and process.poll() is not None:
                                    break
                                if output:
                                    logs += output
                                    log_placeholder.code(logs, language="log")
                            
                            stderr = process.communicate()[1]

                            if process.returncode == 0:
                                st.success(f"ã€Œ{selected_site}ã€ã®åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                                st.session_state['last_analyzed_csv'] = output_csv_path
                                # æˆåŠŸã—ãŸã‚‰ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿ã—ã¦çµæœã‚’è¡¨ç¤º
                                st.experimental_rerun()
                            else:
                                st.error("åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
                                st.error("ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°:")
                                st.code(stderr, language="log")

                        except Exception as e:
                            st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

        else: # CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®å ´åˆ
            uploaded_file_obj = st.file_uploader(
                "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
                type=['csv'],
                help="å†…éƒ¨ãƒªãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„"
            )

        st.header("ğŸ› ï¸ åˆ†æè¨­å®š")
        network_top_n = st.slider("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ï¼šä¸Šä½Nä»¶", 10, 100, 40, 5)
        st.header("ğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆè¨­å®š")
        auto_download = st.checkbox("HTMLãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆ", value=True)

    # --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ ---
    
    # åˆ†æã«ä½¿ç”¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ±ºå®š
    target_file = None
    if uploaded_file_obj:
        target_file = uploaded_file_obj
    elif 'last_analyzed_csv' in st.session_state and os.path.exists(st.session_state['last_analyzed_csv']):
        target_file = st.session_state['last_analyzed_csv']
        st.info(f"ãƒ­ãƒ¼ã‚«ãƒ«ã§åˆ†æã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{os.path.basename(target_file)}ã€ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™ã€‚")

    if target_file is None:
        st.info("ğŸ‘† ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’é¸æŠã—ã€åˆ†æã‚’å®Ÿè¡Œã™ã‚‹ã‹ã€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        st.subheader("ğŸ“‹ æœŸå¾…ã•ã‚Œã‚‹CSVãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ")
        st.code("""A_ç•ªå·,B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«,C_URL,D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«,E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL,F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ""")
        return

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    try:
        if isinstance(target_file, str):
            df = pd.read_csv(target_file, encoding="utf-8-sig").fillna("")
            filename_for_detect = os.path.basename(target_file)
        else:
            df = pd.read_csv(target_file, encoding="utf-8-sig").fillna("")
            filename_for_detect = target_file.name

        expected_columns = ['A_ç•ªå·', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL', 'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL', 'F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ']
        if not all(col in df.columns for col in expected_columns):
            st.error(f"âŒ å¿…è¦ãªåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚æœŸå¾…ã•ã‚Œã‚‹åˆ—: {expected_columns}")
            return
        
        site_name, site_domain = detect_site_info(filename_for_detect, df)
        
        df['C_URL'] = df['C_URL'].apply(lambda u: normalize_url(u, base_domain=site_domain))
        df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'] = df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].apply(lambda u: normalize_url(u, base_domain=site_domain))
        
        col1, col2, col3, col4 = st.columns(4)
        unique_pages = df[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL']].drop_duplicates()
        has_link = (df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].astype(str) != "")
        unique_anchors = df[df['F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'].astype(str) != '']['F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'].nunique()
        col1.metric("ğŸ“Š ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", len(df))
        col2.metric("ğŸ“„ ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒšãƒ¼ã‚¸æ•°", len(unique_pages))
        col3.metric("ğŸ”— å†…éƒ¨ãƒªãƒ³ã‚¯æ•°", int(has_link.sum()))
        col4.metric("ğŸ·ï¸ ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¢ãƒ³ã‚«ãƒ¼æ•°", unique_anchors)
        
        st.markdown(f"""<div class="success-box">
            <strong>ğŸ“ èª­ã¿è¾¼ã¿å®Œäº†:</strong> {filename_for_detect}<br>
            <strong>ğŸŒ ã‚µã‚¤ãƒˆ:</strong> {site_name}<br>
            <strong>ğŸ”— ãƒ‰ãƒ¡ã‚¤ãƒ³:</strong> {site_domain or 'ä¸æ˜'}
        </div>""", unsafe_allow_html=True)

        # ---------------------------------------------------------------------
        # ä»¥é™ã€ä¸»ã‹ã‚‰ã„ãŸã ã„ãŸå…ƒã® `main.py` ã®åˆ†æãƒ»è¡¨ç¤ºãƒ­ã‚¸ãƒƒã‚¯ã‚’ãã®ã¾ã¾æµç”¨ã—ã¾ã™
        # ---------------------------------------------------------------------

        tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ›ï¸ ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸", "ğŸ§© ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ", "ğŸ§­ å­¤ç«‹è¨˜äº‹", "ğŸ“ˆ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³", "ğŸ“Š ç·åˆãƒ¬ãƒãƒ¼ãƒˆ"])
        
        pages_df = df[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL']].drop_duplicates().copy()
        inbound_df = df[(df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].astype(str) != "") & (df['C_URL'].astype(str) != "")]
        inbound_counts = inbound_df.groupby('C_URL').size()
        pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'] = pages_df['C_URL'].map(inbound_counts).fillna(0).astype(int)
        pages_df = pages_df.sort_values(['è¢«ãƒªãƒ³ã‚¯æ•°', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«'], ascending=[False, True]).reset_index(drop=True)

        # Tab 1: ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸åˆ†æ
        with tab1:
            st.header("ğŸ›ï¸ ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸åˆ†æ")
            top_n_pillar = st.slider("è¡¨ç¤ºä»¶æ•°", 5, 50, 20, key="pillar_top_n")
            top_pages = pages_df.head(top_n_pillar)
            fig = px.bar(top_pages.head(15).sort_values('è¢«ãƒªãƒ³ã‚¯æ•°'), x='è¢«ãƒªãƒ³ã‚¯æ•°', y='B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', orientation='h', title="è¢«ãƒªãƒ³ã‚¯æ•° TOP15")
            st.plotly_chart(fig, use_container_width=True)
            st.subheader("ğŸ“‹ ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ä¸€è¦§")
            st.dataframe(top_pages[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL', 'è¢«ãƒªãƒ³ã‚¯æ•°']], use_container_width=True)

        # Tab 2: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ
        with tab2:
            st.header("ğŸ§© ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æï¼ˆã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆï¼‰")
            anchor_df = df[df['F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'].astype(str) != '']
            anchor_counts = Counter(anchor_df['F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'])
            if anchor_counts:
                total_anchors = sum(anchor_counts.values())
                diversity_index = 1 - sum((c/total_anchors)**2 for c in anchor_counts.values())
                col1, col2, col3 = st.columns(3)
                col1.metric("ğŸ”¢ ç·ã‚¢ãƒ³ã‚«ãƒ¼æ•°", total_anchors)
                col2.metric("ğŸ·ï¸ ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¢ãƒ³ã‚«ãƒ¼æ•°", len(anchor_counts))
                col3.metric("ğŸ“Š å¤šæ§˜æ€§æŒ‡æ•°", f"{diversity_index:.3f}", help="1ã«è¿‘ã„ã»ã©å¤šæ§˜")
                
                top_anchors = pd.DataFrame(anchor_counts.most_common(15), columns=['ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ', 'é »åº¦']).sort_values('é »åº¦')
                fig = px.bar(top_anchors, x='é »åº¦', y='ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ', orientation='h', title="ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆé »åº¦ TOP15")
                st.plotly_chart(fig, use_container_width=True)

                st.subheader("ğŸ“‹ ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆä¸€è¦§")
                st.dataframe(pd.DataFrame(anchor_counts.most_common(), columns=['ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ', 'é »åº¦']), use_container_width=True)
            else:
                st.warning("âš ï¸ ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

        # Tab 3: å­¤ç«‹è¨˜äº‹
        with tab3:
            st.header("ğŸ§­ å­¤ç«‹è¨˜äº‹åˆ†æ")
            isolated_pages = pages_df[pages_df['è¢«ãƒªãƒ³ã‚¯æ•°'] == 0].copy()
            if not isolated_pages.empty:
                st.metric("ğŸï¸ å­¤ç«‹è¨˜äº‹æ•°", len(isolated_pages))
                st.dataframe(isolated_pages[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL']], use_container_width=True)
            else:
                st.success("ğŸ‰ å­¤ç«‹è¨˜äº‹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼")

        # Tab 4: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³
        with tab4:
            st.header("ğŸ“ˆ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³")
            if HAS_PYVIS:
                st.info("ğŸ”„ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã‚’ç”Ÿæˆä¸­...")
                try:
                    edges_df = df[(df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'] != "") & (df['C_URL'] != "")].copy()
                    top_urls = set(pages_df.head(network_top_n)['C_URL'])
                    sub_edges = edges_df[edges_df['C_URL'].isin(top_urls) & edges_df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'].isin(top_urls)]
                    
                    if not sub_edges.empty:
                        agg = sub_edges.groupby(['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL', 'C_URL']).size().reset_index(name='weight')
                        
                        url_map = pd.concat([
                            df[['C_URL', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«']].rename(columns={'C_URL':'url', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«':'title'}),
                            df[['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL', 'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«']].rename(columns={'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL':'url', 'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«':'title'})
                        ]).drop_duplicates('url').set_index('url')['title'].to_dict()

                        net = Network(height="800px", width="100%", directed=True, notebook=True)
                        net.set_options('{"physics":{"barnesHut":{"gravitationalConstant":-8000,"springLength":150,"avoidOverlap":0.1}}}')

                        nodes = set(agg['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL']).union(set(agg['C_URL']))
                        for u in nodes:
                            net.add_node(u, label=str(url_map.get(u, u))[:20], title=url_map.get(u, u), size=10 + math.log2(inbound_counts.get(u, 0) + 1) * 5)
                        
                        for _, r in agg.iterrows():
                            net.add_edge(r['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'], r['C_URL'], value=r['weight'])
                        
                        with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False) as tmp:
                            net.save_graph(tmp.name)
                            with open(tmp.name, 'r') as f:
                                st.components.v1.html(f.read(), height=820)
                        os.unlink(tmp.name)
                    else:
                        st.warning("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æç”»ã™ã‚‹ãŸã‚ã®ãƒªãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")

                except Exception as e:
                    st.error(f"âŒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            else:
                st.error("âŒ pyvisãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™ã€‚`pip install pyvis`ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
        
        # Tab 5: ç·åˆãƒ¬ãƒãƒ¼ãƒˆ
        with tab5:
            st.header("ğŸ“Š ç·åˆãƒ¬ãƒãƒ¼ãƒˆ")
            st.write("å…¨ã¦ã®åˆ†æçµæœã‚’ã¾ã¨ã‚ãŸãƒ¬ãƒãƒ¼ãƒˆã§ã™ã€‚")
            if st.button("ğŸ“¥ ç·åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", key="download_summary"):
                zip_buffer = BytesIO()
                with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zf:
                    # ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸
                    rows = [[i, r['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«'], r['C_URL'], r['è¢«ãƒªãƒ³ã‚¯æ•°']] for i, r in pages_df.iterrows()]
                    zf.writestr("1_pillar_report.html", generate_html_table(f"{site_name} ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸", ["#", "ã‚¿ã‚¤ãƒˆãƒ«", "URL", "è¢«ãƒªãƒ³ã‚¯æ•°"], rows))
                    # ã‚¢ãƒ³ã‚«ãƒ¼
                    if anchor_counts:
                        rows = [[i, a, c] for i, (a,c) in enumerate(anchor_counts.most_common())]
                        zf.writestr("2_anchor_report.html", generate_html_table(f"{site_name} ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ", ["#", "ã‚¢ãƒ³ã‚«ãƒ¼", "é »åº¦"], rows))
                    # å­¤ç«‹è¨˜äº‹
                    if not isolated_pages.empty:
                        rows = [[i, r['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«'], r['C_URL']] for i, r in isolated_pages.iterrows()]
                        zf.writestr("3_isolated_report.html", generate_html_table(f"{site_name} å­¤ç«‹è¨˜äº‹", ["#", "ã‚¿ã‚¤ãƒˆãƒ«", "URL"], rows))
                
                zip_buffer.seek(0)
                b64 = base64.b64encode(zip_buffer.getvalue()).decode()
                filename = f"report_{site_name}_{datetime.now().strftime('%Y%m%d')}.zip"
                href = f'<a href="data:application/zip;base64,{b64}" download="{filename}" style="text-decoration: none; background-color: #28a745; color: white; padding: 0.75rem 1.5rem; border-radius: 0.25rem; display: inline-block; font-weight: bold;">ğŸ“¦ ç·åˆãƒ¬ãƒãƒ¼ãƒˆ(ZIP)ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰</a>'
                st.markdown(href, unsafe_allow_html=True)
    
    except Exception as e:
        st.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.exception(e)

if __name__ == "__main__":
    main()

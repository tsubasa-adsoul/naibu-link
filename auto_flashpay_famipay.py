import tkinter as tk
from tkinter import messagebox, filedialog
import customtkinter as ctk
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import time
import re
import threading
import csv
import sys
import os
from datetime import datetime

ctk.set_appearance_mode("light")
ctk.set_default_color_theme("blue")

class LinkAnalyzerApp:
    def __init__(self):
        self.root = ctk.CTk()
        self.root.title("ğŸ”— famipayå°‚ç”¨å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆå…¨è‡ªå‹•ç‰ˆï¼‰")
        self.root.geometry("1200x800")

        self.pages = {}
        self.links = []
        self.detailed_links = []  # è©³ç´°ãƒªãƒ³ã‚¯æƒ…å ±ç”¨

        self.setup_ui()
        
        # å…¨è‡ªå‹•åŒ–ï¼šèµ·å‹•1ç§’å¾Œã«è‡ªå‹•ã§åˆ†æé–‹å§‹
        self.root.after(1000, self.auto_start_analysis)

    def setup_ui(self):
        self.main_frame = ctk.CTkScrollableFrame(self.root)
        self.main_frame.pack(fill="both", expand=True, padx=20, pady=20)

        title_label = ctk.CTkLabel(self.main_frame, text="ğŸ”— famipayå°‚ç”¨å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆå…¨è‡ªå‹•ç‰ˆï¼‰",
                                   font=ctk.CTkFont(size=32, weight="bold"))
        title_label.pack(pady=20)

        self.url_entry = ctk.CTkEntry(self.main_frame, placeholder_text="https://flashpay.jp/famipay/", width=500)
        self.url_entry.pack(pady=10)
        self.url_entry.insert(0, "https://flashpay.jp/famipay/")

        self.analyze_button = ctk.CTkButton(self.main_frame, text="åˆ†æé–‹å§‹", command=self.start_analysis)
        self.analyze_button.pack(pady=10)

        self.status_label = ctk.CTkLabel(self.main_frame, text="")
        self.status_label.pack(pady=10)

        self.progress_bar = ctk.CTkProgressBar(self.main_frame, width=400)
        self.progress_bar.pack(pady=5)
        self.progress_bar.stop()

        self.result_frame = ctk.CTkScrollableFrame(self.main_frame, height=400)
        self.result_frame.pack(fill="both", expand=True, pady=20)

        # è©³ç´°CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒœã‚¿ãƒ³ã®ã¿ï¼ˆå¾“æ¥CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã¯å‰Šé™¤ï¼‰
        button_frame = ctk.CTkFrame(self.main_frame)
        button_frame.pack(pady=10)
        
        self.detailed_export_btn = ctk.CTkButton(button_frame, text="è©³ç´°CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ", command=self.export_detailed_csv)
        self.detailed_export_btn.pack(side="left", padx=5)

    def auto_start_analysis(self):
        """å…¨è‡ªå‹•åŒ–ï¼šè‡ªå‹•åˆ†æé–‹å§‹"""
        print("=== famipay (flashpay.jp/famipay/) è‡ªå‹•åˆ†æé–‹å§‹ ===")
        self.start_analysis()

    def start_analysis(self):
        url = self.url_entry.get().strip()
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url

        self.analyze_button.configure(state="disabled")
        self.status_label.configure(text="ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­...")
        self.progress_bar.start()

        thread = threading.Thread(target=self.analyze_site, args=(url,))
        thread.daemon = True
        thread.start()

    def extract_links_for_crawling(self, soup):
        """ãƒšãƒ¼ã‚¸åé›†ç”¨ï¼šå…¨ä½“ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’å–å¾—"""
        all_links = soup.find_all('a', href=True)
        valid_links = []
        
        for link in all_links:
            href = link.get('href', '')
            # åŸºæœ¬çš„ãªé™¤å¤–ã®ã¿
            if (href and 
                not href.startswith('#') and 
                '/site/' not in href and
                not any(domain in href for domain in ['facebook.com', 'twitter.com', 'x.com', 'line.me', 'hatena.ne.jp'])):
                valid_links.append(link)
        
        return [a['href'] for a in valid_links]

    def extract_links(self, soup):
        """ãƒªãƒ³ã‚¯åˆ†æç”¨ï¼šè¨˜äº‹æœ¬æ–‡ã®ã¿ã‹ã‚‰å–å¾—ï¼ˆè©³ç´°æƒ…å ±ä»˜ãï¼‰"""
        content_area = soup.select_one('.entry-content')
        if not content_area:
            return []
        
        all_links = content_area.find_all('a', href=True)
        valid_links = []
        
        for link in all_links:
            href = link.get('href', '')
            
            # æœ€ä½é™ã®é™¤å¤–ã®ã¿
            if (href and 
                not href.startswith('#') and 
                '/site/' not in href and
                'respond' not in href and
                self.is_internal(href, 'flashpay.jp')):
                
                # ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                anchor_text = link.get_text(strip=True) or link.get('title', '') or '[ãƒªãƒ³ã‚¯]'
                valid_links.append({
                    'url': href,
                    'anchor_text': anchor_text[:100]  # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
                })
        
        return valid_links

    def is_content(self, url):
        normalized_url = self.normalize_url(url)
        path = urlparse(normalized_url).path.lower().split('?')[0].rstrip('/')
        # /famipay/ ç›´ä¸‹ã®è¨˜äº‹ ã¾ãŸã¯ ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸
        if path == '/famipay':
            return True
        return path.startswith('/famipay/') and re.match(r'^/famipay/[a-z0-9\-]+$', path)

    def is_noindex_page(self, soup):
        try:
            meta_robots = soup.find('meta', attrs={'name': 'robots'})
            if meta_robots and 'noindex' in meta_robots.get('content', '').lower():
                return True
            noindex_meta = soup.find('meta', attrs={'name': 'googlebot', 'content': lambda x: x and 'noindex' in x.lower()})
            if noindex_meta:
                return True
            return False
        except:
            return False

    def normalize_url(self, url):
        parsed = urlparse(url)
        scheme = 'https'
        netloc = parsed.netloc.replace('www.', '')
        path = re.sub(r'/+', '/', parsed.path.rstrip('/'))
        return f"{scheme}://{netloc}{path}"

    def is_internal(self, url, domain):
        parsed_url = urlparse(url)
        url_domain = parsed_url.netloc.replace('www.', '')
        target_domain = domain.replace('www.', '')
        return url_domain == target_domain

    def analyze_site(self, base_url):
        try:
            pages = {}
            links = []
            detailed_links = []  # è©³ç´°ãƒªãƒ³ã‚¯æƒ…å ±ç”¨
            visited = set()
            to_visit = [self.normalize_url(base_url)]
            domain = urlparse(base_url).netloc
            processed_links = set()  # é‡è¤‡é™¤å»ç”¨

            session = requests.Session()
            session.headers.update({'User-Agent': 'Mozilla/5.0'})

            # ç¬¬1æ®µéš: ãƒšãƒ¼ã‚¸ã‚’åé›†
            while to_visit and len(pages) < 500:
                url = to_visit.pop(0)
                normalized_url = self.normalize_url(url)
                
                if normalized_url in visited:
                    continue
                    
                try:
                    response = session.get(url, timeout=10)
                    if response.status_code != 200:
                        continue
                        
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    if self.is_noindex_page(soup):
                        visited.add(normalized_url)
                        continue

                    # ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’ä¿å­˜
                    title = soup.title.string.strip() if soup.title and soup.title.string else normalized_url
                    
                    # famipay | ãƒ•ã‚¡ãƒŸãƒšã‚¤ ãªã©ã®ã‚µã‚¤ãƒˆåã‚’é™¤å»
                    title = re.sub(r'\s*[|\-]\s*.*(famipay|ãƒ•ã‚¡ãƒŸãƒšã‚¤|flashpay|ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ãƒšã‚¤).*$', '', title, flags=re.IGNORECASE)
                    
                    pages[normalized_url] = {'title': title, 'outbound_links': []}

                    # æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’æ¢ç´¢ãƒªã‚¹ãƒˆã«è¿½åŠ 
                    extracted_links = self.extract_links_for_crawling(soup)
                    for link in extracted_links:
                        normalized_link = self.normalize_url(link)
                        
                        # /site/ ã‚’å«ã‚€ãƒªãƒ³ã‚¯ã¯é™¤å¤–
                        if '/site/' in normalized_link:
                            continue
                            
                        if (self.is_internal(normalized_link, domain) and 
                            self.is_content(normalized_link) and 
                            normalized_link not in visited and 
                            normalized_link not in to_visit):
                            to_visit.append(normalized_link)

                    visited.add(normalized_url)

                    self.root.after(0, lambda text=f"{len(pages)}ä»¶ç›®: {normalized_url[:60]}...": self.status_label.configure(text=text))
                    self.root.after(0, lambda: self.progress_bar.set(min(len(pages) / 500, 1.0)))
                    time.sleep(0.1)
                    
                except Exception as e:
                    print(f"ã‚¨ãƒ©ãƒ¼: {url} - {e}")
                    continue

            # ç¬¬2æ®µéš: ãƒªãƒ³ã‚¯é–¢ä¿‚ã‚’æ§‹ç¯‰ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            print(f"ãƒªãƒ³ã‚¯é–¢ä¿‚æ§‹ç¯‰é–‹å§‹: {len(pages)}ãƒšãƒ¼ã‚¸ã‚’å‡¦ç†")
            session = requests.Session()
            session.headers.update({'User-Agent': 'Mozilla/5.0'})
            
            for url in list(pages.keys()):
                try:
                    response = session.get(url, timeout=10)
                    if response.status_code != 200:
                        continue
                        
                    soup = BeautifulSoup(response.text, 'html.parser')
                    extracted_links = self.extract_links(soup)  # è©³ç´°æƒ…å ±ä»˜ããƒªãƒ³ã‚¯å–å¾—
                    
                    # creditcardè¨˜äº‹ã®å ´åˆã¯è©³ç´°å‡ºåŠ›
                    if 'creditcard' in url:
                        print(f"ãƒ‡ãƒãƒƒã‚°: {url} ã§{len(extracted_links)}å€‹ã®ãƒªãƒ³ã‚¯ã‚’æ¤œå‡º:")
                        for link in extracted_links:
                            print(f"  - {link}")
                    
                    # ã“ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ã®å†…éƒ¨ãƒªãƒ³ã‚¯ã‚’è¨˜éŒ²ï¼ˆã‚¯ãƒ­ãƒ¼ãƒ«ã—ãŸè¨˜äº‹ã®ã¿ï¼‰
                    for link_data in extracted_links:
                        link_url = link_data['url']
                        anchor_text = link_data['anchor_text']
                        normalized_link = self.normalize_url(link_url)

                        # é™¤å¤–æ¡ä»¶1: /site/ ã‚’å«ã‚€ãƒªãƒ³ã‚¯
                        if '/site/' in normalized_link:
                            continue

                        # é™¤å¤–æ¡ä»¶2: ã‚¯ãƒ­ãƒ¼ãƒ«ã—ãŸè¨˜äº‹ã«å«ã¾ã‚Œã¦ã„ãªã„ãƒªãƒ³ã‚¯ã¯é™¤å¤–
                        if normalized_link not in pages:
                            continue

                        # é™¤å¤–æ¡ä»¶3: åŒã˜ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã¯é™¤å¤–
                        if normalized_link == url:
                            continue

                        if self.is_internal(normalized_link, domain) and self.is_content(normalized_link):
                            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆåŒã˜ã‚½ãƒ¼ã‚¹â†’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ãƒªãƒ³ã‚¯ã¯1å›ã ã‘ï¼‰
                            link_key = (url, normalized_link)
                            if link_key not in processed_links:
                                processed_links.add(link_key)
                                
                                links.append((url, normalized_link))
                                pages[url]['outbound_links'].append(normalized_link)
                                
                                # è©³ç´°ãƒªãƒ³ã‚¯æƒ…å ±ã‚’ä¿å­˜
                                detailed_links.append({
                                    'source_url': url,
                                    'source_title': pages[url]['title'],
                                    'target_url': normalized_link,
                                    'anchor_text': anchor_text
                                })
                                
                                print(f"  {url} -> {normalized_link} (ã‚¢ãƒ³ã‚«ãƒ¼: {anchor_text})")
                            
                except Exception as e:
                    print(f"ãƒªãƒ³ã‚¯è§£æã‚¨ãƒ©ãƒ¼: {url} - {e}")
                    continue

            # è¢«ãƒªãƒ³ã‚¯æ•°ã‚’è¨ˆç®—
            for url in pages:
                pages[url]['inbound_links'] = len(set([src for src, tgt in links if tgt == url]))
                
            print(f"åˆ†æå®Œäº†: {len(pages)}ãƒšãƒ¼ã‚¸ã€{len(links)}ãƒªãƒ³ã‚¯ï¼ˆé‡è¤‡é™¤å»å¾Œï¼‰")

            self.pages = pages
            self.links = links
            self.detailed_links = detailed_links
            self.root.after(0, self.show_results)
            
        except Exception as e:
            print(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            self.root.after(0, lambda: self.status_label.configure(text=f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"))
        finally:
            self.root.after(0, lambda: self.analyze_button.configure(state="normal"))
            self.root.after(0, lambda: self.progress_bar.stop())
            if hasattr(self, 'pages') and self.pages:
                self.root.after(0, lambda: self.status_label.configure(text="åˆ†æå®Œäº†"))
                
                # å…¨è‡ªå‹•åŒ–ï¼šåˆ†æå®Œäº†2ç§’å¾Œã«è‡ªå‹•ã§CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
                self.root.after(2000, self.auto_export_csv)

    def auto_export_csv(self):
        """å…¨è‡ªå‹•åŒ–ï¼šè‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆEXEåŒ–å¯¾å¿œï¼‰"""
        try:
            # EXEåŒ–å¯¾å¿œï¼šå®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
            if hasattr(sys, '_MEIPASS'):
                # PyInstaller ã§EXEåŒ–ã•ã‚ŒãŸå ´åˆ
                base_dir = os.path.dirname(sys.executable)
            else:
                # é€šå¸¸ã®Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œã®å ´åˆ
                base_dir = os.path.dirname(os.path.abspath(__file__))
            
            # æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
            today = datetime.now()
            date_folder = today.strftime("%Y-%m-%d")  # 2025-06-20 å½¢å¼
            folder_path = os.path.join(base_dir, date_folder)
            
            # ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
            os.makedirs(folder_path, exist_ok=True)
            print(f"æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ: {date_folder}")
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«åè¨­å®š
            csv_filename = f"famipay-{date_folder}.csv"
            filename = os.path.join(folder_path, csv_filename)
            
            # è©³ç´°CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Ÿè¡Œ
            self.export_detailed_csv_to_file(filename)
            
            print(f"=== è‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {date_folder}/{csv_filename} ===")
            
        except Exception as e:
            print(f"è‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")

    def show_results(self):
        self.result_frame.configure(height=400)
        for widget in self.result_frame.winfo_children():
            widget.destroy()
            
        total_pages = len(self.pages)
        total_links = len(self.links)
        isolated_pages = sum(1 for p in self.pages.values() if p['inbound_links'] == 0)
        popular_pages = sum(1 for p in self.pages.values() if p['inbound_links'] >= 5)
        
        stats_label = ctk.CTkLabel(self.result_frame,
                                   text=f"ğŸ“Š åˆ†æçµæœ: {total_pages}ãƒšãƒ¼ã‚¸ | {total_links}å†…éƒ¨ãƒªãƒ³ã‚¯ï¼ˆé‡è¤‡é™¤å»å¾Œï¼‰ | å­¤ç«‹è¨˜äº‹{isolated_pages}ä»¶ | äººæ°—è¨˜äº‹{popular_pages}ä»¶",
                                   font=ctk.CTkFont(size=16, weight="bold"))
        stats_label.pack(pady=10)
        
        sorted_pages = sorted(self.pages.items(), key=lambda x: x[1]['inbound_links'], reverse=True)
        
        for i, (url, info) in enumerate(sorted_pages):
            if info['inbound_links'] == 0:
                evaluation = "ğŸš¨ è¦æ”¹å–„"
            elif info['inbound_links'] >= 10:
                evaluation = "ğŸ† è¶…äººæ°—"
            elif info['inbound_links'] >= 5:
                evaluation = "âœ… äººæ°—"
            else:
                evaluation = "âš ï¸ æ™®é€š"
                
            label_text = (
                f"{i+1:3d}. {info['title'][:50]}...\n"
                f"     è¢«ãƒªãƒ³ã‚¯:{info['inbound_links']:3d} | ç™ºãƒªãƒ³ã‚¯:{len(info['outbound_links']):3d} | {evaluation}\n"
                f"     {url}"
            )

            label = ctk.CTkLabel(self.result_frame, text=label_text, anchor="w", justify="left", font=ctk.CTkFont(size=11))
            label.pack(fill="x", padx=10, pady=2)

    def export_detailed_csv(self):
        """è©³ç´°CSVå‡ºåŠ›ï¼ˆè¢«ãƒªãƒ³ã‚¯è©³ç´°ï¼‰"""
        if not self.detailed_links:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            return

        filename = filedialog.asksaveasfilename(defaultextension=".csv", filetypes=[("CSV files", "*.csv")])
        if filename:
            self.export_detailed_csv_to_file(filename)

    def export_detailed_csv_to_file(self, filename):
        try:
            with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œï¼ˆ6åˆ—æ§‹æˆï¼‰
                writer.writerow([
                    'A_ç•ªå·', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL',
                    'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL', 'F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'
                ])
                
                # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURLåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
                target_groups = {}
                for link in self.detailed_links:
                    target_url = link['target_url']
                    if target_url not in target_groups:
                        target_groups[target_url] = []
                    target_groups[target_url].append(link)
                
                # è¢«ãƒªãƒ³ã‚¯æ•°ã§ã‚½ãƒ¼ãƒˆ
                sorted_targets = sorted(target_groups.items(), key=lambda x: len(x[1]), reverse=True)
                
                row_number = 1
                for target_url, links_to_target in sorted_targets:
                    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒšãƒ¼ã‚¸ã®æƒ…å ±ã‚’å–å¾—
                    target_info = self.pages.get(target_url, {})
                    target_title = target_info.get('title', target_url)
                    
                    if links_to_target:
                        # è¢«ãƒªãƒ³ã‚¯ãŒã‚ã‚‹å ´åˆã€å„è¢«ãƒªãƒ³ã‚¯ã‚’1è¡Œãšã¤å‡ºåŠ›
                        for link in links_to_target:
                            writer.writerow([
                                row_number,                      # A_ç•ªå·
                                target_title,                    # B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                                target_url,                      # C_URL
                                link['source_title'],            # D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                                link['source_url'],              # E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL
                                link['anchor_text']              # F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
                            ])
                            row_number += 1
                
                # å­¤ç«‹ãƒšãƒ¼ã‚¸ï¼ˆè¢«ãƒªãƒ³ã‚¯ãŒ0ã®ï¼‰ã‚‚è¿½åŠ 
                for url, info in self.pages.items():
                    if info['inbound_links'] == 0:
                        writer.writerow([
                            row_number,         # A_ç•ªå·
                            info['title'],      # B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                            url,               # C_URL
                            '',                # D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                            '',                # E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL
                            ''                 # F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
                        ])
                        row_number += 1
            
            messagebox.showinfo("å®Œäº†", f"è©³ç´°CSVã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
        except Exception as e:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ä¿å­˜ã«å¤±æ•—: {e}")

    def run(self):
        self.root.mainloop()

def main():
    app = LinkAnalyzerApp()
    app.run()

if __name__ == "__main__":
    main()
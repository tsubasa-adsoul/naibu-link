import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import re
from collections import Counter
import tempfile
import os

# PyVis
try:
    from pyvis.network import Network
    HAS_PYVIS = True
except ImportError:
    HAS_PYVIS = False

st.set_page_config(page_title="å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ«", page_icon="ğŸ”—", layout="wide")

# 14ã‚µã‚¤ãƒˆè¨­å®š
SITES = {
    "Answerç¾é‡‘åŒ–": "https://answer-genkinka.com",
    "ã‚ã‚ŠãŒãŸã‚„": "https://arigataya.co.jp", 
    "ãƒ“ãƒƒã‚¯ã‚®ãƒ•ãƒˆ": "https://bic-gift.co.jp",
    "ã‚¯ãƒ¬ã‹ãˆã‚‹": "https://crecaeru.co.jp",
    "ãƒ•ã‚¡ãƒŸãƒšã‚¤": "https://flashpay-famipay.com",
    "ãƒ¡ãƒ‡ã‚£ã‚¢": "https://flashpay-media.com",
    "ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒšã‚¤": "https://friendpay.me",
    "ä¸ç”¨å“å›åéšŠ": "https://fuyouhin-kaishuu.com",
    "è²·å–LIFE": "https://kaitori-life.com",
    "ã‚«ã‚¦ãƒ¼ãƒ«": "https://kau-ru.com",
    "MorePay": "https://morepay.jp",
    "ãƒšã‚¤ãƒ•ãƒ«": "https://payful.jp", 
    "ã‚¹ãƒãƒ¼ãƒˆãƒšã‚¤": "https://smartpay-gift.com",
    "XGIFT": "https://xgift.jp"
}

def crawl_site(site_name, base_url):
    session = requests.Session()
    session.headers.update({'User-Agent': 'Mozilla/5.0'})
    domain = urlparse(base_url).netloc.lower().replace('www.', '')
    
    # ã‚µã‚¤ãƒˆãƒãƒƒãƒ—å–å¾—
    st.info("ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’è§£æä¸­...")
    urls = set([base_url])
    try:
        res = session.get(urljoin(base_url, '/sitemap.xml'), timeout=20)
        if res.ok:
            soup = BeautifulSoup(res.content, 'lxml')
            for loc in soup.find_all('loc'):
                url_text = loc.text.strip()
                if not any(x in url_text.lower() for x in ['.xml', '/wp-admin/', 'attachment_id']):
                    urls.add(url_text)
    except:
        st.warning("ã‚µã‚¤ãƒˆãƒãƒƒãƒ—å–å¾—ã«å¤±æ•—ã€æ‰‹å‹•ã§ã‚¯ãƒ­ãƒ¼ãƒ«é–‹å§‹")
    
    # ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ã‚’åˆ¶é™
    to_visit = list(urls)[:100]  # æœ€å¤§100ãƒšãƒ¼ã‚¸ã«åˆ¶é™
    
    if not to_visit:
        st.error("ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return pd.DataFrame()
    
    st.success(f"ã‚¯ãƒ­ãƒ¼ãƒ«é–‹å§‹: {len(to_visit)}å€‹ã®URL")
    
    progress_bar = st.progress(0)
    pages = {}
    all_links = []
    
    for i, url in enumerate(to_visit):
        try:
            progress_bar.progress((i + 1) / len(to_visit))
            st.text(f"ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­: {i+1}/{len(to_visit)} - {url[:50]}...")
            
            res = session.get(url, timeout=10)
            if not res.ok:
                continue
                
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
            title = (soup.find('h1') or soup.find('title')).get_text(strip=True) if (soup.find('h1') or soup.find('title')) else url
            title = re.sub(r'\s*[|\-].*$', '', title).strip()
            pages[url] = title
            
            # ãƒªãƒ³ã‚¯æŠ½å‡º
            content = soup.select_one('.entry-content, .post-content, main, article') or soup.body
            
            for a in content.find_all('a', href=True):
                href = a['href']
                if href and not href.startswith('#'):
                    full_url = urljoin(base_url, href)
                    link_domain = urlparse(full_url).netloc.lower().replace('www.', '')
                    
                    if link_domain == domain:
                        all_links.append({
                            'source_url': url,
                            'source_title': title,
                            'target_url': full_url,
                            'anchor_text': a.get_text(strip=True) or '[ãƒªãƒ³ã‚¯]'
                        })
            
            time.sleep(0.3)
            
        except Exception as e:
            st.text(f"ã‚¨ãƒ©ãƒ¼: {url} - {e}")
    
    # CSVãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    if not all_links:
        return pd.DataFrame()
    
    csv_data = []
    target_counts = Counter(link['target_url'] for link in all_links)
    
    row_num = 1
    for target_url, count in target_counts.most_common():
        target_title = pages.get(target_url, target_url)
        target_links = [link for link in all_links if link['target_url'] == target_url]
        
        for link in target_links:
            csv_data.append({
                'A_ç•ªå·': row_num,
                'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«': target_title,
                'C_URL': target_url,
                'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«': link['source_title'],
                'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL': link['source_url'],
                'F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ': link['anchor_text']
            })
        row_num += 1
    
    return pd.DataFrame(csv_data)

def create_network(df, top_n=30):
    if not HAS_PYVIS or df.empty:
        return None
    
    edges = df[(df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'] != '') & (df['C_URL'] != '')].copy()
    if edges.empty:
        return None
    
    # ä¸Šä½ãƒšãƒ¼ã‚¸ã®ã¿
    in_counts = edges.groupby('C_URL').size().sort_values(ascending=False)
    top_pages = set(in_counts.head(top_n).index)
    sub_edges = edges[edges['C_URL'].isin(top_pages)]
    
    if sub_edges.empty:
        return None
    
    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä½œæˆ
    net = Network(height="700px", width="100%", directed=True)
    
    # ãƒãƒ¼ãƒ‰è¿½åŠ 
    for url in top_pages:
        title = df[df['C_URL'] == url]['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«'].iloc[0]
        size = max(15, min(40, int(in_counts.get(url, 0) * 3)))
        net.add_node(url, label=title[:15], title=title, size=size)
    
    # ã‚¨ãƒƒã‚¸è¿½åŠ 
    for _, row in sub_edges.iterrows():
        if row['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'] in top_pages:
            net.add_edge(row['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'], row['C_URL'])
    
    # HTMLç”Ÿæˆ
    try:
        with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False) as tmp:
            net.save_graph(tmp.name)
            with open(tmp.name, 'r', encoding='utf-8') as f:
                html_content = f.read()
            os.unlink(tmp.name)
        return html_content
    except:
        return None

def main():
    st.title("ğŸ”— å†…éƒ¨ãƒªãƒ³ã‚¯æ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ«")
    
    if 'analysis_data' not in st.session_state:
        st.session_state.analysis_data = None
    
    with st.sidebar:
        st.header("ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹")
        
        # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³åˆ†æ
        st.subheader("ã‚ªãƒ³ãƒ©ã‚¤ãƒ³åˆ†æ")
        selected_site = st.selectbox("ã‚µã‚¤ãƒˆé¸æŠ", list(SITES.keys()))
        
        if st.button("ğŸš€ åˆ†æé–‹å§‹"):
            base_url = SITES[selected_site]
            with st.spinner(f"{selected_site}ã‚’åˆ†æä¸­..."):
                df = crawl_site(selected_site, base_url)
                if not df.empty:
                    st.session_state.analysis_data = df
                    st.session_state.site_name = selected_site
                    st.success("åˆ†æå®Œäº†ï¼")
                    st.rerun()
                else:
                    st.error("åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        st.divider()
        
        # CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        st.subheader("CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
        uploaded_file = st.file_uploader("CSVãƒ•ã‚¡ã‚¤ãƒ«", type=['csv'])
        if uploaded_file:
            try:
                df = pd.read_csv(uploaded_file, encoding="utf-8-sig").fillna("")
                st.session_state.analysis_data = df
                st.session_state.site_name = uploaded_file.name
                st.success("èª­ã¿è¾¼ã¿å®Œäº†ï¼")
            except Exception as e:
                st.error(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ãƒ¡ã‚¤ãƒ³è¡¨ç¤º
    if st.session_state.analysis_data is None:
        st.info("ğŸ‘ˆ ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return
    
    df = st.session_state.analysis_data
    site_name = getattr(st.session_state, 'site_name', 'Unknown')
    
    # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    csv_data = df.to_csv(index=False, encoding="utf-8-sig")
    st.download_button("ğŸ“¥ çµæœã‚’CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", csv_data, f"{site_name}_analysis.csv", "text/csv")
    
    # ã‚¿ãƒ–è¡¨ç¤º
    tab1, tab2, tab3 = st.tabs(["ğŸ“Š ãƒ‡ãƒ¼ã‚¿ä¸€è¦§", "ğŸ›ï¸ ãƒ”ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸", "ğŸ“ˆ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³"])
    
    with tab1:
        st.dataframe(df, use_container_width=True)
    
    with tab2:
        pages = df[['B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL']].drop_duplicates()
        has_links = df[(df['D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«'] != '') & (df['E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL'] != '')]
        link_counts = has_links.groupby('C_URL').size()
        pages['è¢«ãƒªãƒ³ã‚¯æ•°'] = pages['C_URL'].map(link_counts).fillna(0).astype(int)
        pages = pages.sort_values('è¢«ãƒªãƒ³ã‚¯æ•°', ascending=False)
        
        st.dataframe(pages, use_container_width=True)
    
    with tab3:
        if HAS_PYVIS:
            with st.spinner("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ç”Ÿæˆä¸­..."):
                html_content = create_network(df)
                if html_content:
                    st.components.v1.html(html_content, height=720)
                else:
                    st.error("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        else:
            st.error("PyVisãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™")

if __name__ == "__main__":
    main()

import tkinter as tk
from tkinter import messagebox, filedialog
import customtkinter as ctk
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
import json
import time
import re
import threading
import webbrowser
from datetime import datetime
import csv
import sys
import os

ctk.set_appearance_mode("light")
ctk.set_default_color_theme("blue")

class KauRuAnalyzer:
    def __init__(self):
        self.root = ctk.CTk()
        self.root.title("ğŸ”— kau-ru.co.jpå°‚ç”¨å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆæ”¹ä¿®ç‰ˆï¼‰")
        self.root.geometry("1200x800")
        self.pages = {}
        self.links = []
        self.detailed_links = []  # è©³ç´°ãƒªãƒ³ã‚¯æƒ…å ±ç”¨
        self.analysis_data = None
        self.is_analyzing = False
        self.excluded_links_count = 0  # é™¤å¤–ã•ã‚ŒãŸãƒªãƒ³ã‚¯æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        
        self.setup_ui()
        
        # è‡ªå‹•åˆ†æé–‹å§‹ï¼ˆEXEåŒ–å¯¾å¿œï¼‰
        self.root.after(1000, self.auto_start_analysis)  # 1ç§’å¾Œã«è‡ªå‹•é–‹å§‹

    def setup_ui(self):
        self.main_frame = ctk.CTkScrollableFrame(self.root)
        self.main_frame.pack(fill="both", expand=True, padx=20, pady=20)

        title_label = ctk.CTkLabel(self.main_frame, text="ğŸ”— kau-ru.co.jpå°‚ç”¨å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆæ”¹ä¿®ç‰ˆï¼‰",
                                   font=ctk.CTkFont(size=28, weight="bold"))
        title_label.pack(pady=20)

        self.url_entry = ctk.CTkEntry(self.main_frame, placeholder_text="https://kau-ru.co.jp", width=500)
        self.url_entry.pack(pady=10)
        self.url_entry.insert(0, "https://kau-ru.co.jp")

        self.analyze_button = ctk.CTkButton(self.main_frame, text="åˆ†æé–‹å§‹", command=self.start_analysis)
        self.analyze_button.pack(pady=10)

        self.status_label = ctk.CTkLabel(self.main_frame, text="")
        self.status_label.pack(pady=10)

        self.progress_bar = ctk.CTkProgressBar(self.main_frame, width=400)
        self.progress_bar.pack(pady=5)
        self.progress_bar.stop()

        self.result_frame = ctk.CTkScrollableFrame(self.main_frame, height=400)
        self.result_frame.pack(fill="both", expand=True, pady=20)

        # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒœã‚¿ãƒ³ï¼ˆè©³ç´°ã®ã¿ï¼‰
        ctk.CTkButton(self.main_frame, text="è©³ç´°CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ", command=self.export_detailed_csv).pack(pady=10)

    def detect_site_type(self, url):
        """ã‚µã‚¤ãƒˆã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š"""
        domain = urlparse(url).netloc.lower()
        
        if 'kau-ru.co.jp' in domain:
            return 'kauru'
        elif 'kaitori-life.com' in domain:
            return 'kaitori_life'
        elif 'friend-pay.com' in domain:
            return 'friend_pay'
        elif 'kurekaeru.com' in domain:
            return 'kurekaeru'
        else:
            return 'unknown'

    def get_exclude_selectors(self, site_type):
        """ã‚µã‚¤ãƒˆã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸé™¤å¤–ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è¿”ã™"""
        base_selectors = ['header', 'footer', 'nav', 'aside', '.sidebar', '.widget']
        
        site_specific_selectors = {
            'kauru': [
                '.textwidget',                    # ã‚«ã‚¦ãƒ¼ãƒ«: textwidget
                '.textwidget.custom-html-widget'  # ã‚«ã‚¦ãƒ¼ãƒ«: custom-html-widgetä»˜ãtextwidget
            ],
            'kaitori_life': [
                '#nav-container.header-style4-animate.animate'  # è²·å–LIFE: ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
            ],
            'friend_pay': [
                '.l-header__inner.l-container',   # ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒšã‚¤: ãƒ˜ãƒƒãƒ€ãƒ¼å†…éƒ¨
                '.l-footer__nav',                 # ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒšã‚¤: ãƒ•ãƒƒã‚¿ãƒ¼ãƒŠãƒ“
                '.c-tabBody.p-postListTabBody'   # ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒšã‚¤: ã‚¿ãƒ–ãƒœãƒ‡ã‚£
            ],
            'kurekaeru': [
                '#gnav.l-header__gnav.c-gnavWrap'  # ã‚¯ãƒ¬ã‹ãˆã‚‹: ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒŠãƒ“
            ]
        }
        
        return base_selectors + site_specific_selectors.get(site_type, [])

    def auto_start_analysis(self):
        """EXEåŒ–å¯¾å¿œï¼šè‡ªå‹•ã§åˆ†æé–‹å§‹"""
        print("=== è‡ªå‹•åˆ†æé–‹å§‹ ===")
        self.status_label.configure(text="è‡ªå‹•åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
        self.start_analysis()

    def start_analysis(self):
        url = self.url_entry.get().strip()
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        # ã‚µã‚¤ãƒˆã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
        site_type = self.detect_site_type(url)
        print(f"ã‚µã‚¤ãƒˆã‚¿ã‚¤ãƒ—åˆ¤å®š: {site_type}")
        
        self.analyze_button.configure(state="disabled")
        self.status_label.configure(text="ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­...")
        self.progress_bar.start()
        self.excluded_links_count = 0  # ãƒªã‚»ãƒƒãƒˆ
        
        thread = threading.Thread(target=self.analyze_site, args=(url,))
        thread.daemon = True
        thread.start()

    def extract_from_sitemap(self, url):
        urls = set()
        try:
            res = requests.get(url, timeout=10)
            res.raise_for_status()
            soup = BeautifulSoup(res.content, 'xml')
            locs = soup.find_all('loc')
            if soup.find('sitemapindex'):
                for loc in locs:
                    urls.update(self.extract_from_sitemap(loc.text.strip()))
            else:
                for loc in locs:
                    loc_url = loc.text.strip()
                    if not re.search(r'sitemap.*\.(xml|html)$', loc_url.lower()):
                        urls.add(self.normalize_url(loc_url))
        except Exception as e:
            print(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return list(urls)

    def generate_seed_urls(self, base_url):
        seed_urls = [self.normalize_url(base_url)]
        
        # ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‹ã‚‰URLå–å¾—
        sitemap_root = urljoin(base_url, '/sitemap.xml')
        sitemap_urls = self.extract_from_sitemap(sitemap_root)
        seed_urls.extend(sitemap_urls)
        
        # WordPressæ¨™æº–ã®ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚‚è©¦è¡Œ
        wp_sitemaps = [
            '/wp-sitemap.xml',
            '/wp-sitemap-posts-post-1.xml',
            '/sitemap_index.xml',
            '/post-sitemap.xml',
            '/sitemap-posttype-post.xml'
        ]
        
        for sitemap_path in wp_sitemaps:
            sitemap_url = urljoin(base_url, sitemap_path)
            try:
                additional_urls = self.extract_from_sitemap(sitemap_url)
                seed_urls.extend(additional_urls)
                if additional_urls:
                    print(f"{sitemap_path} ã‹ã‚‰ {len(additional_urls)} å€‹ã®URLã‚’å–å¾—")
            except:
                pass
        
        # æ‰‹å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚è¿½åŠ ï¼ˆ?p=å½¢å¼ã®é€£ç•ªï¼‰
        parsed = urlparse(base_url)
        base_domain = f"{parsed.scheme}://{parsed.netloc}"
        
        # ?p=1 ã‹ã‚‰ ?p=30000 ã¾ã§å…¨ã¦ãƒã‚§ãƒƒã‚¯
        print("æ‰‹å‹•URLãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆä¸­...")
        for post_id in range(1, 30001):
            seed_urls.append(f"{base_domain}/media/?p={post_id}")
        
        print(f"=== {parsed.netloc} åˆ†æé–‹å§‹ ===")
        print(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‹ã‚‰ {len(sitemap_urls)} å€‹ã®URLã‚’å–å¾—")
        print(f"æ‰‹å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã§30000å€‹ã®URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ ")
        print(f"ç·ã‚·ãƒ¼ãƒ‰URLæ•°: {len(seed_urls)}")
        return list(set(seed_urls))

    def is_content(self, url):
        parsed = urlparse(url)
        path = parsed.path.lower().rstrip('/')
        query = parsed.query.lower()
        
        # /site/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’é™¤å¤–ï¼ˆãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆç”¨ã®ãŸã‚ä¸è¦ï¼‰
        if '/site/' in path:
            return False
        
        # é™çš„ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‚’å«ã‚€URLã¯é™¤å¤–
        if re.search(r'\.(jpg|jpeg|png|gif|svg|webp|bmp|pdf|docx?|xlsx?|zip|rar|mp4|mp3)$', path, re.IGNORECASE):
            return False
        
        # WordPresså„ç¨®ã‚¯ã‚¨ãƒªå½¢å¼ã‚’è¨±å¯
        if query.startswith('p=') and query[2:].isdigit():
            return True
        if query.startswith('page_id=') and query[8:].isdigit():
            return True
        if query.startswith('cat=') and query[4:].isdigit():
            return True
        # ã‚«ãƒ†ã‚´ãƒªåã§ã®æŒ‡å®šã‚‚è¨±å¯
        if query.startswith('cat=') and len(query) > 4:
            return True
        if query.startswith('category_name='):
            return True
        if any(query.startswith(param) for param in ['m=', 'author=', 'tag=']):
            return True
        
        # é‡è¦ãªãƒšãƒ¼ã‚¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¨±å¯
        if any(re.search(pattern, path) for pattern in [
            r'^/$', r'^$', r'^/blog', r'^/news', r'^/media', r'^/posts', r'^/article',
            r'^/category/[a-z0-9\-]+$', r'^/[a-z0-9\-]+$'
        ]):
            return True
        
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆåŸºæœ¬çš„ãªã‚‚ã®ã®ã¿ï¼‰
        exclude_patterns = [
            r'/sitemap', r'/wp-admin', r'/wp-json',
            r'#', r'/feed/', r'mailto:', r'tel:', r'/privacy', r'/terms', r'/contact'
        ]
        
        if '?utm_' in url or '?fbclid=' in url or '?gclid=' in url:
            return False
        
        for pattern in exclude_patterns:
            if re.search(pattern, url.lower()):
                return False
        
        return True

    def normalize_url(self, url):
        """URLæ­£è¦åŒ–å‡¦ç†ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        if not url:
            return ""
            
        # ç›¸å¯¾URLã®å‡¦ç†
        if url.startswith('/'):
            try:
                base_domain = f"{urlparse(self.url_entry.get()).scheme}://{urlparse(self.url_entry.get()).netloc}"
                url = base_domain + url
            except:
                return ""
        
        try:
            parsed = urlparse(url)
            scheme = 'https'
            netloc = parsed.netloc.replace('www.', '')
            path = parsed.path.rstrip('/')
            
            # WordPressã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿æŒ
            if parsed.query:
                query_params = parse_qs(parsed.query)
                for param in ['p', 'page_id', 'cat']:
                    if param in query_params and query_params[param][0].isdigit():
                        return f"{scheme}://{netloc}{path}?{param}={query_params[param][0]}"
                if any(param in query_params for param in ['m', 'author', 'tag']):
                    return f"{scheme}://{netloc}{path}?{parsed.query}"
            
            return f"{scheme}://{netloc}{path}"
        except Exception as e:
            print(f"URLæ­£è¦åŒ–ã‚¨ãƒ©ãƒ¼: {url} -> {e}")
            return ""

    def is_internal(self, url, domain):
        try:
            return urlparse(url).netloc.replace('www.', '') == domain.replace('www.', '')
        except:
            return False

    def is_attachment_page(self, soup):
        """æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ãƒšãƒ¼ã‚¸ã‹ã©ã†ã‹åˆ¤å®š"""
        if not soup.title or not soup.title.string:
            return False
        
        title = soup.title.string.strip()
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’ | ã§åˆ†å‰²
        if '|' in title:
            title_part = title.split('|')[0].strip()  # | ã‚ˆã‚Šå‰ã®éƒ¨åˆ†
            
            # ã‚¿ã‚¤ãƒˆãƒ«éƒ¨åˆ†ãŒ -min ã§çµ‚ã‚ã‚‹å ´åˆã¯æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«
            if title_part.endswith('-min'):
                return True
        
        return False

    def analyze_site(self, base_url):
        try:
            pages = {}
            links = []
            detailed_links = []  # è©³ç´°ãƒªãƒ³ã‚¯æƒ…å ±ç”¨
            visited = set()
            to_visit = self.generate_seed_urls(base_url)
            domain = urlparse(base_url).netloc
            processed_links = set()  # é‡è¤‡é™¤å»ç”¨
            site_type = self.detect_site_type(base_url)

            session = requests.Session()
            session.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'})

            # é‡è¤‡é™¤å»
            unique_to_visit = []
            seen = set()
            for url in to_visit:
                normalized = self.normalize_url(url)
                if normalized and normalized not in seen:
                    seen.add(normalized)
                    unique_to_visit.append(normalized)
            
            to_visit = unique_to_visit
            print(f"é‡è¤‡é™¤å»å¾Œã®ã‚·ãƒ¼ãƒ‰URLæ•°: {len(to_visit)}")

            while to_visit and len(pages) < 1000:
                url = to_visit.pop(0)
                normalized_url = self.normalize_url(url)
                if not normalized_url or normalized_url in visited:
                    continue

                try:
                    print(f"å‡¦ç†ä¸­: {normalized_url}")
                    response = session.get(url, timeout=15)  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå»¶é•·
                    if response.status_code != 200:
                        print(f"  HTTPã‚¨ãƒ©ãƒ¼: {response.status_code}")
                        continue
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ãƒšãƒ¼ã‚¸ã‚’é™¤å¤–
                    if self.is_attachment_page(soup):
                        print(f"  æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒƒãƒ—: {url}")
                        continue
                    
                    # attachment_id ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã‚‚é™¤å¤–
                    final_url = response.url
                    if 'attachment_id=' in urlparse(final_url).query:
                        print(f"  attachment_id ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—: {url}")
                        continue
                    
                    extracted_links = self.extract_links(soup, site_type)
                    new_links_found = 0

                    # ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’ä¿å­˜
                    title = soup.title.string.strip() if soup.title and soup.title.string else normalized_url
                    # ã‚µã‚¤ãƒˆåé™¤å»ï¼ˆæ”¹å–„ç‰ˆï¼‰
                    for site_name in ['kau-ru', 'ã‚«ã‚¦ãƒ¼ãƒ«', 'kaitori-life', 'è²·å–LIFE', 'friend-pay', 'ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒšã‚¤', 'kurekaeru', 'ã‚¯ãƒ¬ã‹ãˆã‚‹']:
                        title = re.sub(rf'\s*[|\-]\s*.*{re.escape(site_name)}.*$', '', title, flags=re.IGNORECASE)
                    title = title.strip()
                    
                    pages[normalized_url] = {'title': title, 'outbound_links': []}

                    # ã“ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ã®å†…éƒ¨ãƒªãƒ³ã‚¯ã‚’è¨˜éŒ²
                    for link in extracted_links:
                        normalized_link = self.normalize_url(link['url'])
                        if not normalized_link:
                            continue
                            
                        anchor_text = link['anchor_text']
                        
                        if (self.is_internal(normalized_link, domain) and 
                            self.is_content(normalized_link)):
                            
                            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆåŒã˜ã‚½ãƒ¼ã‚¹â†’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ãƒªãƒ³ã‚¯ã¯1å›ã ã‘ï¼‰
                            link_key = (normalized_url, normalized_link)
                            if link_key not in processed_links:
                                processed_links.add(link_key)
                                
                                links.append((normalized_url, normalized_link))
                                pages[normalized_url]['outbound_links'].append(normalized_link)
                                
                                # è©³ç´°ãƒªãƒ³ã‚¯æƒ…å ±ã‚’ä¿å­˜
                                detailed_links.append({
                                    'source_url': normalized_url,
                                    'source_title': title,
                                    'target_url': normalized_link,
                                    'anchor_text': anchor_text
                                })
                        
                        # æ–°è¦URLã®ç™ºè¦‹
                        if (self.is_internal(normalized_link, domain) and
                            self.is_content(normalized_link) and
                            normalized_link not in visited and
                            normalized_link not in to_visit):
                            to_visit.append(normalized_link)
                            new_links_found += 1

                    visited.add(normalized_url)

                    status_text = f"{len(pages)}ä»¶ç›®: {title[:50]}..."
                    if new_links_found > 0:
                        status_text += f" (æ–°è¦ãƒªãƒ³ã‚¯{new_links_found}ä»¶ç™ºè¦‹)"
                    
                    self.root.after(0, lambda text=status_text: self.status_label.configure(text=text))
                    self.root.after(0, lambda: self.progress_bar.set(min(len(pages) / 1000, 1.0)))
                    time.sleep(0.1)

                except Exception as e:
                    print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
                    continue

            # è¢«ãƒªãƒ³ã‚¯æ•°ã‚’è¨ˆç®—
            for url in pages:
                pages[url]['inbound_links'] = len(set([src for src, tgt in links if tgt == url]))

            self.pages = pages
            self.links = links
            self.detailed_links = detailed_links
            
            print(f"=== åˆ†æå®Œäº†: {len(pages)}ãƒšãƒ¼ã‚¸, {len(links)}ãƒªãƒ³ã‚¯, é™¤å¤–{self.excluded_links_count}ãƒªãƒ³ã‚¯ ===")
            self.root.after(0, self.show_results)
            
            # å…¨è‡ªå‹•åŒ–ï¼šåˆ†æå®Œäº†å¾Œã«è‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            if self.pages:  # ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿
                self.root.after(2000, self.auto_export_csv)  # 2ç§’å¾Œã«è‡ªå‹•ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            
        except Exception as e:
            self.root.after(0, lambda: messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ã‚µã‚¤ãƒˆåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"))
        finally:
            self.root.after(0, lambda: self.analyze_button.configure(state="normal"))
            self.root.after(0, lambda: self.progress_bar.stop())
            self.root.after(0, lambda: self.status_label.configure(text="åˆ†æå®Œäº†"))

    def extract_links(self, soup, site_type):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºï¼ˆã‚µã‚¤ãƒˆåˆ¥é™¤å¤–å¯¾å¿œï¼‰"""
        selectors = ['.entry-content', '.post-content', '.content', 'main', 'article']
        exclude_selectors = self.get_exclude_selectors(site_type)
        
        print(f"  ã‚µã‚¤ãƒˆã‚¿ã‚¤ãƒ— {site_type} ã®é™¤å¤–ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼: {exclude_selectors}")
        
        for selector in selectors:
            areas = soup.select(selector)
            if areas:
                links = []
                for area in areas:
                    # ã‚µã‚¤ãƒˆåˆ¥é™¤å¤–å‡¦ç†
                    excluded_count = 0
                    for exclude_selector in exclude_selectors:
                        excluded_elements = area.select(exclude_selector)
                        for elem in excluded_elements:
                            excluded_count += len(elem.find_all('a', href=True))
                            elem.decompose()
                    
                    if excluded_count > 0:
                        print(f"    é™¤å¤–ã—ãŸãƒªãƒ³ã‚¯æ•°: {excluded_count}")
                        self.excluded_links_count += excluded_count
                    
                    found_links = area.find_all('a', href=True)
                    for link in found_links:
                        anchor_text = link.get_text(strip=True) or link.get('title', '') or '[ãƒªãƒ³ã‚¯]'
                        links.append({
                            'url': link['href'],
                            'anchor_text': anchor_text[:100]  # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
                        })
                if links:
                    return links
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆé™¤å¤–å‡¦ç†ãªã—ï¼‰
        print("  ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã§ãƒªãƒ³ã‚¯æŠ½å‡º")
        all_links = []
        for link in soup.find_all('a', href=True):
            anchor_text = link.get_text(strip=True) or link.get('title', '') or '[ãƒªãƒ³ã‚¯]'
            all_links.append({
                'url': link['href'],
                'anchor_text': anchor_text[:100]
            })
        return all_links

    def show_results(self):
        for widget in self.result_frame.winfo_children():
            widget.destroy()

        total_pages = len(self.pages)
        total_links = len(self.links)
        isolated_pages = sum(1 for p in self.pages.values() if p['inbound_links'] == 0)
        popular_pages = sum(1 for p in self.pages.values() if p['inbound_links'] >= 5)

        stats_label = ctk.CTkLabel(self.result_frame,
                                   text=f"ğŸ“Š åˆ†æçµæœ: {total_pages}ãƒšãƒ¼ã‚¸ | {total_links}å†…éƒ¨ãƒªãƒ³ã‚¯ | é™¤å¤–{self.excluded_links_count}ãƒªãƒ³ã‚¯ | å­¤ç«‹è¨˜äº‹{isolated_pages}ä»¶ | äººæ°—è¨˜äº‹{popular_pages}ä»¶",
                                   font=ctk.CTkFont(size=16, weight="bold"))
        stats_label.pack(pady=10)

        sorted_pages = sorted(self.pages.items(), key=lambda x: x[1]['inbound_links'], reverse=True)
        for i, (url, info) in enumerate(sorted_pages):
            if info['inbound_links'] == 0:
                evaluation = "ğŸš¨ è¦æ”¹å–„"
            elif info['inbound_links'] >= 10:
                evaluation = "ğŸ† è¶…äººæ°—"
            elif info['inbound_links'] >= 5:
                evaluation = "âœ… äººæ°—"
            else:
                evaluation = "âš ï¸ æ™®é€š"

            label_text = f"{i+1:3d}. {info['title'][:50]}...\n     è¢«ãƒªãƒ³ã‚¯:{info['inbound_links']:3d} | ç™ºãƒªãƒ³ã‚¯:{len(info['outbound_links']):3d} | {evaluation}\n     {url}"
            
            label = ctk.CTkLabel(self.result_frame, text=label_text, anchor="w", justify="left", font=ctk.CTkFont(size=11))
            label.pack(fill="x", padx=10, pady=2)

    def auto_export_csv(self):
        """å…¨è‡ªå‹•åŒ–ï¼šè‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆEXEåŒ–å¯¾å¿œï¼‰"""
        try:
            # EXEåŒ–å¯¾å¿œï¼šå®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
            if hasattr(sys, '_MEIPASS'):
                # PyInstallerã§EXEåŒ–ã•ã‚ŒãŸå ´åˆ
                base_dir = os.path.dirname(sys.executable)
            else:
                # é€šå¸¸ã®Pythonå®Ÿè¡Œã®å ´åˆ
                base_dir = os.path.dirname(os.path.abspath(__file__))
            
            # æ—¥æœ¬ã®å½“æ—¥æ—¥ä»˜ã§ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
            today = datetime.now()
            date_folder = today.strftime("%Y-%m-%d")  # 2025-06-20 å½¢å¼
            folder_path = os.path.join(base_dir, date_folder)
            
            # ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
            if not os.path.exists(folder_path):
                os.makedirs(folder_path)
                print(f"æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ: {date_folder}")
            
            # ã‚µã‚¤ãƒˆã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸãƒ•ã‚¡ã‚¤ãƒ«å
            site_type = self.detect_site_type(self.url_entry.get())
            site_names = {
                'kauru': 'kau-ru',
                'kaitori_life': 'kaitori-life',
                'friend_pay': 'friend-pay',
                'kurekaeru': 'kurekaeru'
            }
            site_name = site_names.get(site_type, 'unknown')
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«åã¨ãƒ‘ã‚¹
            csv_filename = f"{site_name}-{date_folder}.csv"
            filename = os.path.join(folder_path, csv_filename)
            
            if not self.detailed_links:
                print("è©³ç´°ãƒ‡ãƒ¼ã‚¿ãªã— - CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—")
                return
            
            with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œï¼ˆ6åˆ—æ§‹æˆï¼‰
                writer.writerow([
                    'A_ç•ªå·', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL',
                    'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL', 'F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'
                ])
                
                # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURLåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
                target_groups = {}
                for link in self.detailed_links:
                    target_url = link['target_url']
                    if target_url not in target_groups:
                        target_groups[target_url] = []
                    target_groups[target_url].append(link)
                
                # è¢«ãƒªãƒ³ã‚¯æ•°ã§ã‚½ãƒ¼ãƒˆ
                sorted_targets = sorted(target_groups.items(), key=lambda x: len(x[1]), reverse=True)
                
                # ãƒšãƒ¼ã‚¸ç•ªå·ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆï¼ˆåŒã˜ã‚¿ã‚¤ãƒˆãƒ«+URLã«ã¯åŒã˜ç•ªå·ï¼‰
                page_numbers = {}
                current_page_number = 1
                
                # è¢«ãƒªãƒ³ã‚¯ã‚ã‚Šãƒšãƒ¼ã‚¸ã®ç•ªå·å‰²ã‚Šå½“ã¦
                for target_url, links_to_target in sorted_targets:
                    target_info = self.pages.get(target_url, {})
                    target_title = target_info.get('title', target_url)
                    page_key = (target_title, target_url)
                    
                    if page_key not in page_numbers:
                        page_numbers[page_key] = current_page_number
                        current_page_number += 1
                
                # å­¤ç«‹ãƒšãƒ¼ã‚¸ã®ç•ªå·å‰²ã‚Šå½“ã¦
                for url, info in self.pages.items():
                    if info['inbound_links'] == 0:
                        page_key = (info['title'], url)
                        if page_key not in page_numbers:
                            page_numbers[page_key] = current_page_number
                            current_page_number += 1
                
                # è¢«ãƒªãƒ³ã‚¯ã‚ã‚Šãƒšãƒ¼ã‚¸ã®å‡ºåŠ›
                for target_url, links_to_target in sorted_targets:
                    target_info = self.pages.get(target_url, {})
                    target_title = target_info.get('title', target_url)
                    page_key = (target_title, target_url)
                    page_number = page_numbers[page_key]
                    
                    if links_to_target:
                        # è¢«ãƒªãƒ³ã‚¯ãŒã‚ã‚‹å ´åˆã€å„è¢«ãƒªãƒ³ã‚¯ã‚’1è¡Œãšã¤å‡ºåŠ›ï¼ˆåŒã˜ãƒšãƒ¼ã‚¸ç•ªå·ï¼‰
                        for link in links_to_target:
                            writer.writerow([
                                page_number,                     # A_ç•ªå·ï¼ˆçµ±ä¸€ï¼‰
                                target_title,                    # B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                                target_url,                      # C_URL
                                link['source_title'],            # D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                                link['source_url'],              # E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL
                                link['anchor_text']              # F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
                            ])
                
                # å­¤ç«‹ãƒšãƒ¼ã‚¸ï¼ˆè¢«ãƒªãƒ³ã‚¯ãŒ0ã®ï¼‰ã®å‡ºåŠ›
                for url, info in self.pages.items():
                    if info['inbound_links'] == 0:
                        page_key = (info['title'], url)
                        page_number = page_numbers[page_key]
                        
                        writer.writerow([
                            page_number,        # A_ç•ªå·
                            info['title'],      # B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                            url,               # C_URL
                            '',                # D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                            '',                # E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL
                            ''                 # F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
                        ])
            
            print(f"=== è‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {folder_path}/{csv_filename} ===")
            self.status_label.configure(text=f"åˆ†æå®Œäº†ï¼CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {csv_filename}")
            
        except Exception as e:
            print(f"è‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            self.status_label.configure(text="åˆ†æå®Œäº†ï¼ˆCSVä¿å­˜ã‚¨ãƒ©ãƒ¼ï¼‰")

    def export_detailed_csv(self):
        """è©³ç´°CSVå‡ºåŠ›ï¼ˆè¢«ãƒªãƒ³ã‚¯è©³ç´°ï¼‰"""
        if not self.detailed_links:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            return

        # ã‚µã‚¤ãƒˆåã‚’å–å¾—ã—ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¨­å®š
        site_type = self.detect_site_type(self.url_entry.get())
        site_names = {
            'kauru': 'kau-ru',
            'kaitori_life': 'kaitori-life',
            'friend_pay': 'friend-pay',
            'kurekaeru': 'kurekaeru'
        }
        site_name = site_names.get(site_type, 'unknown')
        today = datetime.now().strftime("%Y-%m-%d")
        default_filename = f"{site_name}-{today}.csv"

        filename = filedialog.asksaveasfilename(
            defaultextension=".csv", 
            filetypes=[("CSV files", "*.csv")],
            initialvalue=default_filename
        )
        
        if filename:
            try:
                with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.writer(f)
                    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œï¼ˆ6åˆ—æ§‹æˆï¼‰
                    writer.writerow([
                        'A_ç•ªå·', 'B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'C_URL',
                        'D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL', 'F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'
                    ])
                    
                    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURLåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
                    target_groups = {}
                    for link in self.detailed_links:
                        target_url = link['target_url']
                        if target_url not in target_groups:
                            target_groups[target_url] = []
                        target_groups[target_url].append(link)
                    
                    # è¢«ãƒªãƒ³ã‚¯æ•°ã§ã‚½ãƒ¼ãƒˆ
                    sorted_targets = sorted(target_groups.items(), key=lambda x: len(x[1]), reverse=True)
                    
                    # ãƒšãƒ¼ã‚¸ç•ªå·ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆï¼ˆåŒã˜ã‚¿ã‚¤ãƒˆãƒ«+URLã«ã¯åŒã˜ç•ªå·ï¼‰
                    page_numbers = {}
                    current_page_number = 1
                    
                    # è¢«ãƒªãƒ³ã‚¯ã‚ã‚Šãƒšãƒ¼ã‚¸ã®ç•ªå·å‰²ã‚Šå½“ã¦
                    for target_url, links_to_target in sorted_targets:
                        target_info = self.pages.get(target_url, {})
                        target_title = target_info.get('title', target_url)
                        page_key = (target_title, target_url)
                        
                        if page_key not in page_numbers:
                            page_numbers[page_key] = current_page_number
                            current_page_number += 1
                    
                    # å­¤ç«‹ãƒšãƒ¼ã‚¸ã®ç•ªå·å‰²ã‚Šå½“ã¦
                    for url, info in self.pages.items():
                        if info['inbound_links'] == 0:
                            page_key = (info['title'], url)
                            if page_key not in page_numbers:
                                page_numbers[page_key] = current_page_number
                                current_page_number += 1
                    
                    # è¢«ãƒªãƒ³ã‚¯ã‚ã‚Šãƒšãƒ¼ã‚¸ã®å‡ºåŠ›
                    for target_url, links_to_target in sorted_targets:
                        target_info = self.pages.get(target_url, {})
                        target_title = target_info.get('title', target_url)
                        page_key = (target_title, target_url)
                        page_number = page_numbers[page_key]
                        
                        if links_to_target:
                            # è¢«ãƒªãƒ³ã‚¯ãŒã‚ã‚‹å ´åˆã€å„è¢«ãƒªãƒ³ã‚¯ã‚’1è¡Œãšã¤å‡ºåŠ›ï¼ˆåŒã˜ãƒšãƒ¼ã‚¸ç•ªå·ï¼‰
                            for link in links_to_target:
                                writer.writerow([
                                    page_number,                     # A_ç•ªå·ï¼ˆçµ±ä¸€ï¼‰
                                    target_title,                    # B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                                    target_url,                      # C_URL
                                    link['source_title'],            # D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                                    link['source_url'],              # E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL
                                    link['anchor_text']              # F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
                                ])
                    
                    # å­¤ç«‹ãƒšãƒ¼ã‚¸ï¼ˆè¢«ãƒªãƒ³ã‚¯ãŒ0ã®ï¼‰ã®å‡ºåŠ›
                    for url, info in self.pages.items():
                        if info['inbound_links'] == 0:
                            page_key = (info['title'], url)
                            page_number = page_numbers[page_key]
                            
                            writer.writerow([
                                page_number,        # A_ç•ªå·
                                info['title'],      # B_ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                                url,               # C_URL
                                '',                # D_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
                                '',                # E_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸URL
                                ''                 # F_è¢«ãƒªãƒ³ã‚¯å…ƒãƒšãƒ¼ã‚¸ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
                            ])
                
                messagebox.showinfo("å®Œäº†", f"è©³ç´°CSVã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}\nãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒšãƒ¼ã‚¸æ•°: {len(page_numbers)}")
            except Exception as e:
                messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ä¿å­˜ã«å¤±æ•—: {e}")

    def run(self):
        self.root.mainloop()

def main():
    app = KauRuAnalyzer()
    app.run()

if __name__ == "__main__":
    main()
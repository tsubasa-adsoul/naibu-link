import tkinter as tk
from tkinter import messagebox, filedialog
import customtkinter as ctk
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import json
import time
import re
import threading
import webbrowser
from datetime import datetime
import csv
import sys
import os
import html

ctk.set_appearance_mode("light")
ctk.set_default_color_theme("blue")

class FuyohinKaishuAnalyzer:
    def __init__(self):
        self.root = ctk.CTk()
        self.root.title("ğŸ”— fuyohin-kaishu.co.jpå°‚ç”¨å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆä¿®æ­£ç‰ˆï¼‰")
        self.root.geometry("1400x900")

        self.domain = "fuyohin-kaishu.co.jp"
        self.categories = [
            {
                'name': 'ã‚´ãƒŸå±‹æ•·ãƒ»æ±šéƒ¨å±‹',
                'url': 'https://fuyohin-kaishu.co.jp/garbage-house',
                'path': '/garbage-house/',
                'id': 174
            },
            {
                'name': 'ä¸ç”¨å“å›å',
                'url': 'https://fuyohin-kaishu.co.jp/unwanted-items',
                'path': '/unwanted-items/',
                'id': 173
            },
            {
                'name': 'æœªåˆ†é¡',
                'url': 'https://fuyohin-kaishu.co.jp/uncategorized',
                'path': '/uncategorized/',
                'id': 1
            },
            {
                'name': 'éºå“æ•´ç†ãƒ»ç”Ÿå‰æ•´ç†',
                'url': 'https://fuyohin-kaishu.co.jp/sorting-out-belongings',
                'path': '/sorting-out-belongings/',
                'id': 176
            }
        ]

        self.pages = {}
        self.links = []
        self.detailed_links = []
        self.is_analyzing = False

        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })

        self.setup_ui()
        self.root.after(1000, self.auto_start_analysis)

    def setup_ui(self):
        self.main_frame = ctk.CTkScrollableFrame(self.root)
        self.main_frame.pack(fill="both", expand=True, padx=20, pady=20)

        title_label = ctk.CTkLabel(
            self.main_frame, 
            text="ğŸ”— fuyohin-kaishu.co.jpå°‚ç”¨å†…éƒ¨ãƒªãƒ³ã‚¯åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆä¿®æ­£ç‰ˆï¼‰",
            font=ctk.CTkFont(size=28, weight="bold")
        )
        title_label.pack(pady=20)

        url_frame = ctk.CTkFrame(self.main_frame)
        url_frame.pack(fill="x", pady=10)
        
        ctk.CTkLabel(url_frame, text="å¯¾è±¡ã‚«ãƒ†ã‚´ãƒª:", font=ctk.CTkFont(size=14, weight="bold")).pack(anchor="w", padx=20, pady=5)
        for category in self.categories:
            ctk.CTkLabel(url_frame, text=f"â€¢ {category['name']}: {category['url']}", font=ctk.CTkFont(size=11)).pack(anchor="w", padx=40, pady=2)

        button_frame = ctk.CTkFrame(self.main_frame)
        button_frame.pack(pady=20)
        
        self.analyze_button = ctk.CTkButton(
            button_frame, 
            text="åˆ†æé–‹å§‹", 
            command=self.start_analysis,
            font=ctk.CTkFont(size=16, weight="bold"),
            width=200,
            height=40
        )
        self.analyze_button.pack(side="left", padx=10)

        self.export_button = ctk.CTkButton(
            button_frame, 
            text="è©³ç´°CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ", 
            command=self.export_detailed_csv,
            font=ctk.CTkFont(size=16, weight="bold"),
            width=200,
            height=40
        )
        self.export_button.pack(side="left", padx=10)

        self.status_label = ctk.CTkLabel(
            self.main_frame,
            text="æº–å‚™å®Œäº†",
            font=ctk.CTkFont(size=16, weight="bold")
        )
        self.status_label.pack(pady=10)

        self.progress_bar = ctk.CTkProgressBar(self.main_frame, width=600)
        self.progress_bar.pack(pady=10)
        self.progress_bar.set(0)

        self.result_frame = ctk.CTkScrollableFrame(self.main_frame, height=400)
        self.result_frame.pack(fill="both", expand=True, pady=20)

    def log(self, message):
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] {message}")

    def auto_start_analysis(self):
        self.log("=== fuyohin-kaishu.co.jp è‡ªå‹•åˆ†æé–‹å§‹ ===")
        self.start_analysis()

    def start_analysis(self):
        if self.is_analyzing:
            return
            
        self.analyze_button.configure(state="disabled")
        self.status_label.configure(text="åˆ†æä¸­...")
        self.progress_bar.start()

        thread = threading.Thread(target=self.analyze_site)
        thread.daemon = True
        thread.start()

    def get_all_articles_comprehensive(self):
        all_articles = []
        
        for category in self.categories:
            self.log(f"=== {category['name']} ã‚«ãƒ†ã‚´ãƒªåˆ†æé–‹å§‹ ===")
            category_articles = self.get_articles_from_category(category)
            
            if category_articles:
                all_articles.extend(category_articles)
                self.log(f"{category['name']}: {len(category_articles)}è¨˜äº‹åé›†")
            
            time.sleep(1)
        
        api_articles = self.get_articles_from_wp_api()
        if api_articles:
            all_articles.extend(api_articles)
            self.log(f"WordPress API: {len(api_articles)}è¨˜äº‹è¿½åŠ ")
        
        sitemap_articles = self.get_articles_from_sitemap()
        if sitemap_articles:
            all_articles.extend(sitemap_articles)
            self.log(f"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—: {len(sitemap_articles)}è¨˜äº‹è¿½åŠ ")
        
        unique_articles = self.remove_duplicate_articles(all_articles)
        
        self.log(f"=== å…¨è¨˜äº‹åé›†å®Œäº†: {len(unique_articles)}è¨˜äº‹ ===")
        return unique_articles

    def get_articles_from_category(self, category):
        articles = []
        base_url = category['url']
        
        page = 1
        max_pages = 20
        
        while page <= max_pages:
            try:
                if page == 1:
                    list_url = base_url
                else:
                    list_url = f"{base_url}/page/{page}"
                
                self.log(f"  ãƒšãƒ¼ã‚¸{page}ã‚’ç¢ºèªä¸­: {list_url}")
                response = self.session.get(list_url, timeout=30)
                
                if response.status_code != 200:
                    break
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                page_articles = []
                
                post_list_items = soup.find_all(['article', 'div'], class_=re.compile(r'p-postList__item|post-item|entry-item'))
                for item in post_list_items:
                    link = item.find('a', href=True)
                    if link:
                        href = link.get('href')
                        if not href:
                            continue
                        
                        if href.startswith('/'):
                            full_url = f"https://{self.domain}{href}"
                        elif href.startswith('http'):
                            full_url = href
                        else:
                            continue
                        
                        if category['path'] in full_url:
                            if any(ex in full_url for ex in ['page/', 'feed/', '#', '?']):
                                continue
                            
                            if not any(a['url'] == full_url for a in articles + page_articles):
                                link_text = self.extract_link_title(link, item)
                                if link_text and len(link_text) > 3:
                                    page_articles.append({
                                        'url': full_url.rstrip('/'),
                                        'title': link_text,
                                        'category': category['name']
                                    })
                
                direct_links = soup.find_all('a', href=re.compile(category['path']))
                for link in direct_links:
                    href = link.get('href')
                    if href and category['path'] in href:
                        if href.startswith('/'):
                            full_url = f"https://{self.domain}{href}"
                        elif href.startswith('http'):
                            full_url = href
                        else:
                            continue
                        
                        if any(ex in full_url for ex in ['page/', 'feed/', '#', '?']):
                            continue
                        
                        if not any(a['url'] == full_url for a in articles + page_articles):
                            link_text = self.extract_link_title(link, None)
                            if link_text and len(link_text) > 3:
                                page_articles.append({
                                    'url': full_url.rstrip('/'),
                                    'title': link_text,
                                    'category': category['name']
                                })
                
                if page_articles:
                    articles.extend(page_articles)
                    self.log(f"    ãƒšãƒ¼ã‚¸{page}: {len(page_articles)}è¨˜äº‹ç™ºè¦‹")
                else:
                    break
                
                page += 1
                time.sleep(0.8)
                
            except Exception as e:
                self.log(f"  ãƒšãƒ¼ã‚¸{page}ã‚¨ãƒ©ãƒ¼: {str(e)}")
                break
        
        return articles

    def extract_link_title(self, link, container):
        link_text = link.get_text(strip=True)
        if link_text and len(link_text) > 5 and not link_text.lower() in ['ç¶šãã‚’èª­ã‚€', 'read more', 'è©³ç´°']:
            return link_text[:150]
        
        if link.get('title'):
            return link.get('title').strip()[:150]
        
        if container:
            title_selectors = [
                '.p-postList__title',
                '.entry-title', 
                '.post-title',
                'h2', 'h3', 'h4'
            ]
            
            for selector in title_selectors:
                title_element = container.select_one(selector)
                if title_element:
                    title_text = title_element.get_text(strip=True)
                    if title_text and len(title_text) > 5:
                        return title_text[:150]
        
        return link.get('href', 'ãƒªãƒ³ã‚¯')[:150]

    def get_articles_from_wp_api(self):
        articles = []
        
        try:
            api_url = f"https://{self.domain}/wp-json/wp/v2/posts"
            
            page = 1
            per_page = 100
            
            while page <= 10:
                params = {
                    'page': page,
                    'per_page': per_page,
                    'status': 'publish'
                }
                
                self.log(f"WordPress API ãƒšãƒ¼ã‚¸{page}ã‚’å–å¾—ä¸­...")
                response = self.session.get(api_url, params=params, timeout=30)
                
                if response.status_code != 200:
                    self.log(f"WordPress API ã‚¨ãƒ©ãƒ¼: {response.status_code}")
                    break
                
                posts = response.json()
                if not posts:
                    break
                
                for post in posts:
                    post_url = post.get('link', '')
                    if any(cat['path'] in post_url for cat in self.categories):
                        category_name = "ãã®ä»–"
                        for cat in self.categories:
                            if cat['path'] in post_url:
                                category_name = cat['name']
                                break
                        
                        title = post.get('title', {}).get('rendered', '')
                        if title:
                            title = html.unescape(title)
                            title = re.sub(r'<[^>]+>', '', title)
                            title = title.strip()
                            
                            if len(title) > 3:
                                articles.append({
                                    'url': post_url.rstrip('/'),
                                    'title': title[:150],
                                    'category': category_name
                                })
                                self.log(f"  APIè¨˜äº‹å–å¾—: {title[:30]}...")
                
                page += 1
                time.sleep(0.5)
                
        except Exception as e:
            self.log(f"WordPress APIå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return articles

    def get_articles_from_sitemap(self):
        articles = []
        
        sitemap_urls = [
            f"https://{self.domain}/sitemap.xml",
            f"https://{self.domain}/wp-sitemap.xml",
            f"https://{self.domain}/sitemap_index.xml",
            f"https://{self.domain}/wp-sitemap-posts-post-1.xml"
        ]
        
        for sitemap_url in sitemap_urls:
            try:
                response = self.session.get(sitemap_url, timeout=30)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'xml')
                    
                    urls = soup.find_all('url')
                    for url in urls:
                        loc = url.find('loc')
                        if loc:
                            url_text = loc.text.rstrip('/')
                            for category in self.categories:
                                if category['path'] in url_text:
                                    slug = url_text.split('/')[-1]
                                    title = slug.replace('-', ' ').title() if slug else f"è¨˜äº‹ - {category['name']}"
                                    
                                    articles.append({
                                        'url': url_text,
                                        'title': title,
                                        'category': category['name']
                                    })
                                    break
                    
                    if articles:
                        break
                        
            except Exception as e:
                continue
        
        return articles

    def remove_duplicate_articles(self, articles):
        unique_articles = []
        seen_urls = set()
        
        for article in articles:
            url = article['url'].rstrip('/')
            if url not in seen_urls:
                unique_articles.append(article)
                seen_urls.add(url)
        
        return unique_articles

    def analyze_site(self):
        try:
            self.is_analyzing = True
            
            self.root.after(0, lambda: self.status_label.configure(text="è¨˜äº‹ä¸€è¦§ã‚’å–å¾—ä¸­..."))
            articles = self.get_all_articles_comprehensive()
            
            if not articles:
                raise Exception("è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
            self.log(f"åˆè¨ˆ {len(articles)} è¨˜äº‹ã‚’ç™ºè¦‹")
            
            pages = {}
            detailed_links = []
            processed_links = set()
            
            for i, article in enumerate(articles):
                try:
                    status_text = f"è¨˜äº‹åˆ†æä¸­ {i+1}/{len(articles)}: {article['title'][:30]}..."
                    self.root.after(0, lambda text=status_text: self.status_label.configure(text=text))
                    self.root.after(0, lambda: self.progress_bar.set((i+1) / len(articles)))
                    
                    response = self.session.get(article['url'], timeout=30)
                    
                    if response.status_code != 200:
                        pages[article['url']] = {
                            'title': article['title'],
                            'category': article.get('category', 'ä¸æ˜'),
                            'outbound_links': [],
                            'inbound_links': 0
                        }
                        self.log(f"  ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•— {response.status_code}: {article['title']}")
                        continue
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    page_title = self.extract_page_title(soup, article)
                    
                    pages[article['url']] = {
                        'title': page_title,
                        'category': article.get('category', 'ä¸æ˜'),
                        'outbound_links': [],
                        'inbound_links': 0
                    }
                    
                    content = self.extract_main_content(soup)
                    if content:
                        links_data = self.extract_links(content)
                        
                        for link_data in links_data:
                            target_url = self.normalize_url(link_data['url'])
                            
                            if self.is_target_article(target_url):
                                link_key = (article['url'], target_url)
                                if link_key not in processed_links:
                                    processed_links.add(link_key)
                                    
                                    pages[article['url']]['outbound_links'].append(target_url)
                                    
                                    detailed_links.append({
                                        'source_url': article['url'],
                                        'source_title': page_title,
                                        'source_category': article.get('category', 'ä¸æ˜'),
                                        'target_url': target_url,
                                        'anchor_text': link_data['anchor_text']
                                    })
                    
                    time.sleep(0.3)
                    
                except Exception as e:
                    self.log(f"è¨˜äº‹åˆ†æã‚¨ãƒ©ãƒ¼ {article['url']}: {str(e)}")
                    if article['url'] not in pages:
                        pages[article['url']] = {
                            'title': article['title'],
                            'category': article.get('category', 'ä¸æ˜'),
                            'outbound_links': [],
                            'inbound_links': 0
                        }
                    continue
            
            # ã€é‡è¦ã€‘ãƒªãƒ³ã‚¯å…ˆãƒšãƒ¼ã‚¸ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
            self.log("=== ãƒªãƒ³ã‚¯å…ˆãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—ä¸­ ===")
            self.fetch_missing_page_titles(detailed_links, pages)
            
            for link in detailed_links:
                target_url = link['target_url']
                if target_url in pages:
                    pages[target_url]['inbound_links'] += 1
            
            self.pages = pages
            self.detailed_links = detailed_links
            
            self.log(f"æœ€çµ‚çµæœ: {len(pages)}ãƒšãƒ¼ã‚¸, {len(detailed_links)}å†…éƒ¨ãƒªãƒ³ã‚¯")
            self.root.after(0, self.show_results)
            self.root.after(2000, self.auto_export_csv)
            
        except Exception as e:
            self.root.after(0, lambda: messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"))
        finally:
            self.is_analyzing = False
            self.root.after(0, lambda: self.analyze_button.configure(state="normal"))
            self.root.after(0, lambda: self.progress_bar.stop())
            self.root.after(0, lambda: self.status_label.configure(text="åˆ†æå®Œäº†"))

    def fetch_missing_page_titles(self, detailed_links, pages):
        """ãƒªãƒ³ã‚¯å…ˆãƒšãƒ¼ã‚¸ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—ï¼ˆé‡è¦ãªä¿®æ­£ï¼‰"""
        
        # ãƒªãƒ³ã‚¯å…ˆURLã§ã¾ã ã‚¿ã‚¤ãƒˆãƒ«ãŒå–å¾—ã•ã‚Œã¦ã„ãªã„ãƒšãƒ¼ã‚¸ã‚’ç‰¹å®š
        missing_urls = set()
        for link in detailed_links:
            target_url = link['target_url']
            if target_url not in pages:
                missing_urls.add(target_url)
        
        self.log(f"ã‚¿ã‚¤ãƒˆãƒ«æœªå–å¾—ã®ãƒšãƒ¼ã‚¸: {len(missing_urls)}å€‹")
        
        for i, url in enumerate(missing_urls):
            try:
                self.log(f"  ã‚¿ã‚¤ãƒˆãƒ«å–å¾—ä¸­ {i+1}/{len(missing_urls)}: {url}")
                response = self.session.get(url, timeout=30)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # URLã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’æ¨å®š
                    category = "ãã®ä»–"
                    for cat in self.categories:
                        if cat['path'] in url:
                            category = cat['name']
                            break
                    
                    # ãƒ€ãƒŸãƒ¼è¨˜äº‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ã¦ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
                    dummy_article = {
                        'url': url,
                        'title': '',
                        'category': category
                    }
                    
                    page_title = self.extract_page_title(soup, dummy_article)
                    
                    pages[url] = {
                        'title': page_title,
                        'category': category,
                        'outbound_links': [],
                        'inbound_links': 0
                    }
                    
                    self.log(f"    å–å¾—æˆåŠŸ: {page_title[:50]}")
                else:
                    # ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—æ™‚ã¯URLã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ
                    slug = url.split('/')[-1]
                    generated_title = slug.replace('-', ' ').title() if slug else url
                    
                    pages[url] = {
                        'title': generated_title,
                        'category': category,
                        'outbound_links': [],
                        'inbound_links': 0
                    }
                    
                    self.log(f"    ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—ã€ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ: {generated_title}")
                
                time.sleep(0.5)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                
            except Exception as e:
                # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
                slug = url.split('/')[-1]
                fallback_title = slug.replace('-', ' ').title() if slug else url
                
                pages[url] = {
                    'title': fallback_title,
                    'category': "ãã®ä»–",
                    'outbound_links': [],
                    'inbound_links': 0
                }
                
                self.log(f"    ã‚¨ãƒ©ãƒ¼ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {fallback_title}")

    def extract_page_title(self, soup, article):
        """ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ­£ã—ãæŠ½å‡º"""
        
        title_selectors = [
            '.c-postTitle__ttl',
            'h1.c-postTitle__ttl',
            '.entry-title',
            '.post-title',
            '.article-title',
            'h1'
        ]
        
        for selector in title_selectors:
            try:
                title_elements = soup.select(selector)
                
                for title_element in title_elements:
                    if title_element and title_element.get_text(strip=True):
                        title_text = title_element.get_text(strip=True)
                        
                        if len(title_text) > 3 and not title_text.lower() in ['home', 'top', 'ãƒ›ãƒ¼ãƒ ']:
                            return title_text
            except Exception as e:
                continue
        
        if soup.title and soup.title.string:
            title = soup.title.string.strip()
            
            title = re.sub(r'\s*[|\-â€“]\s*.*(ä¸ç”¨å“å›å|fuyohin|kaishu).*$', '', title, flags=re.IGNORECASE)
            title = re.sub(r'\s*[|\-â€“]\s*.*ä¸ç”¨å“å›åéšŠ.*$', '', title, flags=re.IGNORECASE)
            
            if len(title.strip()) > 3:
                return title.strip()
        
        if article.get('title') and len(article['title']) > 3:
            if not article['title'].startswith('http') and '/' not in article['title']:
                return article['title']
        
        path = urlparse(article['url']).path
        if path:
            slug = path.strip('/').split('/')[-1]
            
            if slug and slug != 'wp' and len(slug) > 1:
                generated_title = slug.replace('-', ' ').replace('_', ' ').title()
                return generated_title
        
        return f"è¨˜äº‹({article['url'].split('/')[-1] if article['url'].split('/')[-1] else 'unknown'})"

    def extract_main_content(self, soup):
        exclude_selectors = [
            '#sidebar', '.l-sidebar', '.sidebar',
            '.p-relatedPosts', '.related-posts',
            '.p-pnLinks', '.prev-next-links',
            '.c-shareBtns', '.share-buttons',
            '.p-authorBox', '.author-box',
            '#breadcrumb', '.breadcrumb',
            '.widget', '.c-widget',
            '.footer_cta', '.fixed-banner'
        ]
        
        for selector in exclude_selectors:
            for element in soup.select(selector):
                element.decompose()
        
        content_selectors = [
            '.post_content',
            '.entry-content',
            '.post-content',
            'main article',
            '#main_content'
        ]
        
        for selector in content_selectors:
            content = soup.select_one(selector)
            if content:
                return content
        
        return soup.find('main') or soup.find('article')

    def extract_links(self, content):
        links = []
        
        if not content:
            return links
        
        for link in content.find_all('a', href=True):
            href = link.get('href')
            anchor_text = link.get_text(strip=True) or link.get('title', '') or '[ãƒªãƒ³ã‚¯]'
            
            if href and len(anchor_text) > 0:
                links.append({
                    'url': href,
                    'anchor_text': anchor_text[:100]
                })
        
        return links

    def normalize_url(self, url):
        if url.startswith('/'):
            url = f"https://{self.domain}{url}"
        
        parsed = urlparse(url)
        if parsed.netloc != self.domain:
            return url
        
        path = parsed.path.rstrip('/')
        if path and not path.endswith('/'):
            path += '/'
        
        return f"https://{self.domain}{path}"

    def is_target_article(self, url):
        target_paths = [cat['path'] for cat in self.categories]
        
        if not any(target_path in url for target_path in target_paths):
            return False
        
        exclude_patterns = [
            '/wp-admin/', '/wp-content/', '/wp-json/',
            '.jpg', '.jpeg', '.png', '.gif', '.pdf',
            '/feed/', '/page/', '#', '?',
            '/contact', '/privacy', '/terms'
        ]
        
        return not any(pattern in url.lower() for pattern in exclude_patterns)

    def show_results(self):
        for widget in self.result_frame.winfo_children():
            widget.destroy()

        total_pages = len(self.pages)
        total_links = len(self.detailed_links)
        isolated_pages = sum(1 for p in self.pages.values() if p['inbound_links'] == 0)
        popular_pages = sum(1 for p in self.pages.values() if p['inbound_links'] >= 5)

        stats_label = ctk.CTkLabel(
            self.result_frame,
            text=f"åˆ†æçµæœ: {total_pages}ãƒšãƒ¼ã‚¸ | {total_links}å†…éƒ¨ãƒªãƒ³ã‚¯ | å­¤ç«‹è¨˜äº‹{isolated_pages}ä»¶ | äººæ°—è¨˜äº‹{popular_pages}ä»¶",
            font=ctk.CTkFont(size=16, weight="bold")
        )
        stats_label.pack(pady=10)

        for category in self.categories:
            category_pages = [p for p in self.pages.values() if p.get('category') == category['name']]
            if category_pages:
                avg_inbound = sum(p['inbound_links'] for p in category_pages) / len(category_pages)
                ctk.CTkLabel(
                    self.result_frame,
                    text=f"{category['name']}: {len(category_pages)}è¨˜äº‹, å¹³å‡è¢«ãƒªãƒ³ã‚¯æ•°: {avg_inbound:.1f}",
                    font=ctk.CTkFont(size=12)
                ).pack(anchor="w", padx=20)

        sorted_pages = sorted(self.pages.items(), key=lambda x: x[1]['inbound_links'], reverse=True)
        
        ctk.CTkLabel(
            self.result_frame,
            text="è¢«ãƒªãƒ³ã‚¯æ•°ãƒˆãƒƒãƒ—10:",
            font=ctk.CTkFont(size=14, weight="bold")
        ).pack(pady=(20, 5), anchor="w")
        
        for i, (url, info) in enumerate(sorted_pages[:10]):
            label_text = f"{i+1:2d}. [{info['inbound_links']:2d}è¢«ãƒªãƒ³ã‚¯] {info['title'][:60]}..."
            
            ctk.CTkLabel(
                self.result_frame,
                text=label_text,
                anchor="w",
                font=ctk.CTkFont(size=11)
            ).pack(fill="x", padx=20, pady=1)

    def auto_export_csv(self):
        try:
            if hasattr(sys, '_MEIPASS'):
                base_dir = os.path.dirname(sys.executable)
            else:
                base_dir = os.path.dirname(os.path.abspath(__file__))
            
            today = datetime.now()
            date_folder = today.strftime("%Y-%m-%d")
            folder_path = os.path.join(base_dir, date_folder)
            
            os.makedirs(folder_path, exist_ok=True)
            
            csv_filename = f"fuyohin-kaishu-{date_folder}.csv"
            filename = os.path.join(folder_path, csv_filename)
            
            self.export_detailed_csv_to_file(filename)
            
            self.log(f"=== è‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {date_folder}/{csv_filename} ===")
            
        except Exception as e:
            self.log(f"è‡ªå‹•CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")

    def export_detailed_csv(self):
        if not self.detailed_links and not self.pages:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            return

        filename = filedialog.asksaveasfilename(
            defaultextension=".csv", 
            filetypes=[("CSV files", "*.csv")],
            initialname=f"fuyohin-kaishu-{datetime.now().strftime('%Y-%m-%d')}.csv"
        )
        if filename:
            self.export_detailed_csv_to_file(filename)

    def export_detailed_csv_to_file(self, filename):
        try:
            print(f"\n=== CSVå‡ºåŠ›é–‹å§‹ ===")
            print(f"ç·ãƒšãƒ¼ã‚¸æ•°: {len(self.pages)}")
            print(f"å†…éƒ¨ãƒªãƒ³ã‚¯æ•°: {len(self.detailed_links)}")
            
            with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                
                writer.writerow([
                    'ç•ªå·', 'ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«', 'URL',
                    'è¢«ãƒªãƒ³ã‚¯å…ƒã‚¿ã‚¤ãƒˆãƒ«', 'è¢«ãƒªãƒ³ã‚¯å…ƒURL', 'ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ'
                ])
                
                row_number = 1
                
                target_groups = {}
                for link in self.detailed_links:
                    target_url = link['target_url']
                    if target_url not in target_groups:
                        target_groups[target_url] = []
                    target_groups[target_url].append(link)
                
                sorted_targets = sorted(target_groups.items(), key=lambda x: len(x[1]), reverse=True)
                processed_urls = set()
                
                for target_url, links_to_target in sorted_targets:
                    target_info = self.pages.get(target_url, {})
                    target_title = target_info.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜')
                    
                    # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
                    print(f"å‡ºåŠ›ä¸­ - ãƒšãƒ¼ã‚¸: {target_title[:30]}... (è¢«ãƒªãƒ³ã‚¯{len(links_to_target)}å€‹)")
                    print(f"  URL: {target_url}")
                    print(f"  ã‚¿ã‚¤ãƒˆãƒ«: '{target_title}'")
                    
                    processed_urls.add(target_url)
                    
                    for link in links_to_target:
                        writer.writerow([
                            row_number,
                            target_title,
                            target_url,
                            link['source_title'],
                            link['source_url'],
                            link['anchor_text']
                        ])
                        row_number += 1
                
                isolated_pages = []
                for url, info in self.pages.items():
                    if url not in processed_urls:
                        isolated_pages.append((url, info))
                
                print(f"å­¤ç«‹ãƒšãƒ¼ã‚¸æ•°: {len(isolated_pages)}")
                
                isolated_pages.sort(key=lambda x: x[1].get('title', ''))
                
                for url, info in isolated_pages:
                    title = info.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜')
                    print(f"å­¤ç«‹ãƒšãƒ¼ã‚¸å‡ºåŠ›: {title[:30]}...")
                    
                    writer.writerow([
                        row_number,
                        title,
                        url,
                        'ï¼ˆè¢«ãƒªãƒ³ã‚¯ãªã—ï¼‰',
                        '',
                        ''
                    ])
                    row_number += 1
            
            print(f"=== CSVå‡ºåŠ›å®Œäº†: {row_number-1}è¡Œ ===")
            messagebox.showinfo("å®Œäº†", f"è©³ç´°CSVã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}\nç·è¡Œæ•°: {row_number-1}è¡Œ")
            
        except Exception as e:
            print(f"CSVå‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {e}")
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ä¿å­˜ã«å¤±æ•—: {e}")

def main():
    app = FuyohinKaishuAnalyzer()
    app.root.mainloop()

if __name__ == "__main__":
    main()